{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:55:28.950972Z",
     "start_time": "2024-11-19T08:55:26.974189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 自定义 MAPE 损失函数\n",
    "class MAPE_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-8  # 避免除零\n",
    "        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "# 自定义 RMSE 损失函数\n",
    "class RMSE_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_true - y_pred) ** 2))"
   ],
   "id": "d68fb7f0c48aabd5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:55:28.964160Z",
     "start_time": "2024-11-19T08:55:28.953973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "id": "3412e0bf4626dcaf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:55:29.934120Z",
     "start_time": "2024-11-19T08:55:29.090602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 12)\n",
    "        self.layer2 = nn.Linear(12, 90)\n",
    "        self.layer3 = nn.Linear(90, 90)\n",
    "        self.layer4 = nn.Linear(90, 60)\n",
    "        self.layer5 = nn.Linear(60, 70)\n",
    "        self.layer6 = nn.Linear(70, 30)\n",
    "        self.output = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = torch.relu(self.layer5(x))\n",
    "        x = torch.relu(self.layer6(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = ANN(input_dim)\n",
    "\n",
    "# 选择损失函数 (可以选择 MAPE 或 RMSE)\n",
    "loss_function = MAPE_Loss()\n",
    "# loss_function = RMSE_Loss()  # 或者使用 MAPE_Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 设置 PyTorch 随机种子\n",
    "# def set_random_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)  # 如果使用 GPU\n",
    "#     np.random.seed(seed)  # 设置 Numpy 随机种子\n",
    "#     random.seed(seed)  # 设置 Python 原生随机数生成器的种子\n",
    "#     torch.backends.cudnn.deterministic = True  # 保证卷积操作的确定性\n",
    "#     torch.backends.cudnn.benchmark = False  # 禁用 cudnn 自动优化（用于固定输入尺寸）\n",
    "#\n",
    "# # 设置种子\n",
    "# set_random_seed(21)"
   ],
   "id": "e0b4d3b1903598b0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:57:33.982263Z",
     "start_time": "2024-11-19T08:55:29.949998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练模型\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_function(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 每 10 个 epoch 打印一次平均损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"ann_model_CPU_5000.pth\")"
   ],
   "id": "baebfe374d23363f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/5000], Loss: 32.6088\n",
      "Epoch [20/5000], Loss: 25.0076\n",
      "Epoch [30/5000], Loss: 20.8672\n",
      "Epoch [40/5000], Loss: 17.4715\n",
      "Epoch [50/5000], Loss: 15.1184\n",
      "Epoch [60/5000], Loss: 13.8035\n",
      "Epoch [70/5000], Loss: 13.0016\n",
      "Epoch [80/5000], Loss: 12.6784\n",
      "Epoch [90/5000], Loss: 12.3825\n",
      "Epoch [100/5000], Loss: 12.3214\n",
      "Epoch [110/5000], Loss: 11.6720\n",
      "Epoch [120/5000], Loss: 11.3387\n",
      "Epoch [130/5000], Loss: 11.1410\n",
      "Epoch [140/5000], Loss: 10.9073\n",
      "Epoch [150/5000], Loss: 10.5253\n",
      "Epoch [160/5000], Loss: 10.3899\n",
      "Epoch [170/5000], Loss: 10.4905\n",
      "Epoch [180/5000], Loss: 9.8678\n",
      "Epoch [190/5000], Loss: 10.2188\n",
      "Epoch [200/5000], Loss: 10.0219\n",
      "Epoch [210/5000], Loss: 9.6355\n",
      "Epoch [220/5000], Loss: 8.9991\n",
      "Epoch [230/5000], Loss: 8.6299\n",
      "Epoch [240/5000], Loss: 9.0249\n",
      "Epoch [250/5000], Loss: 8.2829\n",
      "Epoch [260/5000], Loss: 8.1197\n",
      "Epoch [270/5000], Loss: 7.9727\n",
      "Epoch [280/5000], Loss: 8.4938\n",
      "Epoch [290/5000], Loss: 8.9016\n",
      "Epoch [300/5000], Loss: 7.6739\n",
      "Epoch [310/5000], Loss: 7.5248\n",
      "Epoch [320/5000], Loss: 7.5702\n",
      "Epoch [330/5000], Loss: 7.5730\n",
      "Epoch [340/5000], Loss: 7.2107\n",
      "Epoch [350/5000], Loss: 7.9299\n",
      "Epoch [360/5000], Loss: 7.1563\n",
      "Epoch [370/5000], Loss: 6.5887\n",
      "Epoch [380/5000], Loss: 7.3249\n",
      "Epoch [390/5000], Loss: 6.8131\n",
      "Epoch [400/5000], Loss: 7.2810\n",
      "Epoch [410/5000], Loss: 6.7674\n",
      "Epoch [420/5000], Loss: 7.0949\n",
      "Epoch [430/5000], Loss: 6.8720\n",
      "Epoch [440/5000], Loss: 6.6091\n",
      "Epoch [450/5000], Loss: 6.5069\n",
      "Epoch [460/5000], Loss: 7.2039\n",
      "Epoch [470/5000], Loss: 6.1530\n",
      "Epoch [480/5000], Loss: 6.4525\n",
      "Epoch [490/5000], Loss: 5.7972\n",
      "Epoch [500/5000], Loss: 6.2933\n",
      "Epoch [510/5000], Loss: 6.0699\n",
      "Epoch [520/5000], Loss: 5.6847\n",
      "Epoch [530/5000], Loss: 5.4538\n",
      "Epoch [540/5000], Loss: 6.1075\n",
      "Epoch [550/5000], Loss: 5.7244\n",
      "Epoch [560/5000], Loss: 5.5793\n",
      "Epoch [570/5000], Loss: 5.7325\n",
      "Epoch [580/5000], Loss: 5.4134\n",
      "Epoch [590/5000], Loss: 5.7312\n",
      "Epoch [600/5000], Loss: 5.3830\n",
      "Epoch [610/5000], Loss: 6.1283\n",
      "Epoch [620/5000], Loss: 5.5528\n",
      "Epoch [630/5000], Loss: 5.5563\n",
      "Epoch [640/5000], Loss: 6.1552\n",
      "Epoch [650/5000], Loss: 6.2242\n",
      "Epoch [660/5000], Loss: 5.3953\n",
      "Epoch [670/5000], Loss: 4.7720\n",
      "Epoch [680/5000], Loss: 4.8014\n",
      "Epoch [690/5000], Loss: 5.1834\n",
      "Epoch [700/5000], Loss: 4.5015\n",
      "Epoch [710/5000], Loss: 5.2725\n",
      "Epoch [720/5000], Loss: 5.1992\n",
      "Epoch [730/5000], Loss: 4.4529\n",
      "Epoch [740/5000], Loss: 4.6076\n",
      "Epoch [750/5000], Loss: 4.9047\n",
      "Epoch [760/5000], Loss: 4.5596\n",
      "Epoch [770/5000], Loss: 4.3962\n",
      "Epoch [780/5000], Loss: 4.5575\n",
      "Epoch [790/5000], Loss: 4.7985\n",
      "Epoch [800/5000], Loss: 4.8776\n",
      "Epoch [810/5000], Loss: 4.6792\n",
      "Epoch [820/5000], Loss: 4.8517\n",
      "Epoch [830/5000], Loss: 4.3538\n",
      "Epoch [840/5000], Loss: 4.2112\n",
      "Epoch [850/5000], Loss: 4.1836\n",
      "Epoch [860/5000], Loss: 4.4231\n",
      "Epoch [870/5000], Loss: 4.5437\n",
      "Epoch [880/5000], Loss: 4.0526\n",
      "Epoch [890/5000], Loss: 4.7379\n",
      "Epoch [900/5000], Loss: 4.8751\n",
      "Epoch [910/5000], Loss: 4.3005\n",
      "Epoch [920/5000], Loss: 4.8610\n",
      "Epoch [930/5000], Loss: 4.3452\n",
      "Epoch [940/5000], Loss: 4.1745\n",
      "Epoch [950/5000], Loss: 4.2816\n",
      "Epoch [960/5000], Loss: 4.4149\n",
      "Epoch [970/5000], Loss: 4.2382\n",
      "Epoch [980/5000], Loss: 4.5913\n",
      "Epoch [990/5000], Loss: 4.1547\n",
      "Epoch [1000/5000], Loss: 4.1074\n",
      "Epoch [1010/5000], Loss: 4.0970\n",
      "Epoch [1020/5000], Loss: 3.7907\n",
      "Epoch [1030/5000], Loss: 4.2102\n",
      "Epoch [1040/5000], Loss: 4.2046\n",
      "Epoch [1050/5000], Loss: 4.4741\n",
      "Epoch [1060/5000], Loss: 3.9173\n",
      "Epoch [1070/5000], Loss: 4.2388\n",
      "Epoch [1080/5000], Loss: 4.1825\n",
      "Epoch [1090/5000], Loss: 4.2325\n",
      "Epoch [1100/5000], Loss: 4.9225\n",
      "Epoch [1110/5000], Loss: 4.1257\n",
      "Epoch [1120/5000], Loss: 4.4866\n",
      "Epoch [1130/5000], Loss: 3.5956\n",
      "Epoch [1140/5000], Loss: 4.2097\n",
      "Epoch [1150/5000], Loss: 4.2299\n",
      "Epoch [1160/5000], Loss: 4.1150\n",
      "Epoch [1170/5000], Loss: 4.6716\n",
      "Epoch [1180/5000], Loss: 4.4395\n",
      "Epoch [1190/5000], Loss: 4.0621\n",
      "Epoch [1200/5000], Loss: 3.6777\n",
      "Epoch [1210/5000], Loss: 4.0897\n",
      "Epoch [1220/5000], Loss: 4.2665\n",
      "Epoch [1230/5000], Loss: 3.6316\n",
      "Epoch [1240/5000], Loss: 3.6867\n",
      "Epoch [1250/5000], Loss: 3.8860\n",
      "Epoch [1260/5000], Loss: 3.8571\n",
      "Epoch [1270/5000], Loss: 4.0746\n",
      "Epoch [1280/5000], Loss: 4.1850\n",
      "Epoch [1290/5000], Loss: 3.7876\n",
      "Epoch [1300/5000], Loss: 4.0261\n",
      "Epoch [1310/5000], Loss: 3.9708\n",
      "Epoch [1320/5000], Loss: 4.7381\n",
      "Epoch [1330/5000], Loss: 3.7471\n",
      "Epoch [1340/5000], Loss: 4.0011\n",
      "Epoch [1350/5000], Loss: 3.9149\n",
      "Epoch [1360/5000], Loss: 3.5136\n",
      "Epoch [1370/5000], Loss: 3.4644\n",
      "Epoch [1380/5000], Loss: 4.2093\n",
      "Epoch [1390/5000], Loss: 3.9505\n",
      "Epoch [1400/5000], Loss: 3.7093\n",
      "Epoch [1410/5000], Loss: 3.6535\n",
      "Epoch [1420/5000], Loss: 3.5571\n",
      "Epoch [1430/5000], Loss: 3.5511\n",
      "Epoch [1440/5000], Loss: 3.7882\n",
      "Epoch [1450/5000], Loss: 3.6712\n",
      "Epoch [1460/5000], Loss: 4.1854\n",
      "Epoch [1470/5000], Loss: 3.7575\n",
      "Epoch [1480/5000], Loss: 3.7551\n",
      "Epoch [1490/5000], Loss: 3.6900\n",
      "Epoch [1500/5000], Loss: 3.9315\n",
      "Epoch [1510/5000], Loss: 3.5950\n",
      "Epoch [1520/5000], Loss: 3.4899\n",
      "Epoch [1530/5000], Loss: 3.7440\n",
      "Epoch [1540/5000], Loss: 3.6818\n",
      "Epoch [1550/5000], Loss: 3.6689\n",
      "Epoch [1560/5000], Loss: 3.3889\n",
      "Epoch [1570/5000], Loss: 3.6701\n",
      "Epoch [1580/5000], Loss: 3.7759\n",
      "Epoch [1590/5000], Loss: 3.7196\n",
      "Epoch [1600/5000], Loss: 3.7242\n",
      "Epoch [1610/5000], Loss: 3.5472\n",
      "Epoch [1620/5000], Loss: 3.4535\n",
      "Epoch [1630/5000], Loss: 3.2756\n",
      "Epoch [1640/5000], Loss: 3.2549\n",
      "Epoch [1650/5000], Loss: 3.5262\n",
      "Epoch [1660/5000], Loss: 3.4135\n",
      "Epoch [1670/5000], Loss: 3.4588\n",
      "Epoch [1680/5000], Loss: 4.4514\n",
      "Epoch [1690/5000], Loss: 3.5779\n",
      "Epoch [1700/5000], Loss: 3.3660\n",
      "Epoch [1710/5000], Loss: 3.7267\n",
      "Epoch [1720/5000], Loss: 4.8103\n",
      "Epoch [1730/5000], Loss: 2.9193\n",
      "Epoch [1740/5000], Loss: 3.8450\n",
      "Epoch [1750/5000], Loss: 3.1457\n",
      "Epoch [1760/5000], Loss: 3.4049\n",
      "Epoch [1770/5000], Loss: 3.3290\n",
      "Epoch [1780/5000], Loss: 3.1700\n",
      "Epoch [1790/5000], Loss: 3.8217\n",
      "Epoch [1800/5000], Loss: 3.3760\n",
      "Epoch [1810/5000], Loss: 3.4630\n",
      "Epoch [1820/5000], Loss: 3.6733\n",
      "Epoch [1830/5000], Loss: 3.4969\n",
      "Epoch [1840/5000], Loss: 3.3518\n",
      "Epoch [1850/5000], Loss: 3.2607\n",
      "Epoch [1860/5000], Loss: 3.7186\n",
      "Epoch [1870/5000], Loss: 3.6585\n",
      "Epoch [1880/5000], Loss: 3.4620\n",
      "Epoch [1890/5000], Loss: 3.0345\n",
      "Epoch [1900/5000], Loss: 2.9989\n",
      "Epoch [1910/5000], Loss: 2.9835\n",
      "Epoch [1920/5000], Loss: 2.8770\n",
      "Epoch [1930/5000], Loss: 3.1628\n",
      "Epoch [1940/5000], Loss: 3.3731\n",
      "Epoch [1950/5000], Loss: 3.7553\n",
      "Epoch [1960/5000], Loss: 3.1967\n",
      "Epoch [1970/5000], Loss: 3.1562\n",
      "Epoch [1980/5000], Loss: 3.0809\n",
      "Epoch [1990/5000], Loss: 3.0562\n",
      "Epoch [2000/5000], Loss: 2.9603\n",
      "Epoch [2010/5000], Loss: 3.2569\n",
      "Epoch [2020/5000], Loss: 3.5332\n",
      "Epoch [2030/5000], Loss: 3.4582\n",
      "Epoch [2040/5000], Loss: 3.0777\n",
      "Epoch [2050/5000], Loss: 3.1115\n",
      "Epoch [2060/5000], Loss: 3.3819\n",
      "Epoch [2070/5000], Loss: 3.3599\n",
      "Epoch [2080/5000], Loss: 2.8159\n",
      "Epoch [2090/5000], Loss: 2.8972\n",
      "Epoch [2100/5000], Loss: 3.7555\n",
      "Epoch [2110/5000], Loss: 3.4516\n",
      "Epoch [2120/5000], Loss: 2.9155\n",
      "Epoch [2130/5000], Loss: 3.1007\n",
      "Epoch [2140/5000], Loss: 3.2175\n",
      "Epoch [2150/5000], Loss: 3.3345\n",
      "Epoch [2160/5000], Loss: 3.1721\n",
      "Epoch [2170/5000], Loss: 3.2368\n",
      "Epoch [2180/5000], Loss: 3.4062\n",
      "Epoch [2190/5000], Loss: 3.1632\n",
      "Epoch [2200/5000], Loss: 4.3244\n",
      "Epoch [2210/5000], Loss: 3.3638\n",
      "Epoch [2220/5000], Loss: 3.0608\n",
      "Epoch [2230/5000], Loss: 2.9762\n",
      "Epoch [2240/5000], Loss: 2.8612\n",
      "Epoch [2250/5000], Loss: 3.0119\n",
      "Epoch [2260/5000], Loss: 3.1446\n",
      "Epoch [2270/5000], Loss: 3.0712\n",
      "Epoch [2280/5000], Loss: 3.1983\n",
      "Epoch [2290/5000], Loss: 2.9054\n",
      "Epoch [2300/5000], Loss: 2.9193\n",
      "Epoch [2310/5000], Loss: 2.9009\n",
      "Epoch [2320/5000], Loss: 2.8651\n",
      "Epoch [2330/5000], Loss: 3.6702\n",
      "Epoch [2340/5000], Loss: 3.3607\n",
      "Epoch [2350/5000], Loss: 3.0787\n",
      "Epoch [2360/5000], Loss: 3.2979\n",
      "Epoch [2370/5000], Loss: 3.0584\n",
      "Epoch [2380/5000], Loss: 2.9047\n",
      "Epoch [2390/5000], Loss: 3.0042\n",
      "Epoch [2400/5000], Loss: 2.6868\n",
      "Epoch [2410/5000], Loss: 3.3218\n",
      "Epoch [2420/5000], Loss: 2.8058\n",
      "Epoch [2430/5000], Loss: 3.0195\n",
      "Epoch [2440/5000], Loss: 3.1802\n",
      "Epoch [2450/5000], Loss: 2.9384\n",
      "Epoch [2460/5000], Loss: 3.5892\n",
      "Epoch [2470/5000], Loss: 3.1793\n",
      "Epoch [2480/5000], Loss: 3.0122\n",
      "Epoch [2490/5000], Loss: 3.0645\n",
      "Epoch [2500/5000], Loss: 3.0001\n",
      "Epoch [2510/5000], Loss: 3.0279\n",
      "Epoch [2520/5000], Loss: 2.8178\n",
      "Epoch [2530/5000], Loss: 3.1816\n",
      "Epoch [2540/5000], Loss: 2.7991\n",
      "Epoch [2550/5000], Loss: 2.8903\n",
      "Epoch [2560/5000], Loss: 2.7538\n",
      "Epoch [2570/5000], Loss: 2.9902\n",
      "Epoch [2580/5000], Loss: 2.6921\n",
      "Epoch [2590/5000], Loss: 2.5398\n",
      "Epoch [2600/5000], Loss: 2.7922\n",
      "Epoch [2610/5000], Loss: 2.8399\n",
      "Epoch [2620/5000], Loss: 3.0270\n",
      "Epoch [2630/5000], Loss: 2.9282\n",
      "Epoch [2640/5000], Loss: 2.8139\n",
      "Epoch [2650/5000], Loss: 3.0281\n",
      "Epoch [2660/5000], Loss: 2.5299\n",
      "Epoch [2670/5000], Loss: 2.6450\n",
      "Epoch [2680/5000], Loss: 2.8481\n",
      "Epoch [2690/5000], Loss: 2.9081\n",
      "Epoch [2700/5000], Loss: 2.7763\n",
      "Epoch [2710/5000], Loss: 2.5617\n",
      "Epoch [2720/5000], Loss: 2.6791\n",
      "Epoch [2730/5000], Loss: 2.8067\n",
      "Epoch [2740/5000], Loss: 3.3492\n",
      "Epoch [2750/5000], Loss: 3.0907\n",
      "Epoch [2760/5000], Loss: 3.1334\n",
      "Epoch [2770/5000], Loss: 2.8456\n",
      "Epoch [2780/5000], Loss: 2.8801\n",
      "Epoch [2790/5000], Loss: 2.7640\n",
      "Epoch [2800/5000], Loss: 2.7543\n",
      "Epoch [2810/5000], Loss: 2.6342\n",
      "Epoch [2820/5000], Loss: 3.1190\n",
      "Epoch [2830/5000], Loss: 2.5410\n",
      "Epoch [2840/5000], Loss: 2.6698\n",
      "Epoch [2850/5000], Loss: 3.0689\n",
      "Epoch [2860/5000], Loss: 2.5037\n",
      "Epoch [2870/5000], Loss: 2.8485\n",
      "Epoch [2880/5000], Loss: 2.6036\n",
      "Epoch [2890/5000], Loss: 3.0586\n",
      "Epoch [2900/5000], Loss: 2.9259\n",
      "Epoch [2910/5000], Loss: 2.3828\n",
      "Epoch [2920/5000], Loss: 2.5146\n",
      "Epoch [2930/5000], Loss: 2.7532\n",
      "Epoch [2940/5000], Loss: 2.9539\n",
      "Epoch [2950/5000], Loss: 3.0071\n",
      "Epoch [2960/5000], Loss: 2.6634\n",
      "Epoch [2970/5000], Loss: 2.4644\n",
      "Epoch [2980/5000], Loss: 2.7764\n",
      "Epoch [2990/5000], Loss: 2.6188\n",
      "Epoch [3000/5000], Loss: 2.5905\n",
      "Epoch [3010/5000], Loss: 2.8259\n",
      "Epoch [3020/5000], Loss: 2.9549\n",
      "Epoch [3030/5000], Loss: 2.4158\n",
      "Epoch [3040/5000], Loss: 2.7327\n",
      "Epoch [3050/5000], Loss: 2.9436\n",
      "Epoch [3060/5000], Loss: 3.0859\n",
      "Epoch [3070/5000], Loss: 2.8731\n",
      "Epoch [3080/5000], Loss: 2.6999\n",
      "Epoch [3090/5000], Loss: 3.0282\n",
      "Epoch [3100/5000], Loss: 3.0179\n",
      "Epoch [3110/5000], Loss: 2.7866\n",
      "Epoch [3120/5000], Loss: 3.0927\n",
      "Epoch [3130/5000], Loss: 2.5987\n",
      "Epoch [3140/5000], Loss: 2.8637\n",
      "Epoch [3150/5000], Loss: 2.7676\n",
      "Epoch [3160/5000], Loss: 2.7208\n",
      "Epoch [3170/5000], Loss: 2.7273\n",
      "Epoch [3180/5000], Loss: 2.5767\n",
      "Epoch [3190/5000], Loss: 2.7751\n",
      "Epoch [3200/5000], Loss: 2.6509\n",
      "Epoch [3210/5000], Loss: 2.5634\n",
      "Epoch [3220/5000], Loss: 3.0313\n",
      "Epoch [3230/5000], Loss: 2.5242\n",
      "Epoch [3240/5000], Loss: 2.5881\n",
      "Epoch [3250/5000], Loss: 3.2634\n",
      "Epoch [3260/5000], Loss: 2.7260\n",
      "Epoch [3270/5000], Loss: 2.5299\n",
      "Epoch [3280/5000], Loss: 3.0182\n",
      "Epoch [3290/5000], Loss: 3.1153\n",
      "Epoch [3300/5000], Loss: 2.4928\n",
      "Epoch [3310/5000], Loss: 2.7472\n",
      "Epoch [3320/5000], Loss: 2.6867\n",
      "Epoch [3330/5000], Loss: 2.7683\n",
      "Epoch [3340/5000], Loss: 2.5977\n",
      "Epoch [3350/5000], Loss: 2.5011\n",
      "Epoch [3360/5000], Loss: 3.0163\n",
      "Epoch [3370/5000], Loss: 2.8427\n",
      "Epoch [3380/5000], Loss: 3.1613\n",
      "Epoch [3390/5000], Loss: 2.7982\n",
      "Epoch [3400/5000], Loss: 2.4675\n",
      "Epoch [3410/5000], Loss: 2.6695\n",
      "Epoch [3420/5000], Loss: 2.7328\n",
      "Epoch [3430/5000], Loss: 2.7625\n",
      "Epoch [3440/5000], Loss: 2.4861\n",
      "Epoch [3450/5000], Loss: 2.0908\n",
      "Epoch [3460/5000], Loss: 2.1749\n",
      "Epoch [3470/5000], Loss: 2.3994\n",
      "Epoch [3480/5000], Loss: 2.9362\n",
      "Epoch [3490/5000], Loss: 2.6620\n",
      "Epoch [3500/5000], Loss: 2.4846\n",
      "Epoch [3510/5000], Loss: 2.6337\n",
      "Epoch [3520/5000], Loss: 2.5588\n",
      "Epoch [3530/5000], Loss: 2.3629\n",
      "Epoch [3540/5000], Loss: 2.8419\n",
      "Epoch [3550/5000], Loss: 2.6498\n",
      "Epoch [3560/5000], Loss: 3.0288\n",
      "Epoch [3570/5000], Loss: 2.4709\n",
      "Epoch [3580/5000], Loss: 2.2572\n",
      "Epoch [3590/5000], Loss: 2.2897\n",
      "Epoch [3600/5000], Loss: 2.2511\n",
      "Epoch [3610/5000], Loss: 2.6832\n",
      "Epoch [3620/5000], Loss: 2.6411\n",
      "Epoch [3630/5000], Loss: 2.4859\n",
      "Epoch [3640/5000], Loss: 2.5699\n",
      "Epoch [3650/5000], Loss: 2.7797\n",
      "Epoch [3660/5000], Loss: 2.4915\n",
      "Epoch [3670/5000], Loss: 2.3585\n",
      "Epoch [3680/5000], Loss: 2.5858\n",
      "Epoch [3690/5000], Loss: 2.7620\n",
      "Epoch [3700/5000], Loss: 2.6617\n",
      "Epoch [3710/5000], Loss: 2.6084\n",
      "Epoch [3720/5000], Loss: 2.7182\n",
      "Epoch [3730/5000], Loss: 2.4168\n",
      "Epoch [3740/5000], Loss: 2.4888\n",
      "Epoch [3750/5000], Loss: 2.3594\n",
      "Epoch [3760/5000], Loss: 2.3180\n",
      "Epoch [3770/5000], Loss: 2.4508\n",
      "Epoch [3780/5000], Loss: 2.4260\n",
      "Epoch [3790/5000], Loss: 2.3889\n",
      "Epoch [3800/5000], Loss: 2.1640\n",
      "Epoch [3810/5000], Loss: 2.5790\n",
      "Epoch [3820/5000], Loss: 2.7388\n",
      "Epoch [3830/5000], Loss: 2.7925\n",
      "Epoch [3840/5000], Loss: 2.5627\n",
      "Epoch [3850/5000], Loss: 2.5750\n",
      "Epoch [3860/5000], Loss: 2.5127\n",
      "Epoch [3870/5000], Loss: 2.3193\n",
      "Epoch [3880/5000], Loss: 2.7572\n",
      "Epoch [3890/5000], Loss: 2.3555\n",
      "Epoch [3900/5000], Loss: 2.8043\n",
      "Epoch [3910/5000], Loss: 2.5502\n",
      "Epoch [3920/5000], Loss: 2.3134\n",
      "Epoch [3930/5000], Loss: 2.2019\n",
      "Epoch [3940/5000], Loss: 2.6438\n",
      "Epoch [3950/5000], Loss: 3.0357\n",
      "Epoch [3960/5000], Loss: 2.8817\n",
      "Epoch [3970/5000], Loss: 2.5301\n",
      "Epoch [3980/5000], Loss: 2.2792\n",
      "Epoch [3990/5000], Loss: 2.2447\n",
      "Epoch [4000/5000], Loss: 2.0418\n",
      "Epoch [4010/5000], Loss: 2.4605\n",
      "Epoch [4020/5000], Loss: 2.4455\n",
      "Epoch [4030/5000], Loss: 2.6542\n",
      "Epoch [4040/5000], Loss: 2.7343\n",
      "Epoch [4050/5000], Loss: 2.4552\n",
      "Epoch [4060/5000], Loss: 2.3012\n",
      "Epoch [4070/5000], Loss: 2.6141\n",
      "Epoch [4080/5000], Loss: 2.4535\n",
      "Epoch [4090/5000], Loss: 2.4943\n",
      "Epoch [4100/5000], Loss: 2.1874\n",
      "Epoch [4110/5000], Loss: 2.2784\n",
      "Epoch [4120/5000], Loss: 2.3756\n",
      "Epoch [4130/5000], Loss: 2.4503\n",
      "Epoch [4140/5000], Loss: 2.6752\n",
      "Epoch [4150/5000], Loss: 2.5980\n",
      "Epoch [4160/5000], Loss: 2.3249\n",
      "Epoch [4170/5000], Loss: 2.1773\n",
      "Epoch [4180/5000], Loss: 2.3609\n",
      "Epoch [4190/5000], Loss: 2.3945\n",
      "Epoch [4200/5000], Loss: 2.2340\n",
      "Epoch [4210/5000], Loss: 2.6859\n",
      "Epoch [4220/5000], Loss: 2.4277\n",
      "Epoch [4230/5000], Loss: 2.3550\n",
      "Epoch [4240/5000], Loss: 2.6257\n",
      "Epoch [4250/5000], Loss: 2.4378\n",
      "Epoch [4260/5000], Loss: 2.1856\n",
      "Epoch [4270/5000], Loss: 2.8478\n",
      "Epoch [4280/5000], Loss: 2.5270\n",
      "Epoch [4290/5000], Loss: 2.6168\n",
      "Epoch [4300/5000], Loss: 2.4082\n",
      "Epoch [4310/5000], Loss: 2.3477\n",
      "Epoch [4320/5000], Loss: 2.4010\n",
      "Epoch [4330/5000], Loss: 2.2102\n",
      "Epoch [4340/5000], Loss: 2.2654\n",
      "Epoch [4350/5000], Loss: 2.3726\n",
      "Epoch [4360/5000], Loss: 2.4530\n",
      "Epoch [4370/5000], Loss: 2.1555\n",
      "Epoch [4380/5000], Loss: 2.7840\n",
      "Epoch [4390/5000], Loss: 2.6000\n",
      "Epoch [4400/5000], Loss: 2.2937\n",
      "Epoch [4410/5000], Loss: 2.3583\n",
      "Epoch [4420/5000], Loss: 2.1325\n",
      "Epoch [4430/5000], Loss: 2.3567\n",
      "Epoch [4440/5000], Loss: 2.1714\n",
      "Epoch [4450/5000], Loss: 2.1929\n",
      "Epoch [4460/5000], Loss: 2.3879\n",
      "Epoch [4470/5000], Loss: 2.7405\n",
      "Epoch [4480/5000], Loss: 2.7770\n",
      "Epoch [4490/5000], Loss: 2.7362\n",
      "Epoch [4500/5000], Loss: 2.5505\n",
      "Epoch [4510/5000], Loss: 2.9683\n",
      "Epoch [4520/5000], Loss: 2.1622\n",
      "Epoch [4530/5000], Loss: 2.4346\n",
      "Epoch [4540/5000], Loss: 2.2905\n",
      "Epoch [4550/5000], Loss: 2.2298\n",
      "Epoch [4560/5000], Loss: 2.5874\n",
      "Epoch [4570/5000], Loss: 2.1567\n",
      "Epoch [4580/5000], Loss: 2.0669\n",
      "Epoch [4590/5000], Loss: 2.2917\n",
      "Epoch [4600/5000], Loss: 2.5978\n",
      "Epoch [4610/5000], Loss: 1.9522\n",
      "Epoch [4620/5000], Loss: 2.3156\n",
      "Epoch [4630/5000], Loss: 2.3599\n",
      "Epoch [4640/5000], Loss: 2.4778\n",
      "Epoch [4650/5000], Loss: 2.4969\n",
      "Epoch [4660/5000], Loss: 2.2892\n",
      "Epoch [4670/5000], Loss: 2.3698\n",
      "Epoch [4680/5000], Loss: 2.1278\n",
      "Epoch [4690/5000], Loss: 2.4100\n",
      "Epoch [4700/5000], Loss: 2.5610\n",
      "Epoch [4710/5000], Loss: 2.3920\n",
      "Epoch [4720/5000], Loss: 2.3593\n",
      "Epoch [4730/5000], Loss: 2.4172\n",
      "Epoch [4740/5000], Loss: 2.0829\n",
      "Epoch [4750/5000], Loss: 2.2247\n",
      "Epoch [4760/5000], Loss: 2.2082\n",
      "Epoch [4770/5000], Loss: 2.4512\n",
      "Epoch [4780/5000], Loss: 2.1998\n",
      "Epoch [4790/5000], Loss: 2.7170\n",
      "Epoch [4800/5000], Loss: 2.6099\n",
      "Epoch [4810/5000], Loss: 1.9553\n",
      "Epoch [4820/5000], Loss: 2.6988\n",
      "Epoch [4830/5000], Loss: 2.1720\n",
      "Epoch [4840/5000], Loss: 2.1157\n",
      "Epoch [4850/5000], Loss: 2.3647\n",
      "Epoch [4860/5000], Loss: 1.9639\n",
      "Epoch [4870/5000], Loss: 2.0879\n",
      "Epoch [4880/5000], Loss: 2.6009\n",
      "Epoch [4890/5000], Loss: 2.3710\n",
      "Epoch [4900/5000], Loss: 2.1993\n",
      "Epoch [4910/5000], Loss: 2.2915\n",
      "Epoch [4920/5000], Loss: 2.9367\n",
      "Epoch [4930/5000], Loss: 2.3857\n",
      "Epoch [4940/5000], Loss: 2.1461\n",
      "Epoch [4950/5000], Loss: 2.0936\n",
      "Epoch [4960/5000], Loss: 2.1263\n",
      "Epoch [4970/5000], Loss: 2.2883\n",
      "Epoch [4980/5000], Loss: 1.8605\n",
      "Epoch [4990/5000], Loss: 2.0520\n",
      "Epoch [5000/5000], Loss: 1.9813\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T09:49:22.807645Z",
     "start_time": "2024-11-19T09:49:22.789630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 重新定义模型结构\n",
    "model = ANN(input_dim)  # 使用相同的模型结构\n",
    "\n",
    "# 加载模型参数，确保使用 weights_only=True 来提高安全性\n",
    "model.load_state_dict(torch.load(\"ann_model.pth\", weights_only=True))  # 加载模型参数\n",
    "\n",
    "# 评估模型 (确保在 GPU 上)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train_tensor).cpu().numpy()  # 转换为 CPU 数据，便于后续处理\n",
    "    y_test_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "from function import metrics_to_dataframe, plot_actual_vs_predicted\n",
    "# 绘制实际 vs 预测图\n",
    "# plot_actual_vs_predicted(y_train, y_train_pred, y_test, y_test_pred, 'Artificial Neural Network', 'ann.png')\n",
    "\n",
    "# 计算并显示评估指标\n",
    "ann_metrics = metrics_to_dataframe(y_train, y_train_pred, y_test, y_test_pred, 'ANN')\n",
    "ann_metrics.to_csv('ann_metrics.csv', index=False)\n",
    "ann_metrics"
   ],
   "id": "836f69cd10c28089",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  model  R2_train  MAE_train  MAPE_train  RMSE_train   R2_test  MAE_test  \\\n",
       "0   ANN  0.943932   5.739903    6.449725     9.45267  0.963017  5.347574   \n",
       "\n",
       "   MAPE_test  RMSE_test  \n",
       "0   6.180498   7.632015  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>RMSE_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.943932</td>\n",
       "      <td>5.739903</td>\n",
       "      <td>6.449725</td>\n",
       "      <td>9.45267</td>\n",
       "      <td>0.963017</td>\n",
       "      <td>5.347574</td>\n",
       "      <td>6.180498</td>\n",
       "      <td>7.632015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T10:12:33.321114Z",
     "start_time": "2024-11-19T10:12:33.317860Z"
    }
   },
   "cell_type": "code",
   "source": "y_train.shape, y_train_pred.shape",
   "id": "c977a4f7d1c0b56a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((480,), (480, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T10:13:29.804371Z",
     "start_time": "2024-11-19T10:13:29.798270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ann_train = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_pred.squeeze()})\n",
    "ann_test = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred.squeeze()})\n",
    "ann_train.to_csv('ann_train.csv', index=False)\n",
    "ann_test.to_csv('ann_test.csv', index=False)"
   ],
   "id": "ed6a24026aa81ef3",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
