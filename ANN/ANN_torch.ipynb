{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:40.136145Z",
     "start_time": "2024-11-20T11:10:38.184248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "bc27cd838cd0bf18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:41.454819Z",
     "start_time": "2024-11-20T11:10:41.451612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 自定义 MAPE 损失函数\n",
    "class MAPE_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        epsilon = 1e-8  # 避免除零\n",
    "        return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100\n",
    "\n",
    "# 自定义 RMSE 损失函数\n",
    "class RMSE_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return torch.sqrt(torch.mean((y_true - y_pred) ** 2))"
   ],
   "id": "1d631c13567504d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:44.032230Z",
     "start_time": "2024-11-20T11:10:43.928647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将数据转换为 PyTorch 张量并移动到 GPU\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "fe4554e4bc53015",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:47.947643Z",
     "start_time": "2024-11-20T11:10:47.096139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义 ANN 模型（确保在使用前定义）\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ANN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 12)\n",
    "        self.layer2 = nn.Linear(12, 90)\n",
    "        self.layer3 = nn.Linear(90, 90)\n",
    "        self.layer4 = nn.Linear(90, 60)\n",
    "        self.layer5 = nn.Linear(60, 70)\n",
    "        self.layer6 = nn.Linear(70, 30)\n",
    "        self.output = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = torch.relu(self.layer5(x))\n",
    "        x = torch.relu(self.layer6(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型并将其移到 GPU（如果可用）\n",
    "input_dim = X_train.shape[1]\n",
    "model = ANN(input_dim).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_function = MAPE_Loss().to(device)  # 自定义损失函数也要移到 GPU\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# # 设置 PyTorch 随机种子\n",
    "# def set_random_seed(seed):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)  # 如果使用 GPU\n",
    "#     np.random.seed(seed)  # 设置 Numpy 随机种子\n",
    "#     random.seed(seed)  # 设置 Python 原生随机数生成器的种子\n",
    "#     torch.backends.cudnn.deterministic = True  # 保证卷积操作的确定性\n",
    "#     torch.backends.cudnn.benchmark = False  # 禁用 cudnn 自动优化（用于固定输入尺寸）\n",
    "#\n",
    "# # 设置种子\n",
    "# set_random_seed(42)"
   ],
   "id": "eee97effb366ffda",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T08:49:01.944580Z",
     "start_time": "2024-11-19T08:48:04.559091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练模型+包含早停\n",
    "num_epochs = 3000\n",
    "patience = 100  # 允许的最大连续未改进 epoch 数\n",
    "best_loss = float('inf')  # 初始时验证损失设置为正无穷\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # 移动 batch 数据到 GPU\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = loss_function(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 打印每10个 epoch 的损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # # 计算验证损失\n",
    "    # model.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     # 在 GPU 上进行预测\n",
    "    #     y_val_pred = model(X_test_tensor).to(device)\n",
    "    #     # 验证损失计算时，确保 y_test_tensor 也在同一个设备上\n",
    "    #     y_test_tensor = y_test_tensor.to(device)\n",
    "    #     val_loss = loss_function(y_val_pred, y_test_tensor).item()  # 计算验证损失\n",
    "    #\n",
    "    # # 判断验证损失是否改善\n",
    "    # if val_loss < best_loss:\n",
    "    #     best_loss = val_loss\n",
    "    #     epochs_without_improvement = 0  # 重置计数器\n",
    "    #     # 保存最佳模型\n",
    "    #     torch.save(model.state_dict(), \"ann_best_model.pth\")\n",
    "    # else:\n",
    "    #     epochs_without_improvement += 1\n",
    "    #\n",
    "    # # 如果验证损失在一定次数的 epoch 内没有改进，则停止训练\n",
    "    # if epochs_without_improvement >= patience:\n",
    "    #     print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "    #     break\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"ann_best_model_GPU_3000.pth\")"
   ],
   "id": "4d64b3e9b98a3ea7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/2000], Loss: 35.0127\n",
      "Epoch [20/2000], Loss: 25.7144\n",
      "Epoch [30/2000], Loss: 21.1842\n",
      "Epoch [40/2000], Loss: 19.0967\n",
      "Epoch [50/2000], Loss: 16.1477\n",
      "Epoch [60/2000], Loss: 15.2323\n",
      "Epoch [70/2000], Loss: 13.6890\n",
      "Epoch [80/2000], Loss: 12.5402\n",
      "Epoch [90/2000], Loss: 12.0824\n",
      "Epoch [100/2000], Loss: 12.2371\n",
      "Epoch [110/2000], Loss: 11.6854\n",
      "Epoch [120/2000], Loss: 10.8500\n",
      "Epoch [130/2000], Loss: 11.3526\n",
      "Epoch [140/2000], Loss: 10.6842\n",
      "Epoch [150/2000], Loss: 10.1860\n",
      "Epoch [160/2000], Loss: 10.1437\n",
      "Epoch [170/2000], Loss: 9.6035\n",
      "Epoch [180/2000], Loss: 9.7366\n",
      "Epoch [190/2000], Loss: 9.7357\n",
      "Epoch [200/2000], Loss: 9.5365\n",
      "Epoch [210/2000], Loss: 9.2177\n",
      "Epoch [220/2000], Loss: 9.3579\n",
      "Epoch [230/2000], Loss: 9.1264\n",
      "Epoch [240/2000], Loss: 9.5169\n",
      "Epoch [250/2000], Loss: 9.1272\n",
      "Epoch [260/2000], Loss: 8.4903\n",
      "Epoch [270/2000], Loss: 8.5847\n",
      "Epoch [280/2000], Loss: 8.4329\n",
      "Epoch [290/2000], Loss: 8.3989\n",
      "Epoch [300/2000], Loss: 8.8839\n",
      "Epoch [310/2000], Loss: 8.4185\n",
      "Epoch [320/2000], Loss: 8.5830\n",
      "Epoch [330/2000], Loss: 8.1344\n",
      "Epoch [340/2000], Loss: 8.4750\n",
      "Epoch [350/2000], Loss: 7.8511\n",
      "Epoch [360/2000], Loss: 7.8897\n",
      "Epoch [370/2000], Loss: 8.3519\n",
      "Epoch [380/2000], Loss: 7.7561\n",
      "Epoch [390/2000], Loss: 7.5443\n",
      "Epoch [400/2000], Loss: 7.2906\n",
      "Epoch [410/2000], Loss: 8.1683\n",
      "Epoch [420/2000], Loss: 7.0594\n",
      "Epoch [430/2000], Loss: 8.1155\n",
      "Epoch [440/2000], Loss: 7.6752\n",
      "Epoch [450/2000], Loss: 7.8131\n",
      "Epoch [460/2000], Loss: 7.1760\n",
      "Epoch [470/2000], Loss: 7.1639\n",
      "Epoch [480/2000], Loss: 7.4162\n",
      "Epoch [490/2000], Loss: 7.1015\n",
      "Epoch [500/2000], Loss: 7.3524\n",
      "Epoch [510/2000], Loss: 6.9547\n",
      "Epoch [520/2000], Loss: 6.9469\n",
      "Epoch [530/2000], Loss: 6.6604\n",
      "Epoch [540/2000], Loss: 6.9947\n",
      "Epoch [550/2000], Loss: 6.5404\n",
      "Epoch [560/2000], Loss: 6.6849\n",
      "Epoch [570/2000], Loss: 6.7243\n",
      "Epoch [580/2000], Loss: 7.0643\n",
      "Epoch [590/2000], Loss: 6.8338\n",
      "Epoch [600/2000], Loss: 6.3445\n",
      "Epoch [610/2000], Loss: 6.4171\n",
      "Epoch [620/2000], Loss: 6.4848\n",
      "Epoch [630/2000], Loss: 6.3905\n",
      "Epoch [640/2000], Loss: 6.4780\n",
      "Epoch [650/2000], Loss: 6.6219\n",
      "Epoch [660/2000], Loss: 6.4576\n",
      "Epoch [670/2000], Loss: 6.0636\n",
      "Epoch [680/2000], Loss: 5.9049\n",
      "Epoch [690/2000], Loss: 6.8500\n",
      "Epoch [700/2000], Loss: 5.9716\n",
      "Epoch [710/2000], Loss: 6.2509\n",
      "Epoch [720/2000], Loss: 5.9873\n",
      "Epoch [730/2000], Loss: 6.3285\n",
      "Epoch [740/2000], Loss: 6.0482\n",
      "Epoch [750/2000], Loss: 5.9688\n",
      "Epoch [760/2000], Loss: 6.1211\n",
      "Epoch [770/2000], Loss: 6.1117\n",
      "Epoch [780/2000], Loss: 6.2084\n",
      "Epoch [790/2000], Loss: 6.5779\n",
      "Epoch [800/2000], Loss: 6.0317\n",
      "Epoch [810/2000], Loss: 5.6764\n",
      "Epoch [820/2000], Loss: 5.8769\n",
      "Epoch [830/2000], Loss: 5.7623\n",
      "Epoch [840/2000], Loss: 5.7856\n",
      "Epoch [850/2000], Loss: 6.0242\n",
      "Epoch [860/2000], Loss: 5.6382\n",
      "Epoch [870/2000], Loss: 5.8660\n",
      "Epoch [880/2000], Loss: 5.7306\n",
      "Epoch [890/2000], Loss: 5.8485\n",
      "Epoch [900/2000], Loss: 5.2982\n",
      "Epoch [910/2000], Loss: 5.8247\n",
      "Epoch [920/2000], Loss: 6.0671\n",
      "Epoch [930/2000], Loss: 5.2803\n",
      "Epoch [940/2000], Loss: 6.0545\n",
      "Epoch [950/2000], Loss: 5.3621\n",
      "Epoch [960/2000], Loss: 5.9914\n",
      "Epoch [970/2000], Loss: 5.3566\n",
      "Epoch [980/2000], Loss: 5.2485\n",
      "Epoch [990/2000], Loss: 5.3123\n",
      "Epoch [1000/2000], Loss: 5.6140\n",
      "Epoch [1010/2000], Loss: 5.4989\n",
      "Epoch [1020/2000], Loss: 5.3098\n",
      "Epoch [1030/2000], Loss: 5.7038\n",
      "Epoch [1040/2000], Loss: 5.6972\n",
      "Epoch [1050/2000], Loss: 6.0455\n",
      "Epoch [1060/2000], Loss: 5.6600\n",
      "Epoch [1070/2000], Loss: 5.2130\n",
      "Epoch [1080/2000], Loss: 5.8275\n",
      "Epoch [1090/2000], Loss: 5.3548\n",
      "Epoch [1100/2000], Loss: 5.4491\n",
      "Epoch [1110/2000], Loss: 5.3640\n",
      "Epoch [1120/2000], Loss: 5.1671\n",
      "Epoch [1130/2000], Loss: 4.9955\n",
      "Epoch [1140/2000], Loss: 5.1264\n",
      "Epoch [1150/2000], Loss: 5.8610\n",
      "Epoch [1160/2000], Loss: 5.3636\n",
      "Epoch [1170/2000], Loss: 5.1849\n",
      "Epoch [1180/2000], Loss: 5.2312\n",
      "Epoch [1190/2000], Loss: 5.1557\n",
      "Epoch [1200/2000], Loss: 5.1501\n",
      "Epoch [1210/2000], Loss: 5.0894\n",
      "Epoch [1220/2000], Loss: 6.0666\n",
      "Epoch [1230/2000], Loss: 5.0781\n",
      "Epoch [1240/2000], Loss: 5.4158\n",
      "Epoch [1250/2000], Loss: 5.9286\n",
      "Epoch [1260/2000], Loss: 5.0769\n",
      "Epoch [1270/2000], Loss: 4.7970\n",
      "Epoch [1280/2000], Loss: 5.8166\n",
      "Epoch [1290/2000], Loss: 5.0453\n",
      "Epoch [1300/2000], Loss: 4.9453\n",
      "Epoch [1310/2000], Loss: 5.9729\n",
      "Epoch [1320/2000], Loss: 4.8621\n",
      "Epoch [1330/2000], Loss: 5.8246\n",
      "Epoch [1340/2000], Loss: 5.2744\n",
      "Epoch [1350/2000], Loss: 4.8773\n",
      "Epoch [1360/2000], Loss: 4.8640\n",
      "Epoch [1370/2000], Loss: 4.9538\n",
      "Epoch [1380/2000], Loss: 4.5279\n",
      "Epoch [1390/2000], Loss: 4.5357\n",
      "Epoch [1400/2000], Loss: 4.8661\n",
      "Epoch [1410/2000], Loss: 4.4763\n",
      "Epoch [1420/2000], Loss: 5.1235\n",
      "Epoch [1430/2000], Loss: 4.7934\n",
      "Epoch [1440/2000], Loss: 4.6112\n",
      "Epoch [1450/2000], Loss: 4.5024\n",
      "Epoch [1460/2000], Loss: 4.7960\n",
      "Epoch [1470/2000], Loss: 4.3280\n",
      "Epoch [1480/2000], Loss: 4.7472\n",
      "Epoch [1490/2000], Loss: 4.5953\n",
      "Epoch [1500/2000], Loss: 4.6974\n",
      "Epoch [1510/2000], Loss: 4.4684\n",
      "Epoch [1520/2000], Loss: 5.0239\n",
      "Epoch [1530/2000], Loss: 4.4390\n",
      "Epoch [1540/2000], Loss: 4.6568\n",
      "Epoch [1550/2000], Loss: 4.5495\n",
      "Epoch [1560/2000], Loss: 4.5555\n",
      "Epoch [1570/2000], Loss: 4.7007\n",
      "Epoch [1580/2000], Loss: 4.4744\n",
      "Epoch [1590/2000], Loss: 4.9354\n",
      "Epoch [1600/2000], Loss: 4.5089\n",
      "Epoch [1610/2000], Loss: 4.3676\n",
      "Epoch [1620/2000], Loss: 4.2643\n",
      "Epoch [1630/2000], Loss: 4.7403\n",
      "Epoch [1640/2000], Loss: 4.2971\n",
      "Epoch [1650/2000], Loss: 4.2735\n",
      "Epoch [1660/2000], Loss: 4.8008\n",
      "Epoch [1670/2000], Loss: 4.3862\n",
      "Epoch [1680/2000], Loss: 3.8737\n",
      "Epoch [1690/2000], Loss: 4.4594\n",
      "Epoch [1700/2000], Loss: 4.4799\n",
      "Epoch [1710/2000], Loss: 4.5254\n",
      "Epoch [1720/2000], Loss: 4.2944\n",
      "Epoch [1730/2000], Loss: 4.1471\n",
      "Epoch [1740/2000], Loss: 4.0463\n",
      "Epoch [1750/2000], Loss: 4.2032\n",
      "Epoch [1760/2000], Loss: 4.1153\n",
      "Epoch [1770/2000], Loss: 4.0819\n",
      "Epoch [1780/2000], Loss: 4.2595\n",
      "Epoch [1790/2000], Loss: 4.3177\n",
      "Epoch [1800/2000], Loss: 3.9189\n",
      "Epoch [1810/2000], Loss: 3.8951\n",
      "Epoch [1820/2000], Loss: 4.3668\n",
      "Epoch [1830/2000], Loss: 4.0828\n",
      "Epoch [1840/2000], Loss: 3.9711\n",
      "Epoch [1850/2000], Loss: 4.0301\n",
      "Epoch [1860/2000], Loss: 3.7685\n",
      "Epoch [1870/2000], Loss: 3.7371\n",
      "Epoch [1880/2000], Loss: 3.7897\n",
      "Epoch [1890/2000], Loss: 4.3419\n",
      "Epoch [1900/2000], Loss: 3.8384\n",
      "Epoch [1910/2000], Loss: 3.9342\n",
      "Epoch [1920/2000], Loss: 3.9283\n",
      "Epoch [1930/2000], Loss: 4.0061\n",
      "Epoch [1940/2000], Loss: 3.7534\n",
      "Epoch [1950/2000], Loss: 3.6262\n",
      "Epoch [1960/2000], Loss: 3.8474\n",
      "Epoch [1970/2000], Loss: 3.6390\n",
      "Epoch [1980/2000], Loss: 4.1244\n",
      "Epoch [1990/2000], Loss: 3.6272\n",
      "Epoch [2000/2000], Loss: 4.3335\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:52.873302Z",
     "start_time": "2024-11-20T11:10:52.387659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe\n",
    "\n",
    "# 重新定义模型结构\n",
    "model = ANN(input_dim).to(device)  # 使用相同的模型结构\n",
    "\n",
    "# 加载模型参数，确保使用 weights_only=True 来提高安全性\n",
    "model.load_state_dict(torch.load(\"ann_model.pth\", weights_only=True))  # 加载模型参数\n",
    "\n",
    "# 评估模型 (确保在 GPU 上)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_train_pred = model(X_train_tensor).cpu().numpy()  # 转换为 CPU 数据，便于后续处理\n",
    "    y_test_pred = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "# 计算并显示评估指标\n",
    "ann_metrics = metrics_to_dataframe(y_train, y_train_pred, y_test, y_test_pred, 'ANN')\n",
    "ann_metrics"
   ],
   "id": "877ba666a9f8f737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  model  R2_train  MAE_train  MAPE_train  RMSE_train   R2_test  MAE_test  \\\n",
       "0   ANN  0.943932   5.739903    6.449726    9.452667  0.963017  5.347572   \n",
       "\n",
       "   MAPE_test  RMSE_test  \n",
       "0   6.180497   7.632014  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>RMSE_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANN</td>\n",
       "      <td>0.943932</td>\n",
       "      <td>5.739903</td>\n",
       "      <td>6.449726</td>\n",
       "      <td>9.452667</td>\n",
       "      <td>0.963017</td>\n",
       "      <td>5.347572</td>\n",
       "      <td>6.180497</td>\n",
       "      <td>7.632014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:10:56.286734Z",
     "start_time": "2024-11-20T11:10:56.280750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ann_train = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_pred.squeeze()})\n",
    "ann_test = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred.squeeze()})\n",
    "ann_train.to_csv('ann_train.csv', index=False)\n",
    "ann_test.to_csv('ann_test.csv', index=False)"
   ],
   "id": "255cf4b834ba9c75",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
