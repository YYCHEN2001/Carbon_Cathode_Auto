{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 准备数据",
   "id": "483dc5fd0971e2b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:02:33.466539Z",
     "start_time": "2024-11-20T11:02:31.482410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8874c47a571954c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:04:54.329721Z",
     "start_time": "2024-11-20T11:04:54.184911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "X_train_categ = X_train[:, 8]  # 第九列为类别特征\n",
    "X_train_cont = np.delete(X_train, 8, axis=1)  # 删除第九列，其他为连续特征\n",
    "\n",
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "X_train_categ_torch = torch.tensor(X_train_categ, dtype=torch.long)  # 类别特征需要使用长整型\n",
    "X_train_categ_torch = X_train_categ_torch.unsqueeze(1).to(device)  # 在最后一个维度添加1\n",
    "X_train_cont_torch = torch.tensor(X_train_cont, dtype=torch.float).to(device)  # 连续特征使用浮点型\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float)  # 对于回归问题，通常使用浮点数\n",
    "y_train_torch = y_train_torch.unsqueeze(1).to(device)\n",
    "\n",
    "# 计算连续特征的均值和标准差\n",
    "mean = X_train_cont_torch.mean(dim=0)\n",
    "std = X_train_cont_torch.std(dim=0)\n",
    "continuous_mean_std = torch.stack([mean, std], dim=1).to(device)\n",
    "\n",
    "# 处理测试集\n",
    "X_test_categ = X_test[:, 8]\n",
    "X_test_cont = np.delete(X_test, 8, axis=1)\n",
    "X_test_categ_torch = torch.tensor(X_test_categ, dtype=torch.long)\n",
    "X_test_categ_torch = X_test_categ_torch.unsqueeze(1).to(device)\n",
    "X_test_cont_torch = torch.tensor(X_test_cont, dtype=torch.float).to(device)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float)\n",
    "y_test_torch = y_test_torch.unsqueeze(1).to(device)"
   ],
   "id": "3499816898be9093",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 自定义 MAPE 损失函数",
   "id": "ae2df725710a7b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:04:57.253038Z",
     "start_time": "2024-11-20T11:04:57.250510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MAPELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        epsilon = 1e-8  # 避免除以零\n",
    "        mape = torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 100\n",
    "        return mape"
   ],
   "id": "31c120045a049623",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义模型",
   "id": "53feb800f58be3f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:05:00.473530Z",
     "start_time": "2024-11-20T11:04:59.608828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "\n",
    "# 我们有12个特征，其中有1个类别特征，11个连续值特征\n",
    "# 类别特征每个有2个唯一值\n",
    "categories = (2,)\n",
    "num_continuous = 11\n",
    "\n",
    "# 初始化 TabTransformer 模型\n",
    "model = TabTransformer(\n",
    "    categories=categories,\n",
    "    num_continuous=num_continuous,\n",
    "    dim=16,  # 默认维度为32\n",
    "    dim_out=1,  # 回归问题的输出维度为1\n",
    "    depth=6,  # 默认深度为6\n",
    "    heads=8,  # 注意力机制的头数\n",
    "    attn_dropout=0.01,  # 注意力机制的dropout\n",
    "    ff_dropout=0.01,  # 前馈网络的的dropout\n",
    "    mlp_hidden_mults=(1, 2, 4, 1),  # MLP隐藏层的倍数\n",
    "    mlp_act=nn.ReLU(),  # MLP的激活函数, 默认为ReLU\n",
    "    continuous_mean_std=continuous_mean_std,  # 连续值的均值和标准差\n",
    ")\n",
    "\n",
    "# 将模型移动到 GPU\n",
    "model.to(device)\n",
    "\n",
    "# 初始化损失函数\n",
    "# mse_loss = nn.MSELoss()\n",
    "mape_loss = MAPELoss().to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "f3b60050a04f7e4f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 训练模型",
   "id": "19f0115a86873316"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:05:31.526229Z",
     "start_time": "2024-11-20T11:05:02.074488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe, calculate_metrics\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_categ_torch, X_train_cont_torch)\n",
    "    loss = mape_loss(outputs, y_train_torch)  # 使用MSE损失函数\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cumulative_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "        cumulative_loss = 0.0  # 重置累积损失\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # 保存最佳模型"
   ],
   "id": "8f283be2c8de26be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 99.66233291625977\n",
      "Epoch 20, Average Loss: 96.31603393554687\n",
      "Epoch 30, Average Loss: 75.8355499267578\n",
      "Epoch 40, Average Loss: 45.06487274169922\n",
      "Epoch 50, Average Loss: 41.079642105102536\n",
      "Epoch 60, Average Loss: 39.854240798950194\n",
      "Epoch 70, Average Loss: 39.140023422241214\n",
      "Epoch 80, Average Loss: 38.6615104675293\n",
      "Epoch 90, Average Loss: 38.17184333801269\n",
      "Epoch 100, Average Loss: 37.78788795471191\n",
      "Epoch 110, Average Loss: 37.610953903198244\n",
      "Epoch 120, Average Loss: 37.502573013305664\n",
      "Epoch 130, Average Loss: 37.39039840698242\n",
      "Epoch 140, Average Loss: 37.30012092590332\n",
      "Epoch 150, Average Loss: 37.19655647277832\n",
      "Epoch 160, Average Loss: 37.064575576782225\n",
      "Epoch 170, Average Loss: 36.871530532836914\n",
      "Epoch 180, Average Loss: 36.77138175964355\n",
      "Epoch 190, Average Loss: 36.535007858276366\n",
      "Epoch 200, Average Loss: 36.35811195373535\n",
      "Epoch 210, Average Loss: 36.12175025939941\n",
      "Epoch 220, Average Loss: 35.83407440185547\n",
      "Epoch 230, Average Loss: 35.472109985351565\n",
      "Epoch 240, Average Loss: 35.0392505645752\n",
      "Epoch 250, Average Loss: 34.55082740783691\n",
      "Epoch 260, Average Loss: 33.87090873718262\n",
      "Epoch 270, Average Loss: 33.016532516479494\n",
      "Epoch 280, Average Loss: 32.057111740112305\n",
      "Epoch 290, Average Loss: 31.031242179870606\n",
      "Epoch 300, Average Loss: 30.341669654846193\n",
      "Epoch 310, Average Loss: 29.395827293395996\n",
      "Epoch 320, Average Loss: 28.94639720916748\n",
      "Epoch 330, Average Loss: 28.327436637878417\n",
      "Epoch 340, Average Loss: 27.875756454467773\n",
      "Epoch 350, Average Loss: 27.600375366210937\n",
      "Epoch 360, Average Loss: 27.335105895996094\n",
      "Epoch 370, Average Loss: 27.124320411682127\n",
      "Epoch 380, Average Loss: 26.89671859741211\n",
      "Epoch 390, Average Loss: 26.595882606506347\n",
      "Epoch 400, Average Loss: 26.299778938293457\n",
      "Epoch 410, Average Loss: 26.01399211883545\n",
      "Epoch 420, Average Loss: 25.792030906677248\n",
      "Epoch 430, Average Loss: 25.45433406829834\n",
      "Epoch 440, Average Loss: 25.081689643859864\n",
      "Epoch 450, Average Loss: 24.724449729919435\n",
      "Epoch 460, Average Loss: 24.315576362609864\n",
      "Epoch 470, Average Loss: 24.003277206420897\n",
      "Epoch 480, Average Loss: 23.74119815826416\n",
      "Epoch 490, Average Loss: 23.430583953857422\n",
      "Epoch 500, Average Loss: 23.130675315856934\n",
      "Epoch 510, Average Loss: 22.864001274108887\n",
      "Epoch 520, Average Loss: 22.62993106842041\n",
      "Epoch 530, Average Loss: 22.3580472946167\n",
      "Epoch 540, Average Loss: 22.08867664337158\n",
      "Epoch 550, Average Loss: 21.79174518585205\n",
      "Epoch 560, Average Loss: 21.48448715209961\n",
      "Epoch 570, Average Loss: 21.25261402130127\n",
      "Epoch 580, Average Loss: 20.97602062225342\n",
      "Epoch 590, Average Loss: 20.68904342651367\n",
      "Epoch 600, Average Loss: 20.420399856567382\n",
      "Epoch 610, Average Loss: 20.1091646194458\n",
      "Epoch 620, Average Loss: 19.811656761169434\n",
      "Epoch 630, Average Loss: 19.54915771484375\n",
      "Epoch 640, Average Loss: 19.133824348449707\n",
      "Epoch 650, Average Loss: 18.863467407226562\n",
      "Epoch 660, Average Loss: 18.46881275177002\n",
      "Epoch 670, Average Loss: 17.995189666748047\n",
      "Epoch 680, Average Loss: 17.712803077697753\n",
      "Epoch 690, Average Loss: 17.35270118713379\n",
      "Epoch 700, Average Loss: 17.059897232055665\n",
      "Epoch 710, Average Loss: 16.649790000915527\n",
      "Epoch 720, Average Loss: 16.27711429595947\n",
      "Epoch 730, Average Loss: 15.861723041534423\n",
      "Epoch 740, Average Loss: 15.46054391860962\n",
      "Epoch 750, Average Loss: 15.095843601226807\n",
      "Epoch 760, Average Loss: 14.704534530639648\n",
      "Epoch 770, Average Loss: 14.40464382171631\n",
      "Epoch 780, Average Loss: 14.112391757965089\n",
      "Epoch 790, Average Loss: 13.65805196762085\n",
      "Epoch 800, Average Loss: 13.4379807472229\n",
      "Epoch 810, Average Loss: 12.996198654174805\n",
      "Epoch 820, Average Loss: 12.563050651550293\n",
      "Epoch 830, Average Loss: 12.35077543258667\n",
      "Epoch 840, Average Loss: 12.044362354278565\n",
      "Epoch 850, Average Loss: 11.615954494476318\n",
      "Epoch 860, Average Loss: 11.281466865539551\n",
      "Epoch 870, Average Loss: 11.010341930389405\n",
      "Epoch 880, Average Loss: 10.607643413543702\n",
      "Epoch 890, Average Loss: 10.496222114562988\n",
      "Epoch 900, Average Loss: 10.506608772277833\n",
      "Epoch 910, Average Loss: 9.965732860565186\n",
      "Epoch 920, Average Loss: 9.607590675354004\n",
      "Epoch 930, Average Loss: 9.380349731445312\n",
      "Epoch 940, Average Loss: 9.267264270782471\n",
      "Epoch 950, Average Loss: 9.09450922012329\n",
      "Epoch 960, Average Loss: 8.813623332977295\n",
      "Epoch 970, Average Loss: 8.608421230316162\n",
      "Epoch 980, Average Loss: 8.65508575439453\n",
      "Epoch 990, Average Loss: 8.575927543640137\n",
      "Epoch 1000, Average Loss: 8.493639087677002\n",
      "Epoch 1010, Average Loss: 8.286765193939209\n",
      "Epoch 1020, Average Loss: 8.238129138946533\n",
      "Epoch 1030, Average Loss: 8.154922485351562\n",
      "Epoch 1040, Average Loss: 8.068662452697755\n",
      "Epoch 1050, Average Loss: 7.926713514328003\n",
      "Epoch 1060, Average Loss: 7.752305746078491\n",
      "Epoch 1070, Average Loss: 7.702932834625244\n",
      "Epoch 1080, Average Loss: 7.664324855804443\n",
      "Epoch 1090, Average Loss: 7.521715259552002\n",
      "Epoch 1100, Average Loss: 7.420448970794678\n",
      "Epoch 1110, Average Loss: 7.356210851669312\n",
      "Epoch 1120, Average Loss: 7.2582086563110355\n",
      "Epoch 1130, Average Loss: 7.133285236358643\n",
      "Epoch 1140, Average Loss: 7.261513566970825\n",
      "Epoch 1150, Average Loss: 7.113850116729736\n",
      "Epoch 1160, Average Loss: 7.07195634841919\n",
      "Epoch 1170, Average Loss: 6.901876735687256\n",
      "Epoch 1180, Average Loss: 6.766879987716675\n",
      "Epoch 1190, Average Loss: 6.78699107170105\n",
      "Epoch 1200, Average Loss: 6.748867702484131\n",
      "Epoch 1210, Average Loss: 6.667290544509887\n",
      "Epoch 1220, Average Loss: 6.713166427612305\n",
      "Epoch 1230, Average Loss: 6.598296070098877\n",
      "Epoch 1240, Average Loss: 6.557660055160523\n",
      "Epoch 1250, Average Loss: 6.545057249069214\n",
      "Epoch 1260, Average Loss: 6.521719598770142\n",
      "Epoch 1270, Average Loss: 6.459305191040039\n",
      "Epoch 1280, Average Loss: 6.589108228683472\n",
      "Epoch 1290, Average Loss: 6.613107681274414\n",
      "Epoch 1300, Average Loss: 6.53066873550415\n",
      "Epoch 1310, Average Loss: 6.307117700576782\n",
      "Epoch 1320, Average Loss: 6.167324829101562\n",
      "Epoch 1330, Average Loss: 6.233343124389648\n",
      "Epoch 1340, Average Loss: 6.157671737670898\n",
      "Epoch 1350, Average Loss: 6.108226728439331\n",
      "Epoch 1360, Average Loss: 6.133621549606323\n",
      "Epoch 1370, Average Loss: 6.0327986717224125\n",
      "Epoch 1380, Average Loss: 6.040268850326538\n",
      "Epoch 1390, Average Loss: 5.988739967346191\n",
      "Epoch 1400, Average Loss: 6.062596607208252\n",
      "Epoch 1410, Average Loss: 5.925437545776367\n",
      "Epoch 1420, Average Loss: 5.891968870162964\n",
      "Epoch 1430, Average Loss: 5.868263626098633\n",
      "Epoch 1440, Average Loss: 5.8224420070648195\n",
      "Epoch 1450, Average Loss: 5.844417572021484\n",
      "Epoch 1460, Average Loss: 5.802791690826416\n",
      "Epoch 1470, Average Loss: 5.70872015953064\n",
      "Epoch 1480, Average Loss: 5.706023931503296\n",
      "Epoch 1490, Average Loss: 5.784300804138184\n",
      "Epoch 1500, Average Loss: 5.697584962844848\n",
      "Epoch 1510, Average Loss: 5.615757894515991\n",
      "Epoch 1520, Average Loss: 5.63879714012146\n",
      "Epoch 1530, Average Loss: 5.746784925460815\n",
      "Epoch 1540, Average Loss: 5.631732559204101\n",
      "Epoch 1550, Average Loss: 5.580820989608765\n",
      "Epoch 1560, Average Loss: 5.6491962432861325\n",
      "Epoch 1570, Average Loss: 5.76665472984314\n",
      "Epoch 1580, Average Loss: 5.6691254615783695\n",
      "Epoch 1590, Average Loss: 5.865174341201782\n",
      "Epoch 1600, Average Loss: 5.712487506866455\n",
      "Epoch 1610, Average Loss: 5.508996868133545\n",
      "Epoch 1620, Average Loss: 5.453526830673217\n",
      "Epoch 1630, Average Loss: 5.438686323165894\n",
      "Epoch 1640, Average Loss: 5.428111791610718\n",
      "Epoch 1650, Average Loss: 5.438029670715332\n",
      "Epoch 1660, Average Loss: 5.433563566207885\n",
      "Epoch 1670, Average Loss: 5.328539609909058\n",
      "Epoch 1680, Average Loss: 5.332838106155395\n",
      "Epoch 1690, Average Loss: 5.510016679763794\n",
      "Epoch 1700, Average Loss: 5.4764689922332765\n",
      "Epoch 1710, Average Loss: 5.394122886657715\n",
      "Epoch 1720, Average Loss: 5.343861484527588\n",
      "Epoch 1730, Average Loss: 5.28058557510376\n",
      "Epoch 1740, Average Loss: 5.28706202507019\n",
      "Epoch 1750, Average Loss: 5.419620990753174\n",
      "Epoch 1760, Average Loss: 5.421617555618286\n",
      "Epoch 1770, Average Loss: 5.283020877838135\n",
      "Epoch 1780, Average Loss: 5.182836675643921\n",
      "Epoch 1790, Average Loss: 5.168252849578858\n",
      "Epoch 1800, Average Loss: 5.158405017852783\n",
      "Epoch 1810, Average Loss: 5.192702388763427\n",
      "Epoch 1820, Average Loss: 5.166600894927979\n",
      "Epoch 1830, Average Loss: 5.174418973922729\n",
      "Epoch 1840, Average Loss: 5.133933782577515\n",
      "Epoch 1850, Average Loss: 5.246309280395508\n",
      "Epoch 1860, Average Loss: 5.328768348693847\n",
      "Epoch 1870, Average Loss: 5.266596841812134\n",
      "Epoch 1880, Average Loss: 5.170296955108642\n",
      "Epoch 1890, Average Loss: 5.036823701858521\n",
      "Epoch 1900, Average Loss: 5.073140621185303\n",
      "Epoch 1910, Average Loss: 5.122524881362915\n",
      "Epoch 1920, Average Loss: 5.092253494262695\n",
      "Epoch 1930, Average Loss: 4.952061176300049\n",
      "Epoch 1940, Average Loss: 4.957456254959107\n",
      "Epoch 1950, Average Loss: 5.054289722442627\n",
      "Epoch 1960, Average Loss: 5.083720397949219\n",
      "Epoch 1970, Average Loss: 5.155476188659668\n",
      "Epoch 1980, Average Loss: 5.132939863204956\n",
      "Epoch 1990, Average Loss: 5.149139261245727\n",
      "Epoch 2000, Average Loss: 5.083431148529053\n",
      "Epoch 2010, Average Loss: 4.956036949157715\n",
      "Epoch 2020, Average Loss: 4.963653039932251\n",
      "Epoch 2030, Average Loss: 4.8746177673339846\n",
      "Epoch 2040, Average Loss: 4.866721773147583\n",
      "Epoch 2050, Average Loss: 4.866479206085205\n",
      "Epoch 2060, Average Loss: 4.855594062805176\n",
      "Epoch 2070, Average Loss: 4.887468481063843\n",
      "Epoch 2080, Average Loss: 4.764832782745361\n",
      "Epoch 2090, Average Loss: 4.972427892684936\n",
      "Epoch 2100, Average Loss: 5.131566953659058\n",
      "Epoch 2110, Average Loss: 5.057041835784912\n",
      "Epoch 2120, Average Loss: 4.891595125198364\n",
      "Epoch 2130, Average Loss: 4.79983172416687\n",
      "Epoch 2140, Average Loss: 4.805652904510498\n",
      "Epoch 2150, Average Loss: 4.746325397491455\n",
      "Epoch 2160, Average Loss: 4.746870040893555\n",
      "Epoch 2170, Average Loss: 4.76460280418396\n",
      "Epoch 2180, Average Loss: 4.7156233310699465\n",
      "Epoch 2190, Average Loss: 4.718688726425171\n",
      "Epoch 2200, Average Loss: 4.718669176101685\n",
      "Epoch 2210, Average Loss: 4.714271593093872\n",
      "Epoch 2220, Average Loss: 4.808971118927002\n",
      "Epoch 2230, Average Loss: 4.9642589569091795\n",
      "Epoch 2240, Average Loss: 4.769198608398438\n",
      "Epoch 2250, Average Loss: 4.689593172073364\n",
      "Epoch 2260, Average Loss: 4.649878597259521\n",
      "Epoch 2270, Average Loss: 4.63389778137207\n",
      "Epoch 2280, Average Loss: 4.659183692932129\n",
      "Epoch 2290, Average Loss: 4.637111330032349\n",
      "Epoch 2300, Average Loss: 4.625682258605957\n",
      "Epoch 2310, Average Loss: 4.632523393630981\n",
      "Epoch 2320, Average Loss: 4.636526203155517\n",
      "Epoch 2330, Average Loss: 4.646509504318237\n",
      "Epoch 2340, Average Loss: 4.668514108657837\n",
      "Epoch 2350, Average Loss: 4.61545238494873\n",
      "Epoch 2360, Average Loss: 4.542473077774048\n",
      "Epoch 2370, Average Loss: 4.543833780288696\n",
      "Epoch 2380, Average Loss: 4.594504261016846\n",
      "Epoch 2390, Average Loss: 4.577617120742798\n",
      "Epoch 2400, Average Loss: 4.585318946838379\n",
      "Epoch 2410, Average Loss: 4.576482534408569\n",
      "Epoch 2420, Average Loss: 4.54211277961731\n",
      "Epoch 2430, Average Loss: 4.746660900115967\n",
      "Epoch 2440, Average Loss: 4.770678567886352\n",
      "Epoch 2450, Average Loss: 4.6016278743743895\n",
      "Epoch 2460, Average Loss: 4.543806028366089\n",
      "Epoch 2470, Average Loss: 4.466010570526123\n",
      "Epoch 2480, Average Loss: 4.478297090530395\n",
      "Epoch 2490, Average Loss: 4.536568641662598\n",
      "Epoch 2500, Average Loss: 4.562859916687012\n",
      "Epoch 2510, Average Loss: 4.534542369842529\n",
      "Epoch 2520, Average Loss: 4.5143555164337155\n",
      "Epoch 2530, Average Loss: 4.440971040725708\n",
      "Epoch 2540, Average Loss: 4.577418088912964\n",
      "Epoch 2550, Average Loss: 4.623633480072021\n",
      "Epoch 2560, Average Loss: 4.44913535118103\n",
      "Epoch 2570, Average Loss: 4.436407470703125\n",
      "Epoch 2580, Average Loss: 4.3801618099212645\n",
      "Epoch 2590, Average Loss: 4.447665405273438\n",
      "Epoch 2600, Average Loss: 4.57145676612854\n",
      "Epoch 2610, Average Loss: 4.477939176559448\n",
      "Epoch 2620, Average Loss: 4.390271377563477\n",
      "Epoch 2630, Average Loss: 4.393833446502685\n",
      "Epoch 2640, Average Loss: 4.395113229751587\n",
      "Epoch 2650, Average Loss: 4.5174743175506595\n",
      "Epoch 2660, Average Loss: 4.496534395217895\n",
      "Epoch 2670, Average Loss: 4.438088274002075\n",
      "Epoch 2680, Average Loss: 4.358034992218018\n",
      "Epoch 2690, Average Loss: 4.385435914993286\n",
      "Epoch 2700, Average Loss: 4.382738447189331\n",
      "Epoch 2710, Average Loss: 4.424600601196289\n",
      "Epoch 2720, Average Loss: 4.346285343170166\n",
      "Epoch 2730, Average Loss: 4.291564893722534\n",
      "Epoch 2740, Average Loss: 4.310837984085083\n",
      "Epoch 2750, Average Loss: 4.294984340667725\n",
      "Epoch 2760, Average Loss: 4.2970562934875485\n",
      "Epoch 2770, Average Loss: 4.363150882720947\n",
      "Epoch 2780, Average Loss: 4.285910558700562\n",
      "Epoch 2790, Average Loss: 4.263568592071533\n",
      "Epoch 2800, Average Loss: 4.217009019851685\n",
      "Epoch 2810, Average Loss: 4.207194423675537\n",
      "Epoch 2820, Average Loss: 4.3133142471313475\n",
      "Epoch 2830, Average Loss: 4.279491758346557\n",
      "Epoch 2840, Average Loss: 4.261506700515747\n",
      "Epoch 2850, Average Loss: 4.270095014572144\n",
      "Epoch 2860, Average Loss: 4.2685198307037355\n",
      "Epoch 2870, Average Loss: 4.240319633483887\n",
      "Epoch 2880, Average Loss: 4.303748416900635\n",
      "Epoch 2890, Average Loss: 4.443958854675293\n",
      "Epoch 2900, Average Loss: 4.366132974624634\n",
      "Epoch 2910, Average Loss: 4.242399978637695\n",
      "Epoch 2920, Average Loss: 4.179587507247925\n",
      "Epoch 2930, Average Loss: 4.16227355003357\n",
      "Epoch 2940, Average Loss: 4.2495434284210205\n",
      "Epoch 2950, Average Loss: 4.232889127731323\n",
      "Epoch 2960, Average Loss: 4.112264490127563\n",
      "Epoch 2970, Average Loss: 4.106861281394958\n",
      "Epoch 2980, Average Loss: 4.092556166648865\n",
      "Epoch 2990, Average Loss: 4.097584891319275\n",
      "Epoch 3000, Average Loss: 4.164774608612061\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:05:34.535245Z",
     "start_time": "2024-11-20T11:05:34.474027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载保存的最佳模型状态字典\n",
    "model_state_dict = torch.load('best_model.pth', weights_only=True)\n",
    "model.load_state_dict(model_state_dict)\n",
    "# 评估模式\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_train_categ_torch, X_train_cont_torch)\n",
    "    print(\"Training Predictions:\")\n",
    "    print(predictions)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(y_train_torch.cpu().numpy(), predictions.cpu().numpy())\n",
    "    print(\"Training Metrics:\", train_metrics)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_predictions = model(X_test_categ_torch, X_test_cont_torch)\n",
    "    test_metrics = calculate_metrics(y_test_torch.cpu().numpy(), test_predictions.cpu().numpy())\n",
    "    print(\"Test Metrics:\", test_metrics)\n",
    "\n",
    "    # 将结果转换为DataFrame\n",
    "    tab_transformer_metrics = metrics_to_dataframe(y_train_torch.cpu().numpy(), predictions.cpu().numpy(),\n",
    "                                      y_test_torch.cpu().numpy(), test_predictions.cpu().numpy(), \"TabTransformer\").round(3)\n",
    "    tab_transformer_metrics.to_csv('ann_metrics.csv', index=False)\n",
    "    print(tab_transformer_metrics)"
   ],
   "id": "caf816a60d26e411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Predictions:\n",
      "tensor([[ 43.6939],\n",
      "        [165.9922],\n",
      "        [ 69.9694],\n",
      "        [ 35.4476],\n",
      "        [ 73.6449],\n",
      "        [166.1232],\n",
      "        [ 90.8249],\n",
      "        [142.6071],\n",
      "        [162.4902],\n",
      "        [ 77.6557],\n",
      "        [ 51.6398],\n",
      "        [ 77.3661],\n",
      "        [142.5352],\n",
      "        [ 72.0646],\n",
      "        [ 50.6148],\n",
      "        [ 48.9761],\n",
      "        [110.5851],\n",
      "        [ 45.4163],\n",
      "        [110.9482],\n",
      "        [121.6952],\n",
      "        [ 66.2022],\n",
      "        [146.0184],\n",
      "        [ 91.6008],\n",
      "        [ 41.5884],\n",
      "        [104.5343],\n",
      "        [ 81.2105],\n",
      "        [ 61.7363],\n",
      "        [ 49.0624],\n",
      "        [103.0496],\n",
      "        [ 51.4770],\n",
      "        [ 99.0285],\n",
      "        [ 60.5295],\n",
      "        [128.6222],\n",
      "        [ 74.3125],\n",
      "        [102.8935],\n",
      "        [ 23.8201],\n",
      "        [ 83.6700],\n",
      "        [108.1507],\n",
      "        [119.4769],\n",
      "        [109.2993],\n",
      "        [ 84.2494],\n",
      "        [ 54.6366],\n",
      "        [ 44.7992],\n",
      "        [102.3680],\n",
      "        [ 53.1823],\n",
      "        [ 58.6458],\n",
      "        [ 49.3005],\n",
      "        [122.1956],\n",
      "        [ 14.7462],\n",
      "        [130.5310],\n",
      "        [ 35.6587],\n",
      "        [ 73.4746],\n",
      "        [ 63.6698],\n",
      "        [ 89.7104],\n",
      "        [109.4109],\n",
      "        [ 74.4159],\n",
      "        [116.5799],\n",
      "        [ 84.7306],\n",
      "        [ 46.3987],\n",
      "        [ 70.9400],\n",
      "        [ 84.8837],\n",
      "        [ 58.9373],\n",
      "        [ 71.9329],\n",
      "        [ 56.7969],\n",
      "        [ 36.8303],\n",
      "        [101.7844],\n",
      "        [104.0856],\n",
      "        [103.4192],\n",
      "        [ 75.4286],\n",
      "        [ 92.8403],\n",
      "        [ 83.4848],\n",
      "        [ 28.5800],\n",
      "        [218.0849],\n",
      "        [ 40.6250],\n",
      "        [ 67.5591],\n",
      "        [112.4198],\n",
      "        [ 78.4917],\n",
      "        [ 31.6953],\n",
      "        [134.0301],\n",
      "        [105.0001],\n",
      "        [ 86.3494],\n",
      "        [199.1973],\n",
      "        [ 98.2262],\n",
      "        [ 86.3836],\n",
      "        [ 72.9576],\n",
      "        [102.3433],\n",
      "        [ 75.9880],\n",
      "        [106.5515],\n",
      "        [160.2232],\n",
      "        [129.5371],\n",
      "        [137.9041],\n",
      "        [ 38.2269],\n",
      "        [104.4996],\n",
      "        [ 62.2661],\n",
      "        [118.8353],\n",
      "        [100.4774],\n",
      "        [ 73.2688],\n",
      "        [ 56.3991],\n",
      "        [ 27.4714],\n",
      "        [ 86.8714],\n",
      "        [ 94.3158],\n",
      "        [ 41.6026],\n",
      "        [102.9098],\n",
      "        [ 75.9243],\n",
      "        [ 95.1179],\n",
      "        [135.4886],\n",
      "        [106.1176],\n",
      "        [109.4968],\n",
      "        [ 69.6027],\n",
      "        [ 80.7329],\n",
      "        [ 45.7382],\n",
      "        [154.2290],\n",
      "        [ 21.3724],\n",
      "        [ 61.8295],\n",
      "        [137.9375],\n",
      "        [ 59.4468],\n",
      "        [ 85.8102],\n",
      "        [187.8946],\n",
      "        [ 86.8747],\n",
      "        [110.5179],\n",
      "        [138.1445],\n",
      "        [ 78.3030],\n",
      "        [ 53.2508],\n",
      "        [ 33.8538],\n",
      "        [ 93.4095],\n",
      "        [107.6158],\n",
      "        [100.2822],\n",
      "        [ 91.1503],\n",
      "        [ 73.6246],\n",
      "        [ 85.1286],\n",
      "        [ 43.6889],\n",
      "        [153.3912],\n",
      "        [ 43.7483],\n",
      "        [ 62.7708],\n",
      "        [ 67.0131],\n",
      "        [107.6512],\n",
      "        [182.8316],\n",
      "        [117.3779],\n",
      "        [100.2170],\n",
      "        [172.9610],\n",
      "        [141.3285],\n",
      "        [140.6457],\n",
      "        [145.9230],\n",
      "        [ 69.3058],\n",
      "        [139.3560],\n",
      "        [ 82.6165],\n",
      "        [ 42.0219],\n",
      "        [ 86.9015],\n",
      "        [ 81.5130],\n",
      "        [102.3218],\n",
      "        [153.3022],\n",
      "        [ 60.6158],\n",
      "        [ 85.9995],\n",
      "        [ 97.8793],\n",
      "        [ 84.0811],\n",
      "        [ 30.9325],\n",
      "        [101.2165],\n",
      "        [ 79.6022],\n",
      "        [131.2974],\n",
      "        [114.0969],\n",
      "        [ 59.9915],\n",
      "        [ 63.5903],\n",
      "        [ 61.0399],\n",
      "        [ 86.7681],\n",
      "        [153.0245],\n",
      "        [143.5648],\n",
      "        [ 46.3067],\n",
      "        [104.2102],\n",
      "        [ 80.0501],\n",
      "        [126.8052],\n",
      "        [124.5378],\n",
      "        [ 73.7066],\n",
      "        [ 39.5274],\n",
      "        [ 77.3207],\n",
      "        [112.1027],\n",
      "        [ 55.6485],\n",
      "        [ 96.4029],\n",
      "        [ 88.1101],\n",
      "        [100.6524],\n",
      "        [111.9493],\n",
      "        [105.5239],\n",
      "        [125.4422],\n",
      "        [ 42.0467],\n",
      "        [ 58.1535],\n",
      "        [119.0238],\n",
      "        [ 60.6826],\n",
      "        [ 51.6151],\n",
      "        [109.5260],\n",
      "        [ 61.2174],\n",
      "        [180.0865],\n",
      "        [106.8655],\n",
      "        [ 84.8970],\n",
      "        [133.8467],\n",
      "        [104.7581],\n",
      "        [ 75.6052],\n",
      "        [115.9503],\n",
      "        [ 46.9381],\n",
      "        [ 67.4749],\n",
      "        [ 72.1776],\n",
      "        [117.0703],\n",
      "        [150.8791],\n",
      "        [108.3572],\n",
      "        [114.0974],\n",
      "        [102.5139],\n",
      "        [ 76.3395],\n",
      "        [112.1104],\n",
      "        [ 81.2773],\n",
      "        [108.1357],\n",
      "        [130.2705],\n",
      "        [ 49.3100],\n",
      "        [ 66.6937],\n",
      "        [ 90.5941],\n",
      "        [154.1017],\n",
      "        [ 73.5302],\n",
      "        [102.3217],\n",
      "        [ 42.4761],\n",
      "        [ 55.4192],\n",
      "        [106.4004],\n",
      "        [119.0105],\n",
      "        [ 54.5413],\n",
      "        [ 97.0882],\n",
      "        [ 92.2149],\n",
      "        [ 65.6775],\n",
      "        [169.6201],\n",
      "        [ 45.9881],\n",
      "        [122.5783],\n",
      "        [ 64.4673],\n",
      "        [ 92.0208],\n",
      "        [ 51.2542],\n",
      "        [104.7266],\n",
      "        [137.0856],\n",
      "        [128.0930],\n",
      "        [166.7589],\n",
      "        [ 81.6210],\n",
      "        [ 91.9152],\n",
      "        [ 92.5734],\n",
      "        [105.4010],\n",
      "        [ 68.9273],\n",
      "        [ 72.0135],\n",
      "        [115.4307],\n",
      "        [ 41.8360],\n",
      "        [130.2386],\n",
      "        [ 83.7045],\n",
      "        [ 44.8861],\n",
      "        [171.9663],\n",
      "        [141.9299],\n",
      "        [122.8205],\n",
      "        [ 52.0486],\n",
      "        [106.8007],\n",
      "        [ 85.8922],\n",
      "        [ 45.5698],\n",
      "        [ 23.1407],\n",
      "        [ 75.2352],\n",
      "        [ 43.2343],\n",
      "        [135.8730],\n",
      "        [ 81.8398],\n",
      "        [ 39.2936],\n",
      "        [ 61.7333],\n",
      "        [165.6774],\n",
      "        [ 87.0642],\n",
      "        [ 58.8898],\n",
      "        [103.4347],\n",
      "        [ 71.3270],\n",
      "        [112.0310],\n",
      "        [148.1598],\n",
      "        [ 65.5860],\n",
      "        [ 98.1088],\n",
      "        [ 95.6096],\n",
      "        [ 45.0261],\n",
      "        [ 49.4320],\n",
      "        [174.9792],\n",
      "        [103.5482],\n",
      "        [ 89.9881],\n",
      "        [ 26.9542],\n",
      "        [ 79.0670],\n",
      "        [180.2751],\n",
      "        [110.9104],\n",
      "        [ 91.1540],\n",
      "        [ 17.4005],\n",
      "        [ 62.4232],\n",
      "        [ 67.0246],\n",
      "        [ 30.4601],\n",
      "        [ 80.5423],\n",
      "        [ 88.2447],\n",
      "        [ 72.5762],\n",
      "        [ 67.7694],\n",
      "        [114.6660],\n",
      "        [ 75.2398],\n",
      "        [ 85.7446],\n",
      "        [ 27.3313],\n",
      "        [106.7277],\n",
      "        [108.6414],\n",
      "        [ 58.4460],\n",
      "        [ 60.4655],\n",
      "        [107.8033],\n",
      "        [148.8085],\n",
      "        [ 69.5312],\n",
      "        [ 99.2144],\n",
      "        [190.3082],\n",
      "        [ 88.2205],\n",
      "        [148.5147],\n",
      "        [ 89.1552],\n",
      "        [ 90.5247],\n",
      "        [ 68.8529],\n",
      "        [103.4182],\n",
      "        [149.8372],\n",
      "        [106.6593],\n",
      "        [ 99.5030],\n",
      "        [133.7697],\n",
      "        [ 73.3073],\n",
      "        [185.4465],\n",
      "        [ 72.6411],\n",
      "        [ 72.6875],\n",
      "        [ 84.2086],\n",
      "        [ 92.7622],\n",
      "        [ 67.7528],\n",
      "        [ 64.5901],\n",
      "        [ 92.9686],\n",
      "        [139.2789],\n",
      "        [ 92.7944],\n",
      "        [ 96.2609],\n",
      "        [ 83.0200],\n",
      "        [140.9008],\n",
      "        [ 82.4886],\n",
      "        [ 90.7256],\n",
      "        [ 84.8668],\n",
      "        [118.3931],\n",
      "        [ 61.8771],\n",
      "        [ 67.0910],\n",
      "        [112.5482],\n",
      "        [112.5997],\n",
      "        [ 77.1452],\n",
      "        [100.0406],\n",
      "        [ 69.8685],\n",
      "        [ 86.4333],\n",
      "        [134.6125],\n",
      "        [ 40.0934],\n",
      "        [124.4189],\n",
      "        [ 58.3116],\n",
      "        [ 74.4629],\n",
      "        [ 83.6067],\n",
      "        [128.1011],\n",
      "        [114.9983],\n",
      "        [ 73.9660],\n",
      "        [ 79.3703],\n",
      "        [ 31.9899],\n",
      "        [ 87.8742],\n",
      "        [105.9475],\n",
      "        [ 50.4685],\n",
      "        [118.3694],\n",
      "        [105.0778],\n",
      "        [121.4911],\n",
      "        [ 75.6669],\n",
      "        [ 93.7406],\n",
      "        [122.9149],\n",
      "        [103.5351],\n",
      "        [138.0047],\n",
      "        [129.6635],\n",
      "        [141.1752],\n",
      "        [108.8400],\n",
      "        [ 30.6899],\n",
      "        [ 75.4455],\n",
      "        [ 61.0238],\n",
      "        [ 95.7292],\n",
      "        [ 66.9600],\n",
      "        [ 65.9298],\n",
      "        [ 77.6508],\n",
      "        [ 26.7880],\n",
      "        [ 92.1226],\n",
      "        [ 59.9323],\n",
      "        [ 63.5560],\n",
      "        [168.0280],\n",
      "        [ 89.0921],\n",
      "        [172.0521],\n",
      "        [128.0073],\n",
      "        [ 82.1293],\n",
      "        [ 65.5306],\n",
      "        [ 69.2908],\n",
      "        [ 79.2494],\n",
      "        [ 57.0589],\n",
      "        [170.1724],\n",
      "        [115.0093],\n",
      "        [ 72.5175],\n",
      "        [108.6618],\n",
      "        [ 47.4501],\n",
      "        [ 45.7574],\n",
      "        [ 37.5930],\n",
      "        [121.3867],\n",
      "        [ 21.7275],\n",
      "        [113.7175],\n",
      "        [108.3950],\n",
      "        [ 82.9020],\n",
      "        [ 58.2822],\n",
      "        [ 72.4942],\n",
      "        [150.9446],\n",
      "        [ 45.0216],\n",
      "        [141.3504],\n",
      "        [119.5468],\n",
      "        [120.9382],\n",
      "        [ 71.1879],\n",
      "        [110.6756],\n",
      "        [ 73.1083],\n",
      "        [ 72.5427],\n",
      "        [ 95.3055],\n",
      "        [116.2087],\n",
      "        [ 60.8718],\n",
      "        [ 73.5900],\n",
      "        [147.0681],\n",
      "        [203.6076],\n",
      "        [ 80.9289],\n",
      "        [ 61.1073],\n",
      "        [ 84.0363],\n",
      "        [ 75.7253],\n",
      "        [ 86.4432],\n",
      "        [ 86.6568],\n",
      "        [ 90.8292],\n",
      "        [ 45.0790],\n",
      "        [ 21.3717],\n",
      "        [ 43.6995],\n",
      "        [116.8409],\n",
      "        [100.1804],\n",
      "        [112.7201],\n",
      "        [125.1490],\n",
      "        [ 24.6739],\n",
      "        [ 52.0276],\n",
      "        [ 98.4893],\n",
      "        [ 73.8013],\n",
      "        [ 97.0863],\n",
      "        [ 78.3614],\n",
      "        [ 64.6302],\n",
      "        [105.0358],\n",
      "        [137.1578],\n",
      "        [149.4725],\n",
      "        [ 66.7652],\n",
      "        [ 79.3618],\n",
      "        [ 78.2577],\n",
      "        [ 38.7950],\n",
      "        [ 97.3813],\n",
      "        [170.6663],\n",
      "        [118.1825],\n",
      "        [ 71.6663],\n",
      "        [ 98.9708],\n",
      "        [ 96.0590],\n",
      "        [ 17.6440],\n",
      "        [ 28.9047],\n",
      "        [ 82.3415],\n",
      "        [ 78.3147],\n",
      "        [ 87.4166],\n",
      "        [ 41.2155],\n",
      "        [ 71.5312],\n",
      "        [ 93.2322],\n",
      "        [113.5956],\n",
      "        [140.7395],\n",
      "        [ 55.9561],\n",
      "        [ 95.5727],\n",
      "        [139.5839],\n",
      "        [ 51.4149],\n",
      "        [ 69.9292],\n",
      "        [ 83.6042],\n",
      "        [ 84.4954],\n",
      "        [153.3157],\n",
      "        [ 59.1089],\n",
      "        [131.5200],\n",
      "        [ 66.1910],\n",
      "        [ 94.4288],\n",
      "        [116.9862],\n",
      "        [ 76.0551],\n",
      "        [ 79.5790],\n",
      "        [133.6205],\n",
      "        [ 79.9757],\n",
      "        [134.0953],\n",
      "        [ 88.4384],\n",
      "        [101.9580],\n",
      "        [135.7723],\n",
      "        [108.3053],\n",
      "        [ 92.8985],\n",
      "        [ 40.1230],\n",
      "        [123.9395],\n",
      "        [ 86.3355],\n",
      "        [144.6836]], device='cuda:0')\n",
      "Training Metrics: (0.93295818567276, 4.7515244, 4.042823985219002, 10.336437)\n",
      "Test Metrics: (0.9044901132583618, 8.159978, 10.552524030208588, 12.26485)\n",
      "            model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  \\\n",
      "0  TabTransformer     0.933      4.752       4.043      10.336    0.904   \n",
      "\n",
      "   MAE_test  MAPE_test  RMSE_test  \n",
      "0      8.16     10.553     12.265  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T11:05:56.310568Z",
     "start_time": "2024-11-20T11:05:56.304913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "tab_transformer_train = pd.DataFrame({'Actual': y_train_torch.cpu().numpy().squeeze(), 'Predicted': predictions.cpu().numpy().squeeze()})\n",
    "tab_transformer_test = pd.DataFrame({'Actual': y_test_torch.cpu().numpy().squeeze(), 'Predicted': test_predictions.cpu().numpy().squeeze()})\n",
    "\n",
    "tab_transformer_train.to_csv('tab_transformer_train.csv', index=False)\n",
    "tab_transformer_test.to_csv('tab_transformer_test.csv', index=False)"
   ],
   "id": "86de863b532e6d45",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
