{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T12:26:20.207691Z",
     "start_time": "2024-12-03T12:26:17.774397Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from function import split_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:26:20.263440Z",
     "start_time": "2024-12-03T12:26:20.210200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv(\"../../data/dataset_reduced.csv\")\n",
    "X_train, X_test, y_train, y_test = split_data(data, 'Cs')\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 将数据转换为张量\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "c653b6b1cbb0ab8c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:26:21.236727Z",
     "start_time": "2024-12-03T12:26:20.412142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch_function import MAPE_Loss, RMSE_Loss\n",
    "# 定义 LSTM 模型\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.28):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_layers = len(hidden_sizes)\n",
    "\n",
    "        # Defining multiple LSTM layers with configurable hidden sizes\n",
    "        self.lstm_layers = nn.ModuleList()\n",
    "        for i in range(self.num_layers):\n",
    "            input_dim = input_size if i == 0 else hidden_sizes[i-1]\n",
    "            lstm_dropout = dropout if self.num_layers > 1 and i < self.num_layers - 1 else 0\n",
    "            self.lstm_layers.append(nn.LSTM(input_dim, hidden_sizes[i], batch_first=True, dropout=lstm_dropout))\n",
    "\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_sizes[-1], output_size)\n",
    "\n",
    "    def forward(self, x, seq_lengths=None):\n",
    "        # Initial hidden and cell state for each layer\n",
    "        h0 = [torch.zeros(1, x.size(0), hidden_size).to(x.device) for hidden_size in self.hidden_sizes]\n",
    "        c0 = [torch.zeros(1, x.size(0), hidden_size).to(x.device) for hidden_size in self.hidden_sizes]\n",
    "\n",
    "        # Forward propagate through each LSTM layer\n",
    "        out = x\n",
    "        for i, lstm in enumerate(self.lstm_layers):\n",
    "            if seq_lengths is not None:\n",
    "                packed_input = nn.utils.rnn.pack_padded_sequence(out, seq_lengths, batch_first=True, enforce_sorted=False)\n",
    "                packed_output, (h, c) = lstm(packed_input, (h0[i], c0[i]))\n",
    "                out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "            else:\n",
    "                out, (h, c) = lstm(out, (h0[i], c0[i]))\n",
    "\n",
    "        # Decode the hidden state of the last time step using average pooling\n",
    "        out = torch.mean(out, dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Set model parameters\n",
    "input_size = X_train_scaled.shape[1]  # Number of features per time step\n",
    "hidden_sizes = [64, 64]  # LSTM hidden sizes\n",
    "output_size = 1\n",
    "\n",
    "model = LSTMRegressor(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = RMSE_Loss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Reshape input for LSTM (batch_size, sequence_length, input_size)\n",
    "def reshape_for_lstm(X):\n",
    "    return X.unsqueeze(1)  # Add a sequence length dimension of 1"
   ],
   "id": "3df9490589a74d9e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micha\\.conda\\envs\\pytorch\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:27:06.148305Z",
     "start_time": "2024-12-03T12:26:21.261010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the model\n",
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "patience = 100  # 允许的最大连续未改进 epoch 数\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        X_batch = reshape_for_lstm(X_batch)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs.squeeze(), y_batch)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 打印每10个 epoch 的损失\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # 计算验证损失\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            X_batch = reshape_for_lstm(X_batch)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.squeeze(), y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        val_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # 判断验证损失是否改善\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0  # 重置计数器\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"lstm_best_model.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # 如果验证损失在一定次数的 epoch 内没有改进，则停止训练\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "1c145a4951631b53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/3000], Loss: 77.9750\n",
      "Epoch [20/3000], Loss: 62.9137\n",
      "Epoch [30/3000], Loss: 51.4748\n",
      "Epoch [40/3000], Loss: 42.7763\n",
      "Epoch [50/3000], Loss: 37.7576\n",
      "Epoch [60/3000], Loss: 33.6896\n",
      "Epoch [70/3000], Loss: 30.0957\n",
      "Epoch [80/3000], Loss: 27.1722\n",
      "Epoch [90/3000], Loss: 24.2342\n",
      "Epoch [100/3000], Loss: 20.5811\n",
      "Epoch [110/3000], Loss: 17.7743\n",
      "Epoch [120/3000], Loss: 15.9934\n",
      "Epoch [130/3000], Loss: 15.0194\n",
      "Epoch [140/3000], Loss: 13.6799\n",
      "Epoch [150/3000], Loss: 12.6443\n",
      "Epoch [160/3000], Loss: 12.0691\n",
      "Epoch [170/3000], Loss: 11.1754\n",
      "Epoch [180/3000], Loss: 10.5502\n",
      "Epoch [190/3000], Loss: 10.0584\n",
      "Epoch [200/3000], Loss: 9.6915\n",
      "Epoch [210/3000], Loss: 9.5556\n",
      "Epoch [220/3000], Loss: 9.4715\n",
      "Epoch [230/3000], Loss: 9.1901\n",
      "Epoch [240/3000], Loss: 8.8240\n",
      "Epoch [250/3000], Loss: 8.6781\n",
      "Epoch [260/3000], Loss: 8.2789\n",
      "Epoch [270/3000], Loss: 8.3101\n",
      "Epoch [280/3000], Loss: 8.3272\n",
      "Epoch [290/3000], Loss: 7.8744\n",
      "Epoch [300/3000], Loss: 7.6760\n",
      "Epoch [310/3000], Loss: 7.5668\n",
      "Epoch [320/3000], Loss: 7.7084\n",
      "Epoch [330/3000], Loss: 7.0911\n",
      "Epoch [340/3000], Loss: 7.0411\n",
      "Epoch [350/3000], Loss: 7.0870\n",
      "Epoch [360/3000], Loss: 6.8835\n",
      "Epoch [370/3000], Loss: 7.2391\n",
      "Epoch [380/3000], Loss: 7.0375\n",
      "Epoch [390/3000], Loss: 6.6755\n",
      "Epoch [400/3000], Loss: 6.2922\n",
      "Epoch [410/3000], Loss: 6.3763\n",
      "Epoch [420/3000], Loss: 6.2048\n",
      "Epoch [430/3000], Loss: 6.4523\n",
      "Epoch [440/3000], Loss: 6.0477\n",
      "Epoch [450/3000], Loss: 6.4724\n",
      "Epoch [460/3000], Loss: 5.8304\n",
      "Epoch [470/3000], Loss: 5.9495\n",
      "Epoch [480/3000], Loss: 5.2901\n",
      "Epoch [490/3000], Loss: 5.8377\n",
      "Epoch [500/3000], Loss: 5.3414\n",
      "Epoch [510/3000], Loss: 5.7501\n",
      "Epoch [520/3000], Loss: 5.3893\n",
      "Epoch [530/3000], Loss: 5.5863\n",
      "Epoch [540/3000], Loss: 5.5797\n",
      "Epoch [550/3000], Loss: 5.3874\n",
      "Epoch [560/3000], Loss: 5.9656\n",
      "Epoch [570/3000], Loss: 5.4510\n",
      "Epoch [580/3000], Loss: 5.7918\n",
      "Epoch [590/3000], Loss: 5.0397\n",
      "Epoch [600/3000], Loss: 5.0164\n",
      "Epoch [610/3000], Loss: 4.8161\n",
      "Epoch [620/3000], Loss: 5.2328\n",
      "Epoch [630/3000], Loss: 5.9557\n",
      "Epoch [640/3000], Loss: 4.7604\n",
      "Epoch [650/3000], Loss: 5.2241\n",
      "Epoch [660/3000], Loss: 5.0865\n",
      "Epoch [670/3000], Loss: 4.5707\n",
      "Epoch [680/3000], Loss: 4.6670\n",
      "Epoch [690/3000], Loss: 4.9957\n",
      "Epoch [700/3000], Loss: 4.5594\n",
      "Epoch [710/3000], Loss: 4.6392\n",
      "Epoch [720/3000], Loss: 4.9154\n",
      "Epoch [730/3000], Loss: 4.8461\n",
      "Epoch [740/3000], Loss: 4.7055\n",
      "Epoch [750/3000], Loss: 5.2099\n",
      "Epoch [760/3000], Loss: 5.0658\n",
      "Epoch [770/3000], Loss: 5.1352\n",
      "Epoch [780/3000], Loss: 4.5368\n",
      "Epoch [790/3000], Loss: 4.5442\n",
      "Epoch [800/3000], Loss: 3.9808\n",
      "Epoch [810/3000], Loss: 4.5077\n",
      "Epoch [820/3000], Loss: 4.9186\n",
      "Epoch [830/3000], Loss: 4.2857\n",
      "Epoch [840/3000], Loss: 5.3412\n",
      "Epoch [850/3000], Loss: 5.0620\n",
      "Epoch [860/3000], Loss: 4.6104\n",
      "Epoch [870/3000], Loss: 4.5861\n",
      "Epoch [880/3000], Loss: 4.9141\n",
      "Epoch [890/3000], Loss: 4.1822\n",
      "Epoch [900/3000], Loss: 4.0285\n",
      "Epoch [910/3000], Loss: 4.1964\n",
      "Epoch [920/3000], Loss: 4.0160\n",
      "Epoch [930/3000], Loss: 4.9287\n",
      "Epoch [940/3000], Loss: 4.7957\n",
      "Epoch [950/3000], Loss: 4.7182\n",
      "Epoch [960/3000], Loss: 4.2046\n",
      "Epoch [970/3000], Loss: 4.1403\n",
      "Epoch [980/3000], Loss: 5.0007\n",
      "Epoch [990/3000], Loss: 4.3143\n",
      "Epoch [1000/3000], Loss: 4.0823\n",
      "Epoch [1010/3000], Loss: 4.0645\n",
      "Epoch [1020/3000], Loss: 4.7256\n",
      "Epoch [1030/3000], Loss: 4.0829\n",
      "Epoch [1040/3000], Loss: 4.0747\n",
      "Epoch [1050/3000], Loss: 4.2899\n",
      "Epoch [1060/3000], Loss: 4.6148\n",
      "Epoch [1070/3000], Loss: 4.1489\n",
      "Epoch [1080/3000], Loss: 4.8016\n",
      "Epoch [1090/3000], Loss: 4.3724\n",
      "Epoch [1100/3000], Loss: 4.0389\n",
      "Epoch [1110/3000], Loss: 3.9759\n",
      "Epoch [1120/3000], Loss: 4.5373\n",
      "Epoch [1130/3000], Loss: 3.6466\n",
      "Epoch [1140/3000], Loss: 3.9851\n",
      "Early stopping at epoch 1149\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:27:06.243458Z",
     "start_time": "2024-12-03T12:27:06.213021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load(\"lstm_best_model.pth\", weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_predictions = []\n",
    "    y_train_true = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        X_batch = reshape_for_lstm(X_batch)\n",
    "        outputs = model(X_batch)\n",
    "        train_predictions.append(outputs.cpu().numpy())\n",
    "        y_train_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "    train_predictions = np.concatenate(train_predictions, axis=0)\n",
    "    y_train_true = np.concatenate(y_train_true, axis=0)\n",
    "\n",
    "    test_predictions = []\n",
    "    y_test_true = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        X_batch = reshape_for_lstm(X_batch)\n",
    "        outputs = model(X_batch)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "        y_test_true.append(y_batch.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate(test_predictions, axis=0)\n",
    "    y_test_true = np.concatenate(y_test_true, axis=0)\n",
    "\n",
    "    # 将结果转换为DataFrame\n",
    "    lstm_metrics = metrics_to_dataframe(\n",
    "        y_train_true, train_predictions,\n",
    "        y_test_true, test_predictions, \"LSTM\").round(3)\n",
    "    lstm_metrics.to_csv('LSTM_metrics.csv', index=False)\n",
    "\n",
    "lstm_metrics"
   ],
   "id": "b8b38ebb7b444f9b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  MAE_test  \\\n",
       "0  LSTM     0.986      2.433       2.552        4.83     0.96     4.845   \n",
       "\n",
       "   MAPE_test  RMSE_test  \n",
       "0      8.748      8.322  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>R2_train</th>\n",
       "      <th>MAE_train</th>\n",
       "      <th>MAPE_train</th>\n",
       "      <th>RMSE_train</th>\n",
       "      <th>R2_test</th>\n",
       "      <th>MAE_test</th>\n",
       "      <th>MAPE_test</th>\n",
       "      <th>RMSE_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.986</td>\n",
       "      <td>2.433</td>\n",
       "      <td>2.552</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.96</td>\n",
       "      <td>4.845</td>\n",
       "      <td>8.748</td>\n",
       "      <td>8.322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:27:06.338369Z",
     "start_time": "2024-12-03T12:27:06.333350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存预测结果\n",
    "lstm_train = pd.DataFrame({'Actual': y_train_true, 'Predicted': train_predictions.squeeze()})\n",
    "lstm_test = pd.DataFrame({'Actual': y_test_true, 'Predicted': test_predictions.squeeze()})\n",
    "lstm_train.to_csv('lstm_train.csv', index=False)\n",
    "lstm_test.to_csv('lstm_test.csv', index=False)"
   ],
   "id": "89a0b250ede65b92",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
