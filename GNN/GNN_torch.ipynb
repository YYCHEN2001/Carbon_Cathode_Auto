{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:56:52.803580Z",
     "start_time": "2024-11-20T16:56:51.481225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e7d30d5c108ee240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 读取数据",
   "id": "31d7c4f1b9247569"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:56:53.435303Z",
     "start_time": "2024-11-20T16:56:52.806572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "b295e86f637a2b4c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建图数据对象, 转换数据为图数据结构",
   "id": "e364287c13c7204b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:56:54.895038Z",
     "start_time": "2024-11-20T16:56:53.490880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 材料特性的索引和测试条件的索引\n",
    "material_indices = list(range(7))  # 前七个特性\n",
    "test_indices = list(range(7, 12))  # 后五个条件\n",
    "\n",
    "# 构建边：仅为材料特性之间构建边\n",
    "edges = []\n",
    "for i in material_indices:\n",
    "    for j in material_indices:\n",
    "        if i != j:\n",
    "            edges.append([i, j])\n",
    "\n",
    "edges = np.array(edges).T  # 转置以匹配PyTorch Geometric的edge_index格式\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建图数据对象\n",
    "train_data = Data(x=X_train_torch, edge_index=edge_index, y=y_train_torch).to(device)\n",
    "\n",
    "# 打印数据对象信息，确认构建是否成功\n",
    "print(train_data)"
   ],
   "id": "6fd7f6e086ddd296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[480, 12], edge_index=[2, 42], y=[480, 1])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "6f857b4b31231496"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:56:54.945595Z",
     "start_time": "2024-11-20T16:56:54.910240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class MAPELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        epsilon = 1e-8  # 避免除以零\n",
    "        mape = torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 100\n",
    "        return mape\n",
    "\n",
    "class GNN4TDL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN4TDL, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 24)\n",
    "        self.conv2 = GCNConv(24, 48)\n",
    "        self.conv3 = GCNConv(48, 1)\n",
    "        # self.conv4 = GCNConv(24, 1)\n",
    "        # self.conv5 = GCNConv(24, 1)\n",
    "        # self.conv6 = GCNConv(70, 30)\n",
    "        # self.conv7 = GCNConv(30, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv4(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv5(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv6(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv7(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN4TDL(\n",
    "    input_dim=X_train_scaled.shape[1]\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义损失函数\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "mape_loss = MAPELoss().to(device)\n",
    "## 训练模型"
   ],
   "id": "18cb525202dbc92f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:57:08.922192Z",
     "start_time": "2024-11-20T16:56:54.958652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "patience = 30  # 允许的最大连续未改进 epoch 数\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = mape_loss(out, train_data.y)  # Modify as per your loss function, e.g., mape_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cumulative_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss\n",
    "\n",
    "    # 计算验证损失\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 在 GPU 上进行预测\n",
    "        y_val_pred = model(train_data).to(device)\n",
    "        # 验证损失计算时，确保 y_test_tensor 也在同一个设备上\n",
    "        train_data.y = train_data.y.to(device)\n",
    "        val_loss = mape_loss(y_val_pred, train_data.y).item()  # 计算验证损失\n",
    "\n",
    "    # 判断验证损失是否改善\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0  # 重置计数器\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"gnn_best_model.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # 如果验证损失在一定次数的 epoch 内没有改进，则停止训练\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "49aa5bf124c4f90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 100.2440315246582\n",
      "Epoch 20, Average Loss: 99.4111312866211\n",
      "Epoch 30, Average Loss: 98.5343620300293\n",
      "Epoch 40, Average Loss: 97.54377746582031\n",
      "Epoch 50, Average Loss: 96.38241882324219\n",
      "Epoch 60, Average Loss: 94.95718765258789\n",
      "Epoch 70, Average Loss: 93.16415481567383\n",
      "Epoch 80, Average Loss: 90.9034637451172\n",
      "Epoch 90, Average Loss: 88.08167114257813\n",
      "Epoch 100, Average Loss: 84.74340744018555\n",
      "Epoch 110, Average Loss: 80.91019210815429\n",
      "Epoch 120, Average Loss: 76.70636291503907\n",
      "Epoch 130, Average Loss: 72.52476119995117\n",
      "Epoch 140, Average Loss: 68.69895095825196\n",
      "Epoch 150, Average Loss: 64.99007415771484\n",
      "Epoch 160, Average Loss: 61.330452346801756\n",
      "Epoch 170, Average Loss: 57.87875213623047\n",
      "Epoch 180, Average Loss: 54.5770751953125\n",
      "Epoch 190, Average Loss: 51.419104385375974\n",
      "Epoch 200, Average Loss: 48.53999290466309\n",
      "Epoch 210, Average Loss: 46.18935661315918\n",
      "Epoch 220, Average Loss: 44.1970832824707\n",
      "Epoch 230, Average Loss: 42.3601432800293\n",
      "Epoch 240, Average Loss: 40.61763954162598\n",
      "Epoch 250, Average Loss: 39.02096405029297\n",
      "Epoch 260, Average Loss: 37.47864456176758\n",
      "Epoch 270, Average Loss: 36.02518920898437\n",
      "Epoch 280, Average Loss: 34.67338905334473\n",
      "Epoch 290, Average Loss: 33.4668514251709\n",
      "Epoch 300, Average Loss: 32.43315505981445\n",
      "Epoch 310, Average Loss: 31.534775161743163\n",
      "Epoch 320, Average Loss: 30.732899856567382\n",
      "Epoch 330, Average Loss: 30.033821678161623\n",
      "Epoch 340, Average Loss: 29.443542098999025\n",
      "Epoch 350, Average Loss: 28.93253536224365\n",
      "Epoch 360, Average Loss: 28.48744487762451\n",
      "Epoch 370, Average Loss: 28.07909278869629\n",
      "Epoch 380, Average Loss: 27.69562187194824\n",
      "Epoch 390, Average Loss: 27.336067962646485\n",
      "Epoch 400, Average Loss: 27.00714874267578\n",
      "Epoch 410, Average Loss: 26.703311920166016\n",
      "Epoch 420, Average Loss: 26.427092170715333\n",
      "Epoch 430, Average Loss: 26.178225708007812\n",
      "Epoch 440, Average Loss: 25.937013244628908\n",
      "Epoch 450, Average Loss: 25.70314712524414\n",
      "Epoch 460, Average Loss: 25.47065715789795\n",
      "Epoch 470, Average Loss: 25.23991756439209\n",
      "Epoch 480, Average Loss: 25.036726188659667\n",
      "Epoch 490, Average Loss: 24.84532356262207\n",
      "Epoch 500, Average Loss: 24.657372093200685\n",
      "Epoch 510, Average Loss: 24.472462844848632\n",
      "Epoch 520, Average Loss: 24.29320373535156\n",
      "Epoch 530, Average Loss: 24.121460151672363\n",
      "Epoch 540, Average Loss: 23.942021560668945\n",
      "Epoch 550, Average Loss: 23.759452247619627\n",
      "Epoch 560, Average Loss: 23.58086528778076\n",
      "Epoch 570, Average Loss: 23.40095748901367\n",
      "Epoch 580, Average Loss: 23.217117881774904\n",
      "Epoch 590, Average Loss: 23.033128356933595\n",
      "Epoch 600, Average Loss: 22.857318878173828\n",
      "Epoch 610, Average Loss: 22.688749694824217\n",
      "Epoch 620, Average Loss: 22.52145004272461\n",
      "Epoch 630, Average Loss: 22.35576477050781\n",
      "Epoch 640, Average Loss: 22.196247100830078\n",
      "Epoch 650, Average Loss: 22.0402587890625\n",
      "Epoch 660, Average Loss: 21.889980125427247\n",
      "Epoch 670, Average Loss: 21.74317150115967\n",
      "Epoch 680, Average Loss: 21.599118423461913\n",
      "Epoch 690, Average Loss: 21.45337257385254\n",
      "Epoch 700, Average Loss: 21.307678604125975\n",
      "Epoch 710, Average Loss: 21.1616792678833\n",
      "Epoch 720, Average Loss: 21.014850997924803\n",
      "Epoch 730, Average Loss: 20.868792915344237\n",
      "Epoch 740, Average Loss: 20.72106342315674\n",
      "Epoch 750, Average Loss: 20.569179344177247\n",
      "Epoch 760, Average Loss: 20.40967388153076\n",
      "Epoch 770, Average Loss: 20.245716285705566\n",
      "Epoch 780, Average Loss: 20.081552505493164\n",
      "Epoch 790, Average Loss: 19.911318206787108\n",
      "Epoch 800, Average Loss: 19.736052131652833\n",
      "Epoch 810, Average Loss: 19.565445518493654\n",
      "Epoch 820, Average Loss: 19.40026569366455\n",
      "Epoch 830, Average Loss: 19.23885440826416\n",
      "Epoch 840, Average Loss: 19.08028793334961\n",
      "Epoch 850, Average Loss: 18.924173164367676\n",
      "Epoch 860, Average Loss: 18.773312377929688\n",
      "Epoch 870, Average Loss: 18.62413444519043\n",
      "Epoch 880, Average Loss: 18.465804290771484\n",
      "Epoch 890, Average Loss: 18.303767585754393\n",
      "Epoch 900, Average Loss: 18.148130798339842\n",
      "Epoch 910, Average Loss: 18.000398254394533\n",
      "Epoch 920, Average Loss: 17.856616973876953\n",
      "Epoch 930, Average Loss: 17.71560573577881\n",
      "Epoch 940, Average Loss: 17.580646133422853\n",
      "Epoch 950, Average Loss: 17.442634201049806\n",
      "Epoch 960, Average Loss: 17.305215454101564\n",
      "Epoch 970, Average Loss: 17.16367835998535\n",
      "Epoch 980, Average Loss: 17.023265075683593\n",
      "Epoch 990, Average Loss: 16.88365478515625\n",
      "Epoch 1000, Average Loss: 16.7465576171875\n",
      "Epoch 1010, Average Loss: 16.602621269226074\n",
      "Epoch 1020, Average Loss: 16.468164443969727\n",
      "Epoch 1030, Average Loss: 16.3381010055542\n",
      "Epoch 1040, Average Loss: 16.21314926147461\n",
      "Epoch 1050, Average Loss: 16.093657302856446\n",
      "Epoch 1060, Average Loss: 15.972007274627686\n",
      "Epoch 1070, Average Loss: 15.852718925476074\n",
      "Epoch 1080, Average Loss: 15.732947731018067\n",
      "Epoch 1090, Average Loss: 15.614916801452637\n",
      "Epoch 1100, Average Loss: 15.497569561004639\n",
      "Epoch 1110, Average Loss: 15.379058647155762\n",
      "Epoch 1120, Average Loss: 15.259862804412842\n",
      "Epoch 1130, Average Loss: 15.14238920211792\n",
      "Epoch 1140, Average Loss: 15.02726354598999\n",
      "Epoch 1150, Average Loss: 14.912327289581299\n",
      "Epoch 1160, Average Loss: 14.80035400390625\n",
      "Epoch 1170, Average Loss: 14.69112663269043\n",
      "Epoch 1180, Average Loss: 14.582751083374024\n",
      "Epoch 1190, Average Loss: 14.473480701446533\n",
      "Epoch 1200, Average Loss: 14.360328865051269\n",
      "Epoch 1210, Average Loss: 14.249813842773438\n",
      "Epoch 1220, Average Loss: 14.144931888580322\n",
      "Epoch 1230, Average Loss: 14.041137218475342\n",
      "Epoch 1240, Average Loss: 13.936918640136719\n",
      "Epoch 1250, Average Loss: 13.835157108306884\n",
      "Epoch 1260, Average Loss: 13.73313102722168\n",
      "Epoch 1270, Average Loss: 13.633475589752198\n",
      "Epoch 1280, Average Loss: 13.5422287940979\n",
      "Epoch 1290, Average Loss: 13.447230911254882\n",
      "Epoch 1300, Average Loss: 13.355329990386963\n",
      "Epoch 1310, Average Loss: 13.25533618927002\n",
      "Epoch 1320, Average Loss: 13.155055046081543\n",
      "Epoch 1330, Average Loss: 13.059842014312744\n",
      "Epoch 1340, Average Loss: 12.96835765838623\n",
      "Epoch 1350, Average Loss: 12.881196117401123\n",
      "Epoch 1360, Average Loss: 12.791284275054931\n",
      "Epoch 1370, Average Loss: 12.718830871582032\n",
      "Epoch 1380, Average Loss: 12.653458499908448\n",
      "Epoch 1390, Average Loss: 12.586215591430664\n",
      "Epoch 1400, Average Loss: 12.532752227783202\n",
      "Epoch 1410, Average Loss: 12.474440383911134\n",
      "Epoch 1420, Average Loss: 12.4202730178833\n",
      "Epoch 1430, Average Loss: 12.359525108337403\n",
      "Epoch 1440, Average Loss: 12.301379585266114\n",
      "Epoch 1450, Average Loss: 12.243948459625244\n",
      "Epoch 1460, Average Loss: 12.189553165435791\n",
      "Epoch 1470, Average Loss: 12.138352870941162\n",
      "Epoch 1480, Average Loss: 12.08441743850708\n",
      "Epoch 1490, Average Loss: 12.039054679870606\n",
      "Epoch 1500, Average Loss: 11.988873386383057\n",
      "Epoch 1510, Average Loss: 11.938406944274902\n",
      "Epoch 1520, Average Loss: 11.891772556304932\n",
      "Epoch 1530, Average Loss: 11.848904991149903\n",
      "Epoch 1540, Average Loss: 11.808978748321532\n",
      "Epoch 1550, Average Loss: 11.762686252593994\n",
      "Epoch 1560, Average Loss: 11.726211833953858\n",
      "Epoch 1570, Average Loss: 11.687440299987793\n",
      "Epoch 1580, Average Loss: 11.647636699676514\n",
      "Epoch 1590, Average Loss: 11.611092472076416\n",
      "Epoch 1600, Average Loss: 11.579729175567627\n",
      "Epoch 1610, Average Loss: 11.543594646453858\n",
      "Epoch 1620, Average Loss: 11.510054969787598\n",
      "Epoch 1630, Average Loss: 11.474546432495117\n",
      "Epoch 1640, Average Loss: 11.43658800125122\n",
      "Epoch 1650, Average Loss: 11.406817626953124\n",
      "Epoch 1660, Average Loss: 11.363917446136474\n",
      "Epoch 1670, Average Loss: 11.326178455352784\n",
      "Epoch 1680, Average Loss: 11.300810241699219\n",
      "Epoch 1690, Average Loss: 11.271623229980468\n",
      "Epoch 1700, Average Loss: 11.238621997833253\n",
      "Epoch 1710, Average Loss: 11.213622093200684\n",
      "Epoch 1720, Average Loss: 11.186433696746827\n",
      "Epoch 1730, Average Loss: 11.157783126831054\n",
      "Epoch 1740, Average Loss: 11.130544281005859\n",
      "Epoch 1750, Average Loss: 11.100082683563233\n",
      "Epoch 1760, Average Loss: 11.072256088256836\n",
      "Epoch 1770, Average Loss: 11.048934745788575\n",
      "Epoch 1780, Average Loss: 11.030917835235595\n",
      "Epoch 1790, Average Loss: 11.01698570251465\n",
      "Epoch 1800, Average Loss: 10.98561372756958\n",
      "Epoch 1810, Average Loss: 10.9592848777771\n",
      "Epoch 1820, Average Loss: 10.941768360137939\n",
      "Epoch 1830, Average Loss: 10.916921234130859\n",
      "Epoch 1840, Average Loss: 10.889202880859376\n",
      "Epoch 1850, Average Loss: 10.871079158782958\n",
      "Epoch 1860, Average Loss: 10.846723270416259\n",
      "Epoch 1870, Average Loss: 10.825455379486083\n",
      "Epoch 1880, Average Loss: 10.800133514404298\n",
      "Epoch 1890, Average Loss: 10.78522891998291\n",
      "Epoch 1900, Average Loss: 10.761846256256103\n",
      "Epoch 1910, Average Loss: 10.742896556854248\n",
      "Epoch 1920, Average Loss: 10.718361377716064\n",
      "Epoch 1930, Average Loss: 10.692623710632324\n",
      "Epoch 1940, Average Loss: 10.676404571533203\n",
      "Epoch 1950, Average Loss: 10.649012851715089\n",
      "Epoch 1960, Average Loss: 10.629137897491455\n",
      "Epoch 1970, Average Loss: 10.596345710754395\n",
      "Epoch 1980, Average Loss: 10.578751468658448\n",
      "Epoch 1990, Average Loss: 10.542264461517334\n",
      "Epoch 2000, Average Loss: 10.517995357513428\n",
      "Epoch 2010, Average Loss: 10.495482254028321\n",
      "Epoch 2020, Average Loss: 10.47429256439209\n",
      "Epoch 2030, Average Loss: 10.453756427764892\n",
      "Epoch 2040, Average Loss: 10.433366775512695\n",
      "Epoch 2050, Average Loss: 10.413780689239502\n",
      "Epoch 2060, Average Loss: 10.396666145324707\n",
      "Epoch 2070, Average Loss: 10.371365451812744\n",
      "Epoch 2080, Average Loss: 10.348271942138672\n",
      "Epoch 2090, Average Loss: 10.328784465789795\n",
      "Epoch 2100, Average Loss: 10.303605556488037\n",
      "Epoch 2110, Average Loss: 10.287075424194336\n",
      "Epoch 2120, Average Loss: 10.266500854492188\n",
      "Epoch 2130, Average Loss: 10.243925094604492\n",
      "Epoch 2140, Average Loss: 10.224852275848388\n",
      "Epoch 2150, Average Loss: 10.208334827423096\n",
      "Epoch 2160, Average Loss: 10.184206771850587\n",
      "Epoch 2170, Average Loss: 10.170355033874511\n",
      "Epoch 2180, Average Loss: 10.152217674255372\n",
      "Epoch 2190, Average Loss: 10.138782215118407\n",
      "Epoch 2200, Average Loss: 10.127201366424561\n",
      "Epoch 2210, Average Loss: 10.099086380004882\n",
      "Epoch 2220, Average Loss: 10.070070838928222\n",
      "Epoch 2230, Average Loss: 10.05142583847046\n",
      "Epoch 2240, Average Loss: 10.029329013824462\n",
      "Epoch 2250, Average Loss: 10.019505405426026\n",
      "Epoch 2260, Average Loss: 10.001750946044922\n",
      "Epoch 2270, Average Loss: 9.980142307281493\n",
      "Epoch 2280, Average Loss: 9.959736442565918\n",
      "Epoch 2290, Average Loss: 9.937903022766113\n",
      "Epoch 2300, Average Loss: 9.926810932159423\n",
      "Epoch 2310, Average Loss: 9.911503505706786\n",
      "Epoch 2320, Average Loss: 9.891365909576416\n",
      "Epoch 2330, Average Loss: 9.87194004058838\n",
      "Epoch 2340, Average Loss: 9.856659030914306\n",
      "Epoch 2350, Average Loss: 9.845161056518554\n",
      "Epoch 2360, Average Loss: 9.831818103790283\n",
      "Epoch 2370, Average Loss: 9.81776008605957\n",
      "Epoch 2380, Average Loss: 9.798431396484375\n",
      "Epoch 2390, Average Loss: 9.791713428497314\n",
      "Epoch 2400, Average Loss: 9.7739652633667\n",
      "Epoch 2410, Average Loss: 9.758344173431396\n",
      "Epoch 2420, Average Loss: 9.747585201263428\n",
      "Epoch 2430, Average Loss: 9.74087438583374\n",
      "Epoch 2440, Average Loss: 9.725884628295898\n",
      "Epoch 2450, Average Loss: 9.706283664703369\n",
      "Epoch 2460, Average Loss: 9.694747066497802\n",
      "Epoch 2470, Average Loss: 9.680306434631348\n",
      "Epoch 2480, Average Loss: 9.66421365737915\n",
      "Epoch 2490, Average Loss: 9.652239036560058\n",
      "Epoch 2500, Average Loss: 9.636574745178223\n",
      "Epoch 2510, Average Loss: 9.619803428649902\n",
      "Epoch 2520, Average Loss: 9.606218719482422\n",
      "Epoch 2530, Average Loss: 9.59226198196411\n",
      "Epoch 2540, Average Loss: 9.57524356842041\n",
      "Epoch 2550, Average Loss: 9.561974430084229\n",
      "Epoch 2560, Average Loss: 9.547978496551513\n",
      "Epoch 2570, Average Loss: 9.531052589416504\n",
      "Epoch 2580, Average Loss: 9.520604228973388\n",
      "Epoch 2590, Average Loss: 9.500841999053955\n",
      "Epoch 2600, Average Loss: 9.480203628540039\n",
      "Epoch 2610, Average Loss: 9.462335777282714\n",
      "Epoch 2620, Average Loss: 9.445999908447266\n",
      "Epoch 2630, Average Loss: 9.429280853271484\n",
      "Epoch 2640, Average Loss: 9.416407203674316\n",
      "Epoch 2650, Average Loss: 9.410195350646973\n",
      "Epoch 2660, Average Loss: 9.389053249359131\n",
      "Epoch 2670, Average Loss: 9.373550987243652\n",
      "Epoch 2680, Average Loss: 9.361664009094238\n",
      "Epoch 2690, Average Loss: 9.345932006835938\n",
      "Epoch 2700, Average Loss: 9.335101795196532\n",
      "Epoch 2710, Average Loss: 9.322609615325927\n",
      "Epoch 2720, Average Loss: 9.30815830230713\n",
      "Epoch 2730, Average Loss: 9.29986867904663\n",
      "Epoch 2740, Average Loss: 9.293413257598877\n",
      "Epoch 2750, Average Loss: 9.279946231842041\n",
      "Epoch 2760, Average Loss: 9.26572914123535\n",
      "Epoch 2770, Average Loss: 9.253904342651367\n",
      "Epoch 2780, Average Loss: 9.241625213623047\n",
      "Epoch 2790, Average Loss: 9.23225269317627\n",
      "Epoch 2800, Average Loss: 9.215516948699952\n",
      "Epoch 2810, Average Loss: 9.204572010040284\n",
      "Epoch 2820, Average Loss: 9.191232872009277\n",
      "Epoch 2830, Average Loss: 9.178437232971191\n",
      "Epoch 2840, Average Loss: 9.16927433013916\n",
      "Epoch 2850, Average Loss: 9.154332256317138\n",
      "Epoch 2860, Average Loss: 9.140618991851806\n",
      "Epoch 2870, Average Loss: 9.127041816711426\n",
      "Epoch 2880, Average Loss: 9.11162452697754\n",
      "Epoch 2890, Average Loss: 9.09882469177246\n",
      "Epoch 2900, Average Loss: 9.082425022125244\n",
      "Epoch 2910, Average Loss: 9.064759445190429\n",
      "Epoch 2920, Average Loss: 9.053205680847167\n",
      "Epoch 2930, Average Loss: 9.036250686645507\n",
      "Epoch 2940, Average Loss: 9.02289638519287\n",
      "Epoch 2950, Average Loss: 9.009943199157714\n",
      "Epoch 2960, Average Loss: 8.996045398712159\n",
      "Epoch 2970, Average Loss: 8.989116096496582\n",
      "Epoch 2980, Average Loss: 8.966735553741454\n",
      "Epoch 2990, Average Loss: 8.956068801879884\n",
      "Epoch 3000, Average Loss: 8.937725448608399\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.构建测试集图数据对象",
   "id": "3e942a1212ab0648"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:57:57.082317Z",
     "start_time": "2024-11-20T16:57:57.027308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe, calculate_metrics\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load('gnn_best_model.pth', weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 转换测试数据为张量\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建测试集图数据对象\n",
    "test_data = Data(x=X_test_torch, edge_index=edge_index, y=y_test_torch).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 对训练集进行预测\n",
    "    out = model(train_data)\n",
    "    print(\"训练集预测结果:\")\n",
    "    print(out)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(train_data.y.cpu().numpy(), out.cpu().numpy())\n",
    "    print(\"训练集指标:\", train_metrics)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_out = model(test_data)\n",
    "    print(\"测试集预测结果:\")\n",
    "    print(test_out)\n",
    "\n",
    "    # 计算测试集的指标\n",
    "    test_metrics = calculate_metrics(test_data.y.cpu().numpy(), test_out.cpu().numpy())\n",
    "    print(\"测试集指标:\", test_metrics)\n",
    "\n",
    "    # 保存指标到CSV文件\n",
    "    metrics_df = metrics_to_dataframe(train_data.y.cpu().numpy(), out.cpu().numpy(),\n",
    "                                      test_data.y.cpu().numpy(), test_out.cpu().numpy(), 'GNN').round(3)\n",
    "    metrics_df.to_csv('gnn_metrics.csv', index=False)\n",
    "\n",
    "    print(metrics_df)\n"
   ],
   "id": "116c1a7b5eabdeb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测结果:\n",
      "tensor([[ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [ 43.6256],\n",
      "        [135.7022],\n",
      "        [118.6652],\n",
      "        [ 79.1230],\n",
      "        [ 51.6472],\n",
      "        [ 84.7457],\n",
      "        [135.8450],\n",
      "        [ 76.0553],\n",
      "        [ 91.1397],\n",
      "        [ 49.0745],\n",
      "        [113.0814],\n",
      "        [ 45.1397],\n",
      "        [124.4234],\n",
      "        [121.4571],\n",
      "        [ 66.8061],\n",
      "        [125.8277],\n",
      "        [ 92.2471],\n",
      "        [ 41.1090],\n",
      "        [103.9780],\n",
      "        [ 83.7652],\n",
      "        [ 62.0723],\n",
      "        [ 42.3530],\n",
      "        [103.1193],\n",
      "        [ 46.8105],\n",
      "        [ 96.5112],\n",
      "        [ 61.0572],\n",
      "        [112.5227],\n",
      "        [ 69.1724],\n",
      "        [102.5574],\n",
      "        [ 23.9035],\n",
      "        [ 81.6876],\n",
      "        [ 80.6603],\n",
      "        [116.9810],\n",
      "        [ 59.5095],\n",
      "        [ 80.1431],\n",
      "        [ 45.7775],\n",
      "        [ 46.1962],\n",
      "        [109.6701],\n",
      "        [ 52.6954],\n",
      "        [ 64.1724],\n",
      "        [ 55.6644],\n",
      "        [133.7504],\n",
      "        [ 14.6851],\n",
      "        [126.2074],\n",
      "        [ 34.6364],\n",
      "        [ 76.4352],\n",
      "        [ 63.1526],\n",
      "        [ 85.9496],\n",
      "        [110.8888],\n",
      "        [ 81.9543],\n",
      "        [115.7465],\n",
      "        [ 93.4618],\n",
      "        [ 49.4467],\n",
      "        [ 36.7734],\n",
      "        [ 75.6985],\n",
      "        [ 59.2433],\n",
      "        [ 77.4553],\n",
      "        [ 56.1983],\n",
      "        [ 37.9609],\n",
      "        [ 94.2884],\n",
      "        [102.2747],\n",
      "        [ 51.5156],\n",
      "        [ 79.2735],\n",
      "        [ 93.1205],\n",
      "        [ 82.2447],\n",
      "        [ 28.7840],\n",
      "        [206.0469],\n",
      "        [ 44.0060],\n",
      "        [ 67.1888],\n",
      "        [ 97.8515],\n",
      "        [ 78.3874],\n",
      "        [ 31.3582],\n",
      "        [110.2102],\n",
      "        [106.8606],\n",
      "        [ 84.2153],\n",
      "        [185.9445],\n",
      "        [ 93.0315],\n",
      "        [ 91.0252],\n",
      "        [ 73.4177],\n",
      "        [108.8006],\n",
      "        [ 74.9723],\n",
      "        [110.1670],\n",
      "        [155.0109],\n",
      "        [129.8412],\n",
      "        [145.0450],\n",
      "        [ 38.4393],\n",
      "        [104.9109],\n",
      "        [ 62.2451],\n",
      "        [117.9787],\n",
      "        [ 69.9566],\n",
      "        [ 81.3512],\n",
      "        [ 56.3885],\n",
      "        [ 29.2885],\n",
      "        [ 88.6711],\n",
      "        [ 94.7454],\n",
      "        [ 41.0045],\n",
      "        [104.7611],\n",
      "        [ 87.7255],\n",
      "        [107.6390],\n",
      "        [122.7043],\n",
      "        [ 98.6338],\n",
      "        [ 81.3109],\n",
      "        [ 71.1977],\n",
      "        [ 81.3248],\n",
      "        [ 45.6454],\n",
      "        [150.9124],\n",
      "        [ 25.1118],\n",
      "        [ 66.4752],\n",
      "        [138.4172],\n",
      "        [ 60.0787],\n",
      "        [ 86.4655],\n",
      "        [180.5165],\n",
      "        [ 70.2513],\n",
      "        [111.0913],\n",
      "        [105.9642],\n",
      "        [ 33.8633],\n",
      "        [ 44.9620],\n",
      "        [ 33.2165],\n",
      "        [ 45.2006],\n",
      "        [102.1698],\n",
      "        [103.1572],\n",
      "        [ 92.1865],\n",
      "        [ 63.7457],\n",
      "        [ 86.1003],\n",
      "        [ 43.6124],\n",
      "        [ 89.1455],\n",
      "        [ 48.6179],\n",
      "        [ 61.6125],\n",
      "        [ 67.2204],\n",
      "        [107.1236],\n",
      "        [155.9876],\n",
      "        [125.8882],\n",
      "        [ 96.6552],\n",
      "        [120.8915],\n",
      "        [139.8637],\n",
      "        [139.3345],\n",
      "        [114.6231],\n",
      "        [ 69.0375],\n",
      "        [110.4577],\n",
      "        [ 75.6953],\n",
      "        [ 62.0538],\n",
      "        [ 89.0458],\n",
      "        [ 85.0717],\n",
      "        [ 98.3100],\n",
      "        [168.4224],\n",
      "        [ 73.2107],\n",
      "        [ 85.2012],\n",
      "        [ 97.6052],\n",
      "        [ 87.1357],\n",
      "        [ 30.5179],\n",
      "        [101.0519],\n",
      "        [ 80.1136],\n",
      "        [131.9842],\n",
      "        [112.4415],\n",
      "        [ 74.9167],\n",
      "        [ 62.3641],\n",
      "        [ 60.6943],\n",
      "        [ 85.4477],\n",
      "        [142.2045],\n",
      "        [145.7372],\n",
      "        [ 78.5762],\n",
      "        [102.3290],\n",
      "        [ 84.2764],\n",
      "        [ 79.7140],\n",
      "        [130.9172],\n",
      "        [ 68.7745],\n",
      "        [ 50.4068],\n",
      "        [ 80.7137],\n",
      "        [111.8501],\n",
      "        [ 55.3636],\n",
      "        [ 95.5227],\n",
      "        [ 66.5431],\n",
      "        [104.1724],\n",
      "        [111.3160],\n",
      "        [105.6289],\n",
      "        [115.2906],\n",
      "        [ 42.7394],\n",
      "        [ 58.3827],\n",
      "        [110.4719],\n",
      "        [ 59.3709],\n",
      "        [ 51.3656],\n",
      "        [108.3565],\n",
      "        [ 53.8222],\n",
      "        [157.3120],\n",
      "        [106.3881],\n",
      "        [ 71.4200],\n",
      "        [134.7914],\n",
      "        [ 98.1350],\n",
      "        [ 78.3571],\n",
      "        [104.3040],\n",
      "        [ 46.5545],\n",
      "        [ 67.1847],\n",
      "        [ 72.1043],\n",
      "        [117.3249],\n",
      "        [149.7285],\n",
      "        [119.7925],\n",
      "        [112.7756],\n",
      "        [ 97.2140],\n",
      "        [ 76.4808],\n",
      "        [126.7980],\n",
      "        [ 81.0028],\n",
      "        [108.8228],\n",
      "        [110.4298],\n",
      "        [ 49.1062],\n",
      "        [ 75.6972],\n",
      "        [ 90.0812],\n",
      "        [137.5278],\n",
      "        [ 76.8281],\n",
      "        [102.7287],\n",
      "        [ 42.3131],\n",
      "        [ 57.0994],\n",
      "        [105.7028],\n",
      "        [117.0593],\n",
      "        [ 53.8878],\n",
      "        [ 92.1182],\n",
      "        [ 92.2302],\n",
      "        [ 66.3216],\n",
      "        [149.3719],\n",
      "        [ 45.5507],\n",
      "        [130.9921],\n",
      "        [ 73.7010],\n",
      "        [ 96.9723],\n",
      "        [ 50.4581],\n",
      "        [ 81.4664],\n",
      "        [133.7488],\n",
      "        [127.6805],\n",
      "        [157.9104],\n",
      "        [ 84.1147],\n",
      "        [ 90.2423],\n",
      "        [115.2040],\n",
      "        [ 96.2975],\n",
      "        [ 76.8537],\n",
      "        [107.6205],\n",
      "        [102.2591],\n",
      "        [ 42.1828],\n",
      "        [131.1286],\n",
      "        [ 86.2646],\n",
      "        [ 44.8928],\n",
      "        [160.2556],\n",
      "        [124.6427],\n",
      "        [ 94.0803],\n",
      "        [ 51.8270],\n",
      "        [ 80.0098],\n",
      "        [ 87.6365],\n",
      "        [ 40.1759],\n",
      "        [ 23.2077],\n",
      "        [ 83.7285],\n",
      "        [ 43.4249],\n",
      "        [135.5168],\n",
      "        [ 84.5152],\n",
      "        [ 39.0778],\n",
      "        [ 65.9521],\n",
      "        [159.4709],\n",
      "        [ 86.8582],\n",
      "        [ 57.2401],\n",
      "        [102.9370],\n",
      "        [ 70.9039],\n",
      "        [ 82.6120],\n",
      "        [147.2966],\n",
      "        [ 64.8795],\n",
      "        [ 96.3564],\n",
      "        [ 91.4976],\n",
      "        [ 44.8071],\n",
      "        [ 46.5307],\n",
      "        [168.1242],\n",
      "        [112.5032],\n",
      "        [105.3663],\n",
      "        [ 27.0834],\n",
      "        [ 79.7611],\n",
      "        [173.5013],\n",
      "        [110.2991],\n",
      "        [ 86.8716],\n",
      "        [ 17.3469],\n",
      "        [ 62.4414],\n",
      "        [ 68.6181],\n",
      "        [ 30.4292],\n",
      "        [ 85.2119],\n",
      "        [ 78.1654],\n",
      "        [ 73.6140],\n",
      "        [ 71.8205],\n",
      "        [113.5946],\n",
      "        [ 80.2999],\n",
      "        [ 41.4137],\n",
      "        [ 27.1893],\n",
      "        [ 56.8449],\n",
      "        [106.9622],\n",
      "        [ 55.1958],\n",
      "        [ 56.9689],\n",
      "        [108.9425],\n",
      "        [ 87.2749],\n",
      "        [ 43.5496],\n",
      "        [ 98.2137],\n",
      "        [180.0097],\n",
      "        [ 87.9251],\n",
      "        [163.7457],\n",
      "        [ 88.3167],\n",
      "        [ 92.9580],\n",
      "        [ 65.5914],\n",
      "        [ 98.9097],\n",
      "        [136.8739],\n",
      "        [106.3004],\n",
      "        [ 97.5882],\n",
      "        [126.5817],\n",
      "        [ 76.8365],\n",
      "        [157.2101],\n",
      "        [ 72.4129],\n",
      "        [ 74.8766],\n",
      "        [ 93.1972],\n",
      "        [ 94.9141],\n",
      "        [ 67.5064],\n",
      "        [ 64.5252],\n",
      "        [ 91.2962],\n",
      "        [138.2598],\n",
      "        [ 89.9511],\n",
      "        [ 98.1735],\n",
      "        [ 89.3166],\n",
      "        [110.8648],\n",
      "        [ 82.3251],\n",
      "        [103.5768],\n",
      "        [ 85.1397],\n",
      "        [116.3232],\n",
      "        [ 39.0487],\n",
      "        [ 65.8549],\n",
      "        [100.5780],\n",
      "        [113.4701],\n",
      "        [ 77.7601],\n",
      "        [ 76.7569],\n",
      "        [ 75.5439],\n",
      "        [ 86.9731],\n",
      "        [144.2132],\n",
      "        [ 39.6113],\n",
      "        [119.2340],\n",
      "        [ 60.6161],\n",
      "        [ 71.6571],\n",
      "        [ 70.8471],\n",
      "        [128.4752],\n",
      "        [116.4580],\n",
      "        [ 62.0119],\n",
      "        [ 69.1284],\n",
      "        [ 31.5430],\n",
      "        [ 89.6268],\n",
      "        [109.6845],\n",
      "        [ 43.2240],\n",
      "        [118.7343],\n",
      "        [105.6276],\n",
      "        [ 76.0952],\n",
      "        [ 86.2295],\n",
      "        [ 93.5827],\n",
      "        [122.5170],\n",
      "        [103.4018],\n",
      "        [ 83.5154],\n",
      "        [107.4780],\n",
      "        [135.3210],\n",
      "        [ 81.8220],\n",
      "        [ 30.3767],\n",
      "        [ 78.6330],\n",
      "        [ 56.5542],\n",
      "        [ 94.7123],\n",
      "        [ 60.1768],\n",
      "        [ 65.6985],\n",
      "        [ 74.3872],\n",
      "        [ 26.9374],\n",
      "        [ 84.9828],\n",
      "        [ 59.3306],\n",
      "        [ 60.6899],\n",
      "        [209.8643],\n",
      "        [ 93.7374],\n",
      "        [166.0038],\n",
      "        [127.4263],\n",
      "        [ 83.1378],\n",
      "        [ 62.7054],\n",
      "        [ 68.7363],\n",
      "        [ 78.1758],\n",
      "        [ 72.5522],\n",
      "        [164.9885],\n",
      "        [116.0055],\n",
      "        [ 75.8976],\n",
      "        [106.6801],\n",
      "        [ 46.8637],\n",
      "        [ 48.8422],\n",
      "        [ 36.5969],\n",
      "        [120.8152],\n",
      "        [ 21.6466],\n",
      "        [112.3059],\n",
      "        [108.5303],\n",
      "        [ 86.4988],\n",
      "        [ 58.1380],\n",
      "        [ 67.9788],\n",
      "        [141.7595],\n",
      "        [ 39.1235],\n",
      "        [124.4395],\n",
      "        [120.6577],\n",
      "        [121.3121],\n",
      "        [ 72.1413],\n",
      "        [109.2725],\n",
      "        [ 72.9573],\n",
      "        [ 70.5545],\n",
      "        [ 89.1944],\n",
      "        [117.2732],\n",
      "        [ 60.3101],\n",
      "        [ 73.5568],\n",
      "        [149.5726],\n",
      "        [188.9169],\n",
      "        [111.6118],\n",
      "        [ 60.8323],\n",
      "        [ 82.5552],\n",
      "        [ 77.5792],\n",
      "        [ 80.1473],\n",
      "        [ 83.3624],\n",
      "        [ 86.5277],\n",
      "        [ 44.4743],\n",
      "        [ 20.6966],\n",
      "        [ 43.0966],\n",
      "        [114.7510],\n",
      "        [ 80.1546],\n",
      "        [113.7748],\n",
      "        [119.0911],\n",
      "        [ 23.7603],\n",
      "        [ 57.3017],\n",
      "        [ 96.4116],\n",
      "        [ 77.5206],\n",
      "        [ 95.2592],\n",
      "        [ 77.8365],\n",
      "        [ 66.6348],\n",
      "        [ 96.2160],\n",
      "        [123.0462],\n",
      "        [115.8589],\n",
      "        [ 68.8511],\n",
      "        [ 84.5684],\n",
      "        [ 89.1764],\n",
      "        [ 38.6812],\n",
      "        [ 96.4071],\n",
      "        [159.6754],\n",
      "        [115.9362],\n",
      "        [ 70.8024],\n",
      "        [ 99.4257],\n",
      "        [ 94.3175],\n",
      "        [ 16.9967],\n",
      "        [ 28.6498],\n",
      "        [ 81.7952],\n",
      "        [ 80.6722],\n",
      "        [113.6075],\n",
      "        [ 40.8042],\n",
      "        [ 63.4748],\n",
      "        [ 93.2439],\n",
      "        [102.6347],\n",
      "        [124.2436],\n",
      "        [ 55.8394],\n",
      "        [ 92.5015],\n",
      "        [119.6830],\n",
      "        [ 46.7452],\n",
      "        [ 67.9663],\n",
      "        [ 83.1152],\n",
      "        [ 83.8649],\n",
      "        [136.9924],\n",
      "        [ 59.2751],\n",
      "        [130.5168],\n",
      "        [ 58.6879],\n",
      "        [ 93.1893],\n",
      "        [ 74.6887],\n",
      "        [ 74.2001],\n",
      "        [ 84.9871],\n",
      "        [132.3710],\n",
      "        [ 79.1041],\n",
      "        [134.1225],\n",
      "        [ 88.3682],\n",
      "        [111.1331],\n",
      "        [134.1163],\n",
      "        [107.7569],\n",
      "        [ 67.2017],\n",
      "        [ 39.4362],\n",
      "        [120.0958],\n",
      "        [ 88.4659],\n",
      "        [140.4245]], device='cuda:0')\n",
      "训练集指标: (0.7654480934143066, 9.791675, 8.923248946666718, 19.333813)\n",
      "测试集预测结果:\n",
      "tensor([[ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 42.3983],\n",
      "        [ 94.0190],\n",
      "        [104.7489],\n",
      "        [140.3362],\n",
      "        [ 30.0976],\n",
      "        [ 42.4094],\n",
      "        [ 94.6578],\n",
      "        [ 43.0658],\n",
      "        [ 70.4284],\n",
      "        [ 33.3514],\n",
      "        [ 87.1266],\n",
      "        [113.0506],\n",
      "        [120.3349],\n",
      "        [111.1147],\n",
      "        [113.3505],\n",
      "        [ 80.3176],\n",
      "        [ 70.3318],\n",
      "        [127.0461],\n",
      "        [ 61.4516],\n",
      "        [124.1656],\n",
      "        [ 75.4600],\n",
      "        [ 54.1554],\n",
      "        [ 42.8377],\n",
      "        [ 63.1673],\n",
      "        [ 83.8748],\n",
      "        [ 48.1489],\n",
      "        [ 47.0286],\n",
      "        [ 82.4506],\n",
      "        [ 73.4105],\n",
      "        [108.7527],\n",
      "        [ 32.5555],\n",
      "        [ 70.7009],\n",
      "        [ 92.0885],\n",
      "        [111.8112],\n",
      "        [121.0506],\n",
      "        [ 50.6902],\n",
      "        [ 67.1830],\n",
      "        [135.3359],\n",
      "        [ 94.9070],\n",
      "        [100.8987],\n",
      "        [ 57.8535],\n",
      "        [ 66.4795],\n",
      "        [ 68.8856],\n",
      "        [111.3575],\n",
      "        [149.7153],\n",
      "        [121.6214],\n",
      "        [ 39.1841],\n",
      "        [ 99.4432],\n",
      "        [ 74.8017],\n",
      "        [ 27.1236],\n",
      "        [ 86.6458],\n",
      "        [ 79.3799],\n",
      "        [ 72.7959],\n",
      "        [104.5491],\n",
      "        [ 68.6787],\n",
      "        [103.9492],\n",
      "        [153.7921],\n",
      "        [108.8919],\n",
      "        [ 88.0295],\n",
      "        [ 39.2286],\n",
      "        [ 65.2463],\n",
      "        [102.6724],\n",
      "        [ 59.7706],\n",
      "        [155.7154],\n",
      "        [ 70.8969],\n",
      "        [ 95.2940],\n",
      "        [ 81.6237],\n",
      "        [ 66.2640],\n",
      "        [108.8247],\n",
      "        [ 81.9614],\n",
      "        [ 88.3040],\n",
      "        [ 67.6465],\n",
      "        [190.5718],\n",
      "        [129.3740],\n",
      "        [132.5726],\n",
      "        [ 36.6638],\n",
      "        [ 37.9982],\n",
      "        [ 72.4635],\n",
      "        [ 77.6265],\n",
      "        [ 74.8385],\n",
      "        [ 67.1557],\n",
      "        [121.9498],\n",
      "        [134.0896],\n",
      "        [ 85.2597],\n",
      "        [138.1994],\n",
      "        [ 69.1754],\n",
      "        [111.8245],\n",
      "        [101.4588],\n",
      "        [ 40.3815],\n",
      "        [153.8461],\n",
      "        [ 51.0007],\n",
      "        [ 59.5708],\n",
      "        [118.2483],\n",
      "        [109.5081],\n",
      "        [ 54.8926],\n",
      "        [ 96.4790],\n",
      "        [102.1333],\n",
      "        [170.7608],\n",
      "        [114.0165],\n",
      "        [156.9128],\n",
      "        [178.1781],\n",
      "        [ 92.1368],\n",
      "        [115.0920],\n",
      "        [143.9468],\n",
      "        [141.7459],\n",
      "        [ 52.1748],\n",
      "        [ 79.9118],\n",
      "        [ 51.3277],\n",
      "        [ 96.0877],\n",
      "        [ 49.4903],\n",
      "        [ 92.0063],\n",
      "        [ 69.5542],\n",
      "        [ 99.8526],\n",
      "        [ 88.6046]], device='cuda:0')\n",
      "测试集指标: (0.7343084812164307, 11.735264, 12.629784643650055, 20.456295)\n",
      "  model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  MAE_test  \\\n",
      "0   GNN     0.765      9.792       8.923      19.334    0.734    11.735   \n",
      "\n",
      "   MAPE_test  RMSE_test  \n",
      "0      12.63  20.455999  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:57:09.404779Z",
     "start_time": "2024-11-20T16:57:09.396034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "train_predictions = pd.DataFrame({'Actual': train_data.y.cpu().detach().numpy().flatten(),\n",
    "                                  'Predicted': model(train_data).cpu().detach().numpy().flatten()})\n",
    "test_predictions = pd.DataFrame({'Actual': test_data.y.cpu().detach().numpy().flatten(),\n",
    "                                 'Predicted': model(test_data).cpu().detach().numpy().flatten()})\n",
    "\n",
    "train_predictions.to_csv('gnn_train_predictions.csv', index=False)\n",
    "test_predictions.to_csv('gnn_test_predictions.csv', index=False)"
   ],
   "id": "610248f1bf86c0d1",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
