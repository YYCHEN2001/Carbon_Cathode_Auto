{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:02.142523Z",
     "start_time": "2024-11-21T02:43:00.841574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e7d30d5c108ee240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 读取数据",
   "id": "31d7c4f1b9247569"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:02.758814Z",
     "start_time": "2024-11-21T02:43:02.145661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "b295e86f637a2b4c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建图数据对象, 转换数据为图数据结构",
   "id": "e364287c13c7204b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:04.095910Z",
     "start_time": "2024-11-21T02:43:02.813818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 材料特性的索引和测试条件的索引\n",
    "material_indices = list(range(7))  # 前七个特性\n",
    "test_indices = list(range(7, 12))  # 后五个条件\n",
    "\n",
    "# 构建边：仅为材料特性之间构建边\n",
    "edges = []\n",
    "for i in material_indices:\n",
    "    for j in material_indices:\n",
    "        if i != j:\n",
    "            edges.append([i, j])\n",
    "\n",
    "# 初始化边列表\n",
    "# edges = []\n",
    "#\n",
    "# # 0和1有边\n",
    "# edges.append([0, 1])\n",
    "# edges.append([1, 0])  # 如果是无向图，需要添加反向边\n",
    "#\n",
    "# # 2-3-4-5相互有边\n",
    "# node_group = [2, 3, 4, 5]\n",
    "# for i in range(len(node_group)):\n",
    "#     for j in range(i + 1, len(node_group)):\n",
    "#         edges.append([node_group[i], node_group[j]])\n",
    "#         edges.append([node_group[j], node_group[i]])  # 如果是无向图，需要添加反向边\n",
    "#\n",
    "# # 0、1、2、3分别与6有边\n",
    "# for node in [0, 1, 2, 3]:\n",
    "#     edges.append([node, 6])\n",
    "#     edges.append([6, node])  # 如果是无向图，需要添加反向边\n",
    "#\n",
    "# # 0和2有边\n",
    "# edges.append([0, 2])\n",
    "# edges.append([2, 0])  # 如果是无向图，需要添加反向边\n",
    "\n",
    "# 转换为Tensor\n",
    "edges = np.array(edges).T  # 转置以匹配PyTorch Geometric的edge_index格式\n",
    "\n",
    "print(edges)\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建图数据对象\n",
    "train_data = Data(x=X_train_torch, edge_index=edge_index, y=y_train_torch).to(device)\n",
    "\n",
    "# 打印数据对象信息，确认构建是否成功\n",
    "print(train_data)"
   ],
   "id": "6fd7f6e086ddd296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5 5 5\n",
      "  6 6 6 6 6 6]\n",
      " [1 2 3 4 5 6 0 2 3 4 5 6 0 1 3 4 5 6 0 1 2 4 5 6 0 1 2 3 5 6 0 1 2 3 4 6\n",
      "  0 1 2 3 4 5]]\n",
      "Data(x=[480, 12], edge_index=[2, 42], y=[480, 1])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "6f857b4b31231496"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:04.148517Z",
     "start_time": "2024-11-21T02:43:04.112148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn, optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class MAPELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        epsilon = 1e-8  # 避免除以零\n",
    "        mape = torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 100\n",
    "        return mape\n",
    "\n",
    "class GNN4TDL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN4TDL, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 24)\n",
    "        self.conv2 = GCNConv(24, 48)\n",
    "        self.conv3 = GCNConv(48, 1)\n",
    "        # self.conv4 = GCNConv(24, 1)\n",
    "        # self.conv5 = GCNConv(24, 1)\n",
    "        # self.conv6 = GCNConv(70, 30)\n",
    "        # self.conv7 = GCNConv(30, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv4(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv5(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv6(x, edge_index)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.conv7(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN4TDL(\n",
    "    input_dim=X_train_scaled.shape[1]\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 定义损失函数\n",
    "mse_loss = nn.MSELoss().to(device)\n",
    "mape_loss = MAPELoss().to(device)\n",
    "## 训练模型"
   ],
   "id": "18cb525202dbc92f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:19.172762Z",
     "start_time": "2024-11-21T02:43:04.162536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "patience = 30  # 允许的最大连续未改进 epoch 数\n",
    "epochs_without_improvement = 0  # 连续未改进的 epoch 数\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = mape_loss(out, train_data.y)  # Modify as per your loss function, e.g., mape_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cumulative_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss\n",
    "\n",
    "    # 计算验证损失\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 在 GPU 上进行预测\n",
    "        y_val_pred = model(train_data).to(device)\n",
    "        # 验证损失计算时，确保 y_test_tensor 也在同一个设备上\n",
    "        train_data.y = train_data.y.to(device)\n",
    "        val_loss = mape_loss(y_val_pred, train_data.y).item()  # 计算验证损失\n",
    "\n",
    "    # 判断验证损失是否改善\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        epochs_without_improvement = 0  # 重置计数器\n",
    "        # 保存最佳模型\n",
    "        torch.save(model.state_dict(), \"gnn_best_model.pth\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    # 如果验证损失在一定次数的 epoch 内没有改进，则停止训练\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break"
   ],
   "id": "49aa5bf124c4f90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 99.74763641357421\n",
      "Epoch 20, Average Loss: 99.03573226928711\n",
      "Epoch 30, Average Loss: 98.26322326660156\n",
      "Epoch 40, Average Loss: 97.36977996826172\n",
      "Epoch 50, Average Loss: 96.26036148071289\n",
      "Epoch 60, Average Loss: 94.83712768554688\n",
      "Epoch 70, Average Loss: 93.00299911499023\n",
      "Epoch 80, Average Loss: 90.64888992309571\n",
      "Epoch 90, Average Loss: 87.6869384765625\n",
      "Epoch 100, Average Loss: 84.31017684936523\n",
      "Epoch 110, Average Loss: 80.70155563354493\n",
      "Epoch 120, Average Loss: 76.93786849975587\n",
      "Epoch 130, Average Loss: 73.09059143066406\n",
      "Epoch 140, Average Loss: 69.05624771118164\n",
      "Epoch 150, Average Loss: 64.86262359619141\n",
      "Epoch 160, Average Loss: 60.77327346801758\n",
      "Epoch 170, Average Loss: 56.97887649536133\n",
      "Epoch 180, Average Loss: 53.389325332641604\n",
      "Epoch 190, Average Loss: 50.094200897216794\n",
      "Epoch 200, Average Loss: 47.20117073059082\n",
      "Epoch 210, Average Loss: 44.70979042053223\n",
      "Epoch 220, Average Loss: 42.61548080444336\n",
      "Epoch 230, Average Loss: 40.909960174560545\n",
      "Epoch 240, Average Loss: 39.49262657165527\n",
      "Epoch 250, Average Loss: 38.219228744506836\n",
      "Epoch 260, Average Loss: 37.0087215423584\n",
      "Epoch 270, Average Loss: 35.89358978271484\n",
      "Epoch 280, Average Loss: 34.882433319091795\n",
      "Epoch 290, Average Loss: 33.95105972290039\n",
      "Epoch 300, Average Loss: 33.09726829528809\n",
      "Epoch 310, Average Loss: 32.36409301757813\n",
      "Epoch 320, Average Loss: 31.719807052612303\n",
      "Epoch 330, Average Loss: 31.132967948913574\n",
      "Epoch 340, Average Loss: 30.611387252807617\n",
      "Epoch 350, Average Loss: 30.124428749084473\n",
      "Epoch 360, Average Loss: 29.664606666564943\n",
      "Epoch 370, Average Loss: 29.250461196899415\n",
      "Epoch 380, Average Loss: 28.86717643737793\n",
      "Epoch 390, Average Loss: 28.50974178314209\n",
      "Epoch 400, Average Loss: 28.183970069885255\n",
      "Epoch 410, Average Loss: 27.87325839996338\n",
      "Epoch 420, Average Loss: 27.58630542755127\n",
      "Epoch 430, Average Loss: 27.318477058410643\n",
      "Epoch 440, Average Loss: 27.06838893890381\n",
      "Epoch 450, Average Loss: 26.830043601989747\n",
      "Epoch 460, Average Loss: 26.60662078857422\n",
      "Epoch 470, Average Loss: 26.393941116333007\n",
      "Epoch 480, Average Loss: 26.185462188720702\n",
      "Epoch 490, Average Loss: 25.98790626525879\n",
      "Epoch 500, Average Loss: 25.796514892578124\n",
      "Epoch 510, Average Loss: 25.6106819152832\n",
      "Epoch 520, Average Loss: 25.43451805114746\n",
      "Epoch 530, Average Loss: 25.260943984985353\n",
      "Epoch 540, Average Loss: 25.083660697937013\n",
      "Epoch 550, Average Loss: 24.900870895385744\n",
      "Epoch 560, Average Loss: 24.730669212341308\n",
      "Epoch 570, Average Loss: 24.567243576049805\n",
      "Epoch 580, Average Loss: 24.407173538208006\n",
      "Epoch 590, Average Loss: 24.247200584411623\n",
      "Epoch 600, Average Loss: 24.082523918151857\n",
      "Epoch 610, Average Loss: 23.916608810424805\n",
      "Epoch 620, Average Loss: 23.745928001403808\n",
      "Epoch 630, Average Loss: 23.572718620300293\n",
      "Epoch 640, Average Loss: 23.392693328857423\n",
      "Epoch 650, Average Loss: 23.216042137145998\n",
      "Epoch 660, Average Loss: 23.041730690002442\n",
      "Epoch 670, Average Loss: 22.869696426391602\n",
      "Epoch 680, Average Loss: 22.698291778564453\n",
      "Epoch 690, Average Loss: 22.5323637008667\n",
      "Epoch 700, Average Loss: 22.373660469055174\n",
      "Epoch 710, Average Loss: 22.216549682617188\n",
      "Epoch 720, Average Loss: 22.058687210083008\n",
      "Epoch 730, Average Loss: 21.897922706604003\n",
      "Epoch 740, Average Loss: 21.739230728149415\n",
      "Epoch 750, Average Loss: 21.582053184509277\n",
      "Epoch 760, Average Loss: 21.424668884277345\n",
      "Epoch 770, Average Loss: 21.267409324645996\n",
      "Epoch 780, Average Loss: 21.110234642028807\n",
      "Epoch 790, Average Loss: 20.951564979553222\n",
      "Epoch 800, Average Loss: 20.791890335083007\n",
      "Epoch 810, Average Loss: 20.63800811767578\n",
      "Epoch 820, Average Loss: 20.486988639831544\n",
      "Epoch 830, Average Loss: 20.337638092041015\n",
      "Epoch 840, Average Loss: 20.195701026916502\n",
      "Epoch 850, Average Loss: 20.057195472717286\n",
      "Epoch 860, Average Loss: 19.91749973297119\n",
      "Epoch 870, Average Loss: 19.773921012878418\n",
      "Epoch 880, Average Loss: 19.628149604797365\n",
      "Epoch 890, Average Loss: 19.47776527404785\n",
      "Epoch 900, Average Loss: 19.324740982055665\n",
      "Epoch 910, Average Loss: 19.175980186462404\n",
      "Epoch 920, Average Loss: 19.03279342651367\n",
      "Epoch 930, Average Loss: 18.89352970123291\n",
      "Epoch 940, Average Loss: 18.754804039001463\n",
      "Epoch 950, Average Loss: 18.61485710144043\n",
      "Epoch 960, Average Loss: 18.47372703552246\n",
      "Epoch 970, Average Loss: 18.33929557800293\n",
      "Epoch 980, Average Loss: 18.20087890625\n",
      "Epoch 990, Average Loss: 18.06355724334717\n",
      "Epoch 1000, Average Loss: 17.93435935974121\n",
      "Epoch 1010, Average Loss: 17.808719062805174\n",
      "Epoch 1020, Average Loss: 17.683777618408204\n",
      "Epoch 1030, Average Loss: 17.55888843536377\n",
      "Epoch 1040, Average Loss: 17.431330108642577\n",
      "Epoch 1050, Average Loss: 17.303705596923827\n",
      "Epoch 1060, Average Loss: 17.17901020050049\n",
      "Epoch 1070, Average Loss: 17.051566696166994\n",
      "Epoch 1080, Average Loss: 16.92247829437256\n",
      "Epoch 1090, Average Loss: 16.80278606414795\n",
      "Epoch 1100, Average Loss: 16.687874603271485\n",
      "Epoch 1110, Average Loss: 16.57159423828125\n",
      "Epoch 1120, Average Loss: 16.45354804992676\n",
      "Epoch 1130, Average Loss: 16.337902450561522\n",
      "Epoch 1140, Average Loss: 16.223448753356934\n",
      "Epoch 1150, Average Loss: 16.118041610717775\n",
      "Epoch 1160, Average Loss: 16.014247798919676\n",
      "Epoch 1170, Average Loss: 15.906150245666504\n",
      "Epoch 1180, Average Loss: 15.802129173278809\n",
      "Epoch 1190, Average Loss: 15.707355117797851\n",
      "Epoch 1200, Average Loss: 15.613803005218506\n",
      "Epoch 1210, Average Loss: 15.529963970184326\n",
      "Epoch 1220, Average Loss: 15.436140537261963\n",
      "Epoch 1230, Average Loss: 15.345352745056152\n",
      "Epoch 1240, Average Loss: 15.255278587341309\n",
      "Epoch 1250, Average Loss: 15.163034629821777\n",
      "Epoch 1260, Average Loss: 15.07737627029419\n",
      "Epoch 1270, Average Loss: 14.996684265136718\n",
      "Epoch 1280, Average Loss: 14.917060089111327\n",
      "Epoch 1290, Average Loss: 14.833176326751708\n",
      "Epoch 1300, Average Loss: 14.750380706787109\n",
      "Epoch 1310, Average Loss: 14.66563539505005\n",
      "Epoch 1320, Average Loss: 14.580522632598877\n",
      "Epoch 1330, Average Loss: 14.497091102600098\n",
      "Epoch 1340, Average Loss: 14.415270805358887\n",
      "Epoch 1350, Average Loss: 14.333716869354248\n",
      "Epoch 1360, Average Loss: 14.248457908630371\n",
      "Epoch 1370, Average Loss: 14.166629219055176\n",
      "Epoch 1380, Average Loss: 14.085455799102784\n",
      "Epoch 1390, Average Loss: 14.000717258453369\n",
      "Epoch 1400, Average Loss: 13.920456504821777\n",
      "Epoch 1410, Average Loss: 13.84005527496338\n",
      "Epoch 1420, Average Loss: 13.768711757659911\n",
      "Epoch 1430, Average Loss: 13.701168632507324\n",
      "Epoch 1440, Average Loss: 13.619026947021485\n",
      "Epoch 1450, Average Loss: 13.547262191772461\n",
      "Epoch 1460, Average Loss: 13.479399108886719\n",
      "Epoch 1470, Average Loss: 13.416692543029786\n",
      "Epoch 1480, Average Loss: 13.355239963531494\n",
      "Epoch 1490, Average Loss: 13.29089412689209\n",
      "Epoch 1500, Average Loss: 13.226441955566406\n",
      "Epoch 1510, Average Loss: 13.16332187652588\n",
      "Epoch 1520, Average Loss: 13.102316951751709\n",
      "Epoch 1530, Average Loss: 13.052943706512451\n",
      "Epoch 1540, Average Loss: 12.994166374206543\n",
      "Epoch 1550, Average Loss: 12.94000597000122\n",
      "Epoch 1560, Average Loss: 12.887182998657227\n",
      "Epoch 1570, Average Loss: 12.838265419006348\n",
      "Epoch 1580, Average Loss: 12.790997219085693\n",
      "Epoch 1590, Average Loss: 12.738449382781983\n",
      "Epoch 1600, Average Loss: 12.684040069580078\n",
      "Epoch 1610, Average Loss: 12.634334087371826\n",
      "Epoch 1620, Average Loss: 12.587897396087646\n",
      "Epoch 1630, Average Loss: 12.541385936737061\n",
      "Epoch 1640, Average Loss: 12.497319602966309\n",
      "Epoch 1650, Average Loss: 12.442792797088623\n",
      "Epoch 1660, Average Loss: 12.392104816436767\n",
      "Epoch 1670, Average Loss: 12.33790397644043\n",
      "Epoch 1680, Average Loss: 12.285698795318604\n",
      "Epoch 1690, Average Loss: 12.235257053375244\n",
      "Epoch 1700, Average Loss: 12.190553569793702\n",
      "Epoch 1710, Average Loss: 12.148343563079834\n",
      "Epoch 1720, Average Loss: 12.09577112197876\n",
      "Epoch 1730, Average Loss: 12.04592227935791\n",
      "Epoch 1740, Average Loss: 11.999431133270264\n",
      "Epoch 1750, Average Loss: 11.95509376525879\n",
      "Epoch 1760, Average Loss: 11.913846492767334\n",
      "Epoch 1770, Average Loss: 11.880540466308593\n",
      "Epoch 1780, Average Loss: 11.841272449493408\n",
      "Epoch 1790, Average Loss: 11.790597724914551\n",
      "Epoch 1800, Average Loss: 11.75222864151001\n",
      "Epoch 1810, Average Loss: 11.712316799163819\n",
      "Epoch 1820, Average Loss: 11.676517772674561\n",
      "Epoch 1830, Average Loss: 11.636638355255126\n",
      "Epoch 1840, Average Loss: 11.5984468460083\n",
      "Epoch 1850, Average Loss: 11.566056632995606\n",
      "Epoch 1860, Average Loss: 11.534695434570313\n",
      "Epoch 1870, Average Loss: 11.500803470611572\n",
      "Epoch 1880, Average Loss: 11.46833257675171\n",
      "Epoch 1890, Average Loss: 11.443015956878662\n",
      "Epoch 1900, Average Loss: 11.41173095703125\n",
      "Epoch 1910, Average Loss: 11.37752571105957\n",
      "Epoch 1920, Average Loss: 11.348038291931152\n",
      "Epoch 1930, Average Loss: 11.31555995941162\n",
      "Epoch 1940, Average Loss: 11.285183334350586\n",
      "Epoch 1950, Average Loss: 11.25307388305664\n",
      "Epoch 1960, Average Loss: 11.223514175415039\n",
      "Epoch 1970, Average Loss: 11.197770023345948\n",
      "Epoch 1980, Average Loss: 11.165832233428954\n",
      "Epoch 1990, Average Loss: 11.134545230865479\n",
      "Epoch 2000, Average Loss: 11.106184673309325\n",
      "Epoch 2010, Average Loss: 11.081407737731933\n",
      "Epoch 2020, Average Loss: 11.050611019134521\n",
      "Epoch 2030, Average Loss: 11.025666618347168\n",
      "Epoch 2040, Average Loss: 10.994388484954834\n",
      "Epoch 2050, Average Loss: 10.968434810638428\n",
      "Epoch 2060, Average Loss: 10.942437076568604\n",
      "Epoch 2070, Average Loss: 10.917952156066894\n",
      "Epoch 2080, Average Loss: 10.891875267028809\n",
      "Epoch 2090, Average Loss: 10.864419746398926\n",
      "Epoch 2100, Average Loss: 10.84032745361328\n",
      "Epoch 2110, Average Loss: 10.81682014465332\n",
      "Epoch 2120, Average Loss: 10.792299461364745\n",
      "Epoch 2130, Average Loss: 10.768810653686524\n",
      "Epoch 2140, Average Loss: 10.744097900390624\n",
      "Epoch 2150, Average Loss: 10.721238136291504\n",
      "Epoch 2160, Average Loss: 10.697104167938232\n",
      "Epoch 2170, Average Loss: 10.668932914733887\n",
      "Epoch 2180, Average Loss: 10.646321773529053\n",
      "Epoch 2190, Average Loss: 10.618797397613525\n",
      "Epoch 2200, Average Loss: 10.596723556518555\n",
      "Epoch 2210, Average Loss: 10.579998970031738\n",
      "Epoch 2220, Average Loss: 10.550663566589355\n",
      "Epoch 2230, Average Loss: 10.531035709381104\n",
      "Epoch 2240, Average Loss: 10.505935287475586\n",
      "Epoch 2250, Average Loss: 10.483758926391602\n",
      "Epoch 2260, Average Loss: 10.464507102966309\n",
      "Epoch 2270, Average Loss: 10.440433692932128\n",
      "Epoch 2280, Average Loss: 10.418180370330811\n",
      "Epoch 2290, Average Loss: 10.396423053741454\n",
      "Epoch 2300, Average Loss: 10.372741603851319\n",
      "Epoch 2310, Average Loss: 10.34870491027832\n",
      "Epoch 2320, Average Loss: 10.330453014373779\n",
      "Epoch 2330, Average Loss: 10.30743055343628\n",
      "Epoch 2340, Average Loss: 10.293955516815185\n",
      "Epoch 2350, Average Loss: 10.268185806274413\n",
      "Epoch 2360, Average Loss: 10.2437237739563\n",
      "Epoch 2370, Average Loss: 10.221553516387939\n",
      "Epoch 2380, Average Loss: 10.198147392272949\n",
      "Epoch 2390, Average Loss: 10.18225793838501\n",
      "Epoch 2400, Average Loss: 10.164268302917481\n",
      "Epoch 2410, Average Loss: 10.146464538574218\n",
      "Epoch 2420, Average Loss: 10.13138132095337\n",
      "Epoch 2430, Average Loss: 10.110743522644043\n",
      "Epoch 2440, Average Loss: 10.093696212768554\n",
      "Epoch 2450, Average Loss: 10.078919410705566\n",
      "Epoch 2460, Average Loss: 10.059570693969727\n",
      "Epoch 2470, Average Loss: 10.04270248413086\n",
      "Epoch 2480, Average Loss: 10.024348831176757\n",
      "Epoch 2490, Average Loss: 9.998106384277344\n",
      "Epoch 2500, Average Loss: 9.982173728942872\n",
      "Epoch 2510, Average Loss: 9.96889419555664\n",
      "Epoch 2520, Average Loss: 9.94485969543457\n",
      "Epoch 2530, Average Loss: 9.925618743896484\n",
      "Epoch 2540, Average Loss: 9.911280918121339\n",
      "Epoch 2550, Average Loss: 9.887636756896972\n",
      "Epoch 2560, Average Loss: 9.86462469100952\n",
      "Epoch 2570, Average Loss: 9.842455005645752\n",
      "Epoch 2580, Average Loss: 9.81833848953247\n",
      "Epoch 2590, Average Loss: 9.805624961853027\n",
      "Epoch 2600, Average Loss: 9.78116340637207\n",
      "Epoch 2610, Average Loss: 9.762826251983643\n",
      "Epoch 2620, Average Loss: 9.742868041992187\n",
      "Epoch 2630, Average Loss: 9.721603775024414\n",
      "Epoch 2640, Average Loss: 9.701128578186035\n",
      "Epoch 2650, Average Loss: 9.687709331512451\n",
      "Epoch 2660, Average Loss: 9.66672716140747\n",
      "Epoch 2670, Average Loss: 9.642803573608399\n",
      "Epoch 2680, Average Loss: 9.629718399047851\n",
      "Epoch 2690, Average Loss: 9.612764453887939\n",
      "Epoch 2700, Average Loss: 9.597112274169922\n",
      "Epoch 2710, Average Loss: 9.575838661193847\n",
      "Epoch 2720, Average Loss: 9.564024925231934\n",
      "Epoch 2730, Average Loss: 9.548986625671386\n",
      "Epoch 2740, Average Loss: 9.534685325622558\n",
      "Epoch 2750, Average Loss: 9.520359802246094\n",
      "Epoch 2760, Average Loss: 9.505494499206543\n",
      "Epoch 2770, Average Loss: 9.488675212860107\n",
      "Epoch 2780, Average Loss: 9.47910213470459\n",
      "Epoch 2790, Average Loss: 9.460995101928711\n",
      "Epoch 2800, Average Loss: 9.445988273620605\n",
      "Epoch 2810, Average Loss: 9.43506669998169\n",
      "Epoch 2820, Average Loss: 9.418062305450439\n",
      "Epoch 2830, Average Loss: 9.399467468261719\n",
      "Epoch 2840, Average Loss: 9.386966705322266\n",
      "Epoch 2850, Average Loss: 9.36569423675537\n",
      "Epoch 2860, Average Loss: 9.35324592590332\n",
      "Epoch 2870, Average Loss: 9.339577198028564\n",
      "Epoch 2880, Average Loss: 9.315743446350098\n",
      "Epoch 2890, Average Loss: 9.300434970855713\n",
      "Epoch 2900, Average Loss: 9.28018569946289\n",
      "Epoch 2910, Average Loss: 9.263008403778077\n",
      "Epoch 2920, Average Loss: 9.24842700958252\n",
      "Epoch 2930, Average Loss: 9.23031759262085\n",
      "Epoch 2940, Average Loss: 9.204062175750732\n",
      "Epoch 2950, Average Loss: 9.185238552093505\n",
      "Epoch 2960, Average Loss: 9.168570613861084\n",
      "Epoch 2970, Average Loss: 9.147746562957764\n",
      "Epoch 2980, Average Loss: 9.132991790771484\n",
      "Epoch 2990, Average Loss: 9.109178352355958\n",
      "Epoch 3000, Average Loss: 9.09185209274292\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.构建测试集图数据对象",
   "id": "3e942a1212ab0648"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:19.592404Z",
     "start_time": "2024-11-21T02:43:19.240778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe, calculate_metrics\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load('gnn_best_model.pth', weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 转换测试数据为张量\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建测试集图数据对象\n",
    "test_data = Data(x=X_test_torch, edge_index=edge_index, y=y_test_torch).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 对训练集进行预测\n",
    "    out = model(train_data)\n",
    "    print(\"训练集预测结果:\")\n",
    "    print(out)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(train_data.y.cpu().numpy(), out.cpu().numpy())\n",
    "    print(\"训练集指标:\", train_metrics)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_out = model(test_data)\n",
    "    print(\"测试集预测结果:\")\n",
    "    print(test_out)\n",
    "\n",
    "    # 计算测试集的指标\n",
    "    test_metrics = calculate_metrics(test_data.y.cpu().numpy(), test_out.cpu().numpy())\n",
    "    print(\"测试集指标:\", test_metrics)\n",
    "\n",
    "    # 保存指标到CSV文件\n",
    "    metrics_df = metrics_to_dataframe(train_data.y.cpu().numpy(), out.cpu().numpy(),\n",
    "                                      test_data.y.cpu().numpy(), test_out.cpu().numpy(), 'GNN').round(3)\n",
    "    metrics_df.to_csv('gnn_metrics.csv', index=False)\n",
    "\n",
    "    print(metrics_df)\n"
   ],
   "id": "116c1a7b5eabdeb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测结果:\n",
      "tensor([[ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [ 49.6322],\n",
      "        [134.5268],\n",
      "        [124.3092],\n",
      "        [ 78.6683],\n",
      "        [ 51.5766],\n",
      "        [ 81.7555],\n",
      "        [138.5731],\n",
      "        [ 71.9933],\n",
      "        [ 70.6052],\n",
      "        [ 49.0343],\n",
      "        [113.1268],\n",
      "        [ 45.1720],\n",
      "        [120.7163],\n",
      "        [124.2427],\n",
      "        [ 66.8808],\n",
      "        [137.3500],\n",
      "        [ 80.2200],\n",
      "        [ 41.2620],\n",
      "        [104.0936],\n",
      "        [ 85.9277],\n",
      "        [ 64.6213],\n",
      "        [ 40.6692],\n",
      "        [103.2055],\n",
      "        [ 41.6549],\n",
      "        [ 95.8690],\n",
      "        [ 61.1720],\n",
      "        [110.1070],\n",
      "        [ 68.9228],\n",
      "        [101.7975],\n",
      "        [ 23.9795],\n",
      "        [ 81.9069],\n",
      "        [ 77.3336],\n",
      "        [117.1922],\n",
      "        [ 61.1172],\n",
      "        [ 81.8908],\n",
      "        [ 43.8438],\n",
      "        [ 47.4775],\n",
      "        [106.8591],\n",
      "        [ 52.7342],\n",
      "        [ 58.2501],\n",
      "        [ 50.2414],\n",
      "        [129.5638],\n",
      "        [ 14.7522],\n",
      "        [128.4771],\n",
      "        [ 34.6231],\n",
      "        [ 74.8876],\n",
      "        [ 63.4263],\n",
      "        [ 80.7263],\n",
      "        [108.1970],\n",
      "        [ 73.8220],\n",
      "        [103.0122],\n",
      "        [ 88.2764],\n",
      "        [ 52.8203],\n",
      "        [ 33.3151],\n",
      "        [ 79.5060],\n",
      "        [ 59.2432],\n",
      "        [ 78.9682],\n",
      "        [ 56.6590],\n",
      "        [ 43.5869],\n",
      "        [ 89.8057],\n",
      "        [101.1223],\n",
      "        [ 54.4281],\n",
      "        [ 77.2115],\n",
      "        [ 80.5507],\n",
      "        [ 82.4766],\n",
      "        [ 28.9804],\n",
      "        [202.4300],\n",
      "        [ 47.4949],\n",
      "        [ 64.1292],\n",
      "        [106.2286],\n",
      "        [ 78.9606],\n",
      "        [ 35.6453],\n",
      "        [114.8551],\n",
      "        [105.2419],\n",
      "        [ 79.1666],\n",
      "        [184.4165],\n",
      "        [ 85.2741],\n",
      "        [ 91.1539],\n",
      "        [ 74.9626],\n",
      "        [107.1932],\n",
      "        [ 72.6180],\n",
      "        [112.7043],\n",
      "        [152.7980],\n",
      "        [125.2524],\n",
      "        [136.5368],\n",
      "        [ 38.7166],\n",
      "        [105.1638],\n",
      "        [ 64.8433],\n",
      "        [121.8706],\n",
      "        [ 66.3846],\n",
      "        [ 85.5869],\n",
      "        [ 56.4550],\n",
      "        [ 28.2523],\n",
      "        [ 87.1149],\n",
      "        [ 93.4114],\n",
      "        [ 41.0556],\n",
      "        [103.1907],\n",
      "        [ 88.0471],\n",
      "        [104.6294],\n",
      "        [132.0562],\n",
      "        [ 98.8458],\n",
      "        [ 78.0206],\n",
      "        [ 68.2935],\n",
      "        [ 76.5671],\n",
      "        [ 45.4913],\n",
      "        [143.2156],\n",
      "        [ 31.5040],\n",
      "        [ 68.7390],\n",
      "        [139.1492],\n",
      "        [ 46.8707],\n",
      "        [ 94.4007],\n",
      "        [178.5604],\n",
      "        [ 66.3419],\n",
      "        [108.4268],\n",
      "        [113.0643],\n",
      "        [ 31.2776],\n",
      "        [ 43.0502],\n",
      "        [ 33.1826],\n",
      "        [ 44.8504],\n",
      "        [102.0459],\n",
      "        [103.7208],\n",
      "        [ 93.6279],\n",
      "        [ 59.4721],\n",
      "        [ 86.1812],\n",
      "        [ 43.6096],\n",
      "        [ 87.2128],\n",
      "        [ 46.8788],\n",
      "        [ 61.7832],\n",
      "        [ 65.2407],\n",
      "        [104.1219],\n",
      "        [150.9617],\n",
      "        [127.5553],\n",
      "        [104.1269],\n",
      "        [126.8261],\n",
      "        [134.6931],\n",
      "        [134.1687],\n",
      "        [114.3718],\n",
      "        [ 69.2417],\n",
      "        [ 95.4949],\n",
      "        [ 79.3982],\n",
      "        [ 64.1213],\n",
      "        [ 86.6398],\n",
      "        [ 86.9780],\n",
      "        [ 97.6527],\n",
      "        [164.2135],\n",
      "        [ 67.6375],\n",
      "        [ 85.5301],\n",
      "        [ 98.5636],\n",
      "        [ 92.3827],\n",
      "        [ 30.6314],\n",
      "        [101.0987],\n",
      "        [ 81.1207],\n",
      "        [133.3017],\n",
      "        [112.6968],\n",
      "        [ 78.0161],\n",
      "        [ 63.2759],\n",
      "        [ 63.0215],\n",
      "        [ 85.3685],\n",
      "        [137.5935],\n",
      "        [144.4758],\n",
      "        [ 61.4326],\n",
      "        [ 94.3496],\n",
      "        [ 86.1158],\n",
      "        [ 73.9867],\n",
      "        [136.2871],\n",
      "        [ 68.5468],\n",
      "        [ 41.0685],\n",
      "        [ 79.0819],\n",
      "        [112.8134],\n",
      "        [ 55.7464],\n",
      "        [ 95.8386],\n",
      "        [ 56.8107],\n",
      "        [102.3922],\n",
      "        [115.1362],\n",
      "        [106.4386],\n",
      "        [113.1779],\n",
      "        [ 41.5987],\n",
      "        [ 56.5130],\n",
      "        [116.1531],\n",
      "        [ 59.5619],\n",
      "        [ 51.3059],\n",
      "        [108.4986],\n",
      "        [ 59.9467],\n",
      "        [153.6485],\n",
      "        [103.4205],\n",
      "        [ 72.1474],\n",
      "        [135.7174],\n",
      "        [104.2947],\n",
      "        [ 75.3637],\n",
      "        [ 97.9465],\n",
      "        [ 45.6618],\n",
      "        [ 66.9004],\n",
      "        [ 70.4941],\n",
      "        [119.5657],\n",
      "        [147.2330],\n",
      "        [114.8179],\n",
      "        [114.0364],\n",
      "        [ 99.2197],\n",
      "        [ 79.9576],\n",
      "        [123.6655],\n",
      "        [ 81.0373],\n",
      "        [109.7003],\n",
      "        [110.0810],\n",
      "        [ 49.0150],\n",
      "        [ 72.4245],\n",
      "        [ 91.7667],\n",
      "        [137.5989],\n",
      "        [ 76.0178],\n",
      "        [103.3398],\n",
      "        [ 42.4162],\n",
      "        [ 53.4640],\n",
      "        [ 99.6945],\n",
      "        [118.7712],\n",
      "        [ 53.7242],\n",
      "        [ 92.6437],\n",
      "        [ 90.9645],\n",
      "        [ 65.1474],\n",
      "        [145.6077],\n",
      "        [ 45.5918],\n",
      "        [121.6967],\n",
      "        [ 75.5765],\n",
      "        [ 99.6532],\n",
      "        [ 50.5916],\n",
      "        [ 77.6630],\n",
      "        [136.2186],\n",
      "        [126.6821],\n",
      "        [155.8663],\n",
      "        [ 81.6406],\n",
      "        [ 92.1635],\n",
      "        [101.0712],\n",
      "        [103.2718],\n",
      "        [ 74.2673],\n",
      "        [ 89.4627],\n",
      "        [101.4936],\n",
      "        [ 42.1099],\n",
      "        [133.3308],\n",
      "        [ 88.2712],\n",
      "        [ 37.2454],\n",
      "        [158.2904],\n",
      "        [130.7729],\n",
      "        [ 97.2120],\n",
      "        [ 48.2959],\n",
      "        [ 76.6466],\n",
      "        [ 89.5645],\n",
      "        [ 43.2625],\n",
      "        [ 23.4451],\n",
      "        [ 84.2455],\n",
      "        [ 44.7512],\n",
      "        [130.4973],\n",
      "        [ 82.2135],\n",
      "        [ 39.1183],\n",
      "        [ 66.5759],\n",
      "        [164.9509],\n",
      "        [ 91.3588],\n",
      "        [ 52.6024],\n",
      "        [105.7653],\n",
      "        [ 70.3501],\n",
      "        [ 79.3945],\n",
      "        [144.9053],\n",
      "        [ 64.8918],\n",
      "        [ 97.1386],\n",
      "        [ 99.9306],\n",
      "        [ 44.8823],\n",
      "        [ 49.2918],\n",
      "        [159.6353],\n",
      "        [111.0339],\n",
      "        [ 93.0387],\n",
      "        [ 26.8856],\n",
      "        [ 79.5979],\n",
      "        [174.0341],\n",
      "        [113.9085],\n",
      "        [ 90.6965],\n",
      "        [ 17.5125],\n",
      "        [ 62.2636],\n",
      "        [ 66.8446],\n",
      "        [ 30.3834],\n",
      "        [ 90.1293],\n",
      "        [ 81.7357],\n",
      "        [ 71.2042],\n",
      "        [ 69.7438],\n",
      "        [115.0098],\n",
      "        [ 81.8048],\n",
      "        [ 36.5741],\n",
      "        [ 27.2578],\n",
      "        [ 58.8875],\n",
      "        [107.4840],\n",
      "        [ 42.2842],\n",
      "        [ 56.3457],\n",
      "        [109.7679],\n",
      "        [ 84.4421],\n",
      "        [ 29.8963],\n",
      "        [ 93.3649],\n",
      "        [179.4787],\n",
      "        [ 91.7743],\n",
      "        [159.0645],\n",
      "        [ 87.1061],\n",
      "        [ 89.9197],\n",
      "        [ 65.5392],\n",
      "        [ 98.2473],\n",
      "        [150.0246],\n",
      "        [106.5155],\n",
      "        [ 97.2991],\n",
      "        [116.3536],\n",
      "        [ 78.2484],\n",
      "        [152.0325],\n",
      "        [ 72.3727],\n",
      "        [ 75.4851],\n",
      "        [ 93.0632],\n",
      "        [ 91.8775],\n",
      "        [ 62.7088],\n",
      "        [ 62.8614],\n",
      "        [ 89.1386],\n",
      "        [133.1197],\n",
      "        [ 97.0733],\n",
      "        [ 98.7251],\n",
      "        [ 89.3395],\n",
      "        [ 96.0481],\n",
      "        [ 79.9433],\n",
      "        [100.1701],\n",
      "        [ 88.7416],\n",
      "        [103.7480],\n",
      "        [ 27.4193],\n",
      "        [ 66.5083],\n",
      "        [101.7868],\n",
      "        [111.6956],\n",
      "        [ 77.6147],\n",
      "        [ 73.2117],\n",
      "        [ 72.6833],\n",
      "        [ 86.4962],\n",
      "        [138.1773],\n",
      "        [ 39.6329],\n",
      "        [126.1741],\n",
      "        [ 60.9567],\n",
      "        [ 72.9952],\n",
      "        [ 71.5585],\n",
      "        [130.2820],\n",
      "        [113.5926],\n",
      "        [ 60.9265],\n",
      "        [ 69.7915],\n",
      "        [ 31.5683],\n",
      "        [ 79.2278],\n",
      "        [101.9702],\n",
      "        [ 41.4629],\n",
      "        [119.7828],\n",
      "        [106.0512],\n",
      "        [ 72.1290],\n",
      "        [ 86.6215],\n",
      "        [ 92.8961],\n",
      "        [126.0665],\n",
      "        [ 99.6315],\n",
      "        [ 78.9009],\n",
      "        [118.0918],\n",
      "        [137.9512],\n",
      "        [ 77.6986],\n",
      "        [ 30.2556],\n",
      "        [ 77.9490],\n",
      "        [ 57.1552],\n",
      "        [ 94.0853],\n",
      "        [ 57.8290],\n",
      "        [ 58.8887],\n",
      "        [ 78.8448],\n",
      "        [ 29.7081],\n",
      "        [ 85.1178],\n",
      "        [ 59.1020],\n",
      "        [ 53.0660],\n",
      "        [178.9397],\n",
      "        [ 94.5918],\n",
      "        [157.0198],\n",
      "        [127.5279],\n",
      "        [ 83.2018],\n",
      "        [ 66.0608],\n",
      "        [ 68.7862],\n",
      "        [ 74.5532],\n",
      "        [ 62.8506],\n",
      "        [164.6651],\n",
      "        [116.1671],\n",
      "        [ 73.3462],\n",
      "        [111.3809],\n",
      "        [ 46.9260],\n",
      "        [ 52.2286],\n",
      "        [ 36.5287],\n",
      "        [124.4930],\n",
      "        [ 21.6836],\n",
      "        [116.6258],\n",
      "        [106.1247],\n",
      "        [ 91.6434],\n",
      "        [ 57.1300],\n",
      "        [ 67.7949],\n",
      "        [137.0261],\n",
      "        [ 36.3847],\n",
      "        [134.9972],\n",
      "        [119.6317],\n",
      "        [121.2384],\n",
      "        [ 69.3746],\n",
      "        [112.4139],\n",
      "        [ 74.1466],\n",
      "        [ 71.4211],\n",
      "        [ 85.1390],\n",
      "        [118.8358],\n",
      "        [ 60.1238],\n",
      "        [ 70.9963],\n",
      "        [146.6615],\n",
      "        [186.8854],\n",
      "        [ 95.3372],\n",
      "        [ 64.3160],\n",
      "        [ 82.8365],\n",
      "        [ 75.4243],\n",
      "        [ 88.0136],\n",
      "        [ 83.9187],\n",
      "        [ 81.2462],\n",
      "        [ 44.5020],\n",
      "        [ 20.4353],\n",
      "        [ 43.3227],\n",
      "        [116.3621],\n",
      "        [ 74.9358],\n",
      "        [113.2380],\n",
      "        [122.4705],\n",
      "        [ 23.3627],\n",
      "        [ 50.6188],\n",
      "        [ 95.6862],\n",
      "        [ 74.0076],\n",
      "        [ 89.7698],\n",
      "        [ 77.7996],\n",
      "        [ 66.0072],\n",
      "        [ 89.9857],\n",
      "        [128.2329],\n",
      "        [121.1631],\n",
      "        [ 69.8435],\n",
      "        [ 89.3722],\n",
      "        [ 80.5087],\n",
      "        [ 37.4027],\n",
      "        [ 96.6784],\n",
      "        [157.6883],\n",
      "        [117.7583],\n",
      "        [ 68.3909],\n",
      "        [ 98.5875],\n",
      "        [ 95.4802],\n",
      "        [ 17.1079],\n",
      "        [ 29.6604],\n",
      "        [ 82.5451],\n",
      "        [ 80.6760],\n",
      "        [ 98.5228],\n",
      "        [ 41.0328],\n",
      "        [ 62.8939],\n",
      "        [ 87.9139],\n",
      "        [ 96.5897],\n",
      "        [130.1379],\n",
      "        [ 55.4526],\n",
      "        [ 92.3955],\n",
      "        [131.2137],\n",
      "        [ 50.9196],\n",
      "        [ 67.8010],\n",
      "        [ 83.5305],\n",
      "        [ 82.6811],\n",
      "        [136.9962],\n",
      "        [ 59.3651],\n",
      "        [130.6727],\n",
      "        [ 59.9001],\n",
      "        [ 97.3583],\n",
      "        [ 71.1716],\n",
      "        [ 76.2401],\n",
      "        [ 85.0569],\n",
      "        [132.5596],\n",
      "        [ 78.9879],\n",
      "        [134.1010],\n",
      "        [ 87.6948],\n",
      "        [ 97.1300],\n",
      "        [128.2518],\n",
      "        [110.8396],\n",
      "        [ 61.5976],\n",
      "        [ 39.3449],\n",
      "        [121.5749],\n",
      "        [ 87.6796],\n",
      "        [135.4863]], device='cuda:0')\n",
      "训练集指标: (0.7600659132003784, 9.971547, 9.078767895698547, 19.554377)\n",
      "测试集预测结果:\n",
      "tensor([[ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 56.5025],\n",
      "        [ 95.4229],\n",
      "        [104.7175],\n",
      "        [130.5261],\n",
      "        [ 32.6005],\n",
      "        [ 41.0565],\n",
      "        [ 94.7135],\n",
      "        [ 47.3989],\n",
      "        [ 70.3616],\n",
      "        [ 37.2075],\n",
      "        [ 85.3512],\n",
      "        [112.7627],\n",
      "        [126.1969],\n",
      "        [109.5936],\n",
      "        [108.1588],\n",
      "        [ 84.6751],\n",
      "        [ 72.8796],\n",
      "        [126.1113],\n",
      "        [ 60.4175],\n",
      "        [120.0076],\n",
      "        [ 75.8885],\n",
      "        [ 60.7781],\n",
      "        [ 38.4594],\n",
      "        [ 57.3247],\n",
      "        [ 85.1829],\n",
      "        [ 51.0545],\n",
      "        [ 50.4535],\n",
      "        [ 92.3422],\n",
      "        [ 75.8294],\n",
      "        [109.5183],\n",
      "        [ 33.3154],\n",
      "        [ 75.0466],\n",
      "        [ 91.9539],\n",
      "        [112.1641],\n",
      "        [125.0579],\n",
      "        [ 54.1548],\n",
      "        [ 67.0430],\n",
      "        [135.1820],\n",
      "        [ 91.2706],\n",
      "        [ 80.4092],\n",
      "        [ 52.2249],\n",
      "        [ 62.8880],\n",
      "        [ 62.7481],\n",
      "        [108.6429],\n",
      "        [150.5139],\n",
      "        [120.5470],\n",
      "        [ 33.9420],\n",
      "        [ 97.9580],\n",
      "        [ 80.3811],\n",
      "        [ 22.0785],\n",
      "        [ 86.2746],\n",
      "        [ 80.3262],\n",
      "        [ 69.2568],\n",
      "        [101.6670],\n",
      "        [ 66.9486],\n",
      "        [111.5414],\n",
      "        [149.1770],\n",
      "        [109.2471],\n",
      "        [ 88.4481],\n",
      "        [ 31.0752],\n",
      "        [ 72.4961],\n",
      "        [104.7016],\n",
      "        [ 58.8906],\n",
      "        [152.0779],\n",
      "        [ 71.7318],\n",
      "        [ 93.4456],\n",
      "        [ 81.7569],\n",
      "        [ 66.8466],\n",
      "        [110.4365],\n",
      "        [ 78.7075],\n",
      "        [ 85.2205],\n",
      "        [ 62.9189],\n",
      "        [188.3668],\n",
      "        [126.6146],\n",
      "        [132.1584],\n",
      "        [ 35.7602],\n",
      "        [ 36.7010],\n",
      "        [ 76.8272],\n",
      "        [ 75.2967],\n",
      "        [ 78.0599],\n",
      "        [ 64.4518],\n",
      "        [125.5420],\n",
      "        [135.1135],\n",
      "        [ 77.5743],\n",
      "        [133.3138],\n",
      "        [ 76.5727],\n",
      "        [115.7499],\n",
      "        [109.6378],\n",
      "        [ 33.9901],\n",
      "        [146.5550],\n",
      "        [ 55.2318],\n",
      "        [ 65.8014],\n",
      "        [117.3435],\n",
      "        [108.6574],\n",
      "        [ 53.7080],\n",
      "        [ 93.4438],\n",
      "        [104.1396],\n",
      "        [166.8939],\n",
      "        [100.8047],\n",
      "        [153.1670],\n",
      "        [177.0619],\n",
      "        [ 89.6542],\n",
      "        [118.2034],\n",
      "        [140.0876],\n",
      "        [141.7187],\n",
      "        [ 48.4394],\n",
      "        [ 87.3042],\n",
      "        [ 49.3036],\n",
      "        [ 93.0522],\n",
      "        [ 50.2438],\n",
      "        [ 90.4072],\n",
      "        [ 71.0115],\n",
      "        [ 94.3284],\n",
      "        [ 97.6312]], device='cuda:0')\n",
      "测试集指标: (0.7792774438858032, 10.542228, 10.930801928043365, 18.644962)\n",
      "  model  R2_train  MAE_train  MAPE_train  RMSE_train  R2_test  MAE_test  \\\n",
      "0   GNN      0.76      9.972       9.079   19.554001    0.779    10.542   \n",
      "\n",
      "   MAPE_test  RMSE_test  \n",
      "0     10.931     18.645  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-21T02:43:19.654839Z",
     "start_time": "2024-11-21T02:43:19.644202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "train_predictions = pd.DataFrame({'Actual': train_data.y.cpu().detach().numpy().flatten(),\n",
    "                                  'Predicted': model(train_data).cpu().detach().numpy().flatten()})\n",
    "test_predictions = pd.DataFrame({'Actual': test_data.y.cpu().detach().numpy().flatten(),\n",
    "                                 'Predicted': model(test_data).cpu().detach().numpy().flatten()})\n",
    "\n",
    "train_predictions.to_csv('gnn_train_predictions.csv', index=False)\n",
    "test_predictions.to_csv('gnn_test_predictions.csv', index=False)"
   ],
   "id": "610248f1bf86c0d1",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
