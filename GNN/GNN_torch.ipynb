{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:17.083493Z",
     "start_time": "2024-11-20T16:11:15.567528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e7d30d5c108ee240",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 读取数据",
   "id": "31d7c4f1b9247569"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:17.823868Z",
     "start_time": "2024-11-20T16:11:17.095456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(\"../data/dataset.csv\")\n",
    "\n",
    "# 数据分割\n",
    "data['target_class'] = pd.qcut(data['Cs'], q=10, labels=False)\n",
    "X = data.drop(['Cs', 'target_class'], axis=1).values\n",
    "y = data['Cs'].values\n",
    "stratify_column = data['target_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=stratify_column)\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "id": "b295e86f637a2b4c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建图数据对象, 转换数据为图数据结构",
   "id": "e364287c13c7204b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:19.427021Z",
     "start_time": "2024-11-20T16:11:17.878092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# 材料特性的索引和测试条件的索引\n",
    "material_indices = list(range(7))  # 前七个特性\n",
    "test_indices = list(range(7, 12))  # 后五个条件\n",
    "\n",
    "# 构建边：仅为材料特性之间构建边\n",
    "edges = []\n",
    "for i in material_indices:\n",
    "    for j in material_indices:\n",
    "        if i != j:\n",
    "            edges.append([i, j])\n",
    "\n",
    "edges = np.array(edges).T  # 转置以匹配PyTorch Geometric的edge_index格式\n",
    "\n",
    "# 转换为PyTorch张量\n",
    "edge_index = torch.tensor(edges, dtype=torch.long)\n",
    "X_train_torch = torch.tensor(X_train_scaled, dtype=torch.float)\n",
    "y_train_torch = torch.tensor(y_train, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建图数据对象\n",
    "train_data = Data(x=X_train_torch, edge_index=edge_index, y=y_train_torch).to(device)\n",
    "\n",
    "# 打印数据对象信息，确认构建是否成功\n",
    "print(train_data)"
   ],
   "id": "6fd7f6e086ddd296",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[480, 12], edge_index=[2, 42], y=[480, 1])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "6f857b4b31231496"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:19.447505Z",
     "start_time": "2024-11-20T16:11:19.443688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class MAPELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        epsilon = 1e-8  # 避免除以零\n",
    "        mape = torch.mean(torch.abs((targets - predictions) / (targets + epsilon))) * 100\n",
    "        return mape\n",
    "\n",
    "class GNN4TDL(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GNN4TDL, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 12)\n",
    "        self.conv2 = GCNConv(12, 90)\n",
    "        self.conv3 = GCNConv(90, 90)\n",
    "        self.conv4 = GCNConv(90, 60)\n",
    "        self.conv5 = GCNConv(60, 70)\n",
    "        self.conv6 = GCNConv(70, 30)\n",
    "        self.conv7 = GCNConv(30, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv6(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv7(x, edge_index)\n",
    "        return x\n"
   ],
   "id": "18cb525202dbc92f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:19.502755Z",
     "start_time": "2024-11-20T16:11:19.463607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import optim\n",
    "\n",
    "model = GNN4TDL(\n",
    "    input_dim=X_train_scaled.shape[1]\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# mse_loss = nn.MSELoss().to(device)\n",
    "mape_loss = MAPELoss().to(device)"
   ],
   "id": "da93f66b9c3027e9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 训练模型",
   "id": "ccaffc254eeae169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:35.005700Z",
     "start_time": "2024-11-20T16:11:19.517531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 3000\n",
    "best_loss = float('inf')\n",
    "cumulative_loss = 0.0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.zero_grad()\n",
    "    out = model(train_data)\n",
    "    loss = mape_loss(out, train_data.y)  # Modify as per your loss function, e.g., mape_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    cumulative_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        average_loss = cumulative_loss / 10\n",
    "        print(f'Epoch {epoch+1}, Average Loss: {average_loss}')\n",
    "        cumulative_loss = 0.0  # Reset cumulative loss\n",
    "\n",
    "    # Save the best model\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), 'gnn_best_model.pth')"
   ],
   "id": "49aa5bf124c4f90d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 99.34335861206054\n",
      "Epoch 20, Average Loss: 95.8920280456543\n",
      "Epoch 30, Average Loss: 83.20887985229493\n",
      "Epoch 40, Average Loss: 59.976660537719724\n",
      "Epoch 50, Average Loss: 54.005150985717776\n",
      "Epoch 60, Average Loss: 47.51027717590332\n",
      "Epoch 70, Average Loss: 42.405466079711914\n",
      "Epoch 80, Average Loss: 38.30121841430664\n",
      "Epoch 90, Average Loss: 34.87344970703125\n",
      "Epoch 100, Average Loss: 32.11855621337891\n",
      "Epoch 110, Average Loss: 29.76716556549072\n",
      "Epoch 120, Average Loss: 27.826988983154298\n",
      "Epoch 130, Average Loss: 26.03719367980957\n",
      "Epoch 140, Average Loss: 24.484006118774413\n",
      "Epoch 150, Average Loss: 23.12535228729248\n",
      "Epoch 160, Average Loss: 21.841281700134278\n",
      "Epoch 170, Average Loss: 20.718341255187987\n",
      "Epoch 180, Average Loss: 19.788359451293946\n",
      "Epoch 190, Average Loss: 18.85236930847168\n",
      "Epoch 200, Average Loss: 17.870597076416015\n",
      "Epoch 210, Average Loss: 16.9438720703125\n",
      "Epoch 220, Average Loss: 16.117920303344725\n",
      "Epoch 230, Average Loss: 15.415686511993409\n",
      "Epoch 240, Average Loss: 14.861627006530762\n",
      "Epoch 250, Average Loss: 14.361828231811524\n",
      "Epoch 260, Average Loss: 13.983484363555908\n",
      "Epoch 270, Average Loss: 13.639286041259766\n",
      "Epoch 280, Average Loss: 13.37819414138794\n",
      "Epoch 290, Average Loss: 13.170643424987793\n",
      "Epoch 300, Average Loss: 12.94529323577881\n",
      "Epoch 310, Average Loss: 12.61695384979248\n",
      "Epoch 320, Average Loss: 12.421775817871094\n",
      "Epoch 330, Average Loss: 12.299590492248536\n",
      "Epoch 340, Average Loss: 12.080073165893555\n",
      "Epoch 350, Average Loss: 11.918263244628907\n",
      "Epoch 360, Average Loss: 11.8052809715271\n",
      "Epoch 370, Average Loss: 11.635009765625\n",
      "Epoch 380, Average Loss: 11.437623023986816\n",
      "Epoch 390, Average Loss: 11.308996200561523\n",
      "Epoch 400, Average Loss: 11.144619846343994\n",
      "Epoch 410, Average Loss: 11.05793399810791\n",
      "Epoch 420, Average Loss: 10.888392543792724\n",
      "Epoch 430, Average Loss: 10.760704135894775\n",
      "Epoch 440, Average Loss: 10.710199546813964\n",
      "Epoch 450, Average Loss: 10.632147216796875\n",
      "Epoch 460, Average Loss: 10.718417358398437\n",
      "Epoch 470, Average Loss: 10.376635456085205\n",
      "Epoch 480, Average Loss: 10.177153873443604\n",
      "Epoch 490, Average Loss: 10.48047170639038\n",
      "Epoch 500, Average Loss: 10.207472991943359\n",
      "Epoch 510, Average Loss: 10.017245292663574\n",
      "Epoch 520, Average Loss: 10.005526351928712\n",
      "Epoch 530, Average Loss: 9.795610618591308\n",
      "Epoch 540, Average Loss: 9.708077144622802\n",
      "Epoch 550, Average Loss: 9.680643939971924\n",
      "Epoch 560, Average Loss: 9.75559597015381\n",
      "Epoch 570, Average Loss: 9.627352809906006\n",
      "Epoch 580, Average Loss: 9.518555545806885\n",
      "Epoch 590, Average Loss: 9.431939888000489\n",
      "Epoch 600, Average Loss: 9.388724899291992\n",
      "Epoch 610, Average Loss: 9.570540809631348\n",
      "Epoch 620, Average Loss: 9.298583889007569\n",
      "Epoch 630, Average Loss: 9.399864673614502\n",
      "Epoch 640, Average Loss: 9.341747856140136\n",
      "Epoch 650, Average Loss: 9.487461948394776\n",
      "Epoch 660, Average Loss: 9.23970422744751\n",
      "Epoch 670, Average Loss: 9.210387802124023\n",
      "Epoch 680, Average Loss: 9.137378025054932\n",
      "Epoch 690, Average Loss: 9.084626388549804\n",
      "Epoch 700, Average Loss: 8.967911052703858\n",
      "Epoch 710, Average Loss: 9.018799781799316\n",
      "Epoch 720, Average Loss: 8.924279499053956\n",
      "Epoch 730, Average Loss: 8.827820968627929\n",
      "Epoch 740, Average Loss: 8.822162628173828\n",
      "Epoch 750, Average Loss: 8.920708751678466\n",
      "Epoch 760, Average Loss: 8.824098682403564\n",
      "Epoch 770, Average Loss: 8.785708045959472\n",
      "Epoch 780, Average Loss: 8.915770530700684\n",
      "Epoch 790, Average Loss: 8.687026214599609\n",
      "Epoch 800, Average Loss: 8.670751762390136\n",
      "Epoch 810, Average Loss: 8.584670257568359\n",
      "Epoch 820, Average Loss: 8.743282699584961\n",
      "Epoch 830, Average Loss: 8.619101524353027\n",
      "Epoch 840, Average Loss: 8.791584587097168\n",
      "Epoch 850, Average Loss: 8.631783199310302\n",
      "Epoch 860, Average Loss: 8.376632976531983\n",
      "Epoch 870, Average Loss: 8.372026824951172\n",
      "Epoch 880, Average Loss: 8.312868022918702\n",
      "Epoch 890, Average Loss: 8.37789478302002\n",
      "Epoch 900, Average Loss: 9.03582706451416\n",
      "Epoch 910, Average Loss: 8.706368350982666\n",
      "Epoch 920, Average Loss: 8.415999794006348\n",
      "Epoch 930, Average Loss: 8.42876091003418\n",
      "Epoch 940, Average Loss: 8.290562915802003\n",
      "Epoch 950, Average Loss: 8.118806457519531\n",
      "Epoch 960, Average Loss: 8.103245401382447\n",
      "Epoch 970, Average Loss: 8.119375658035278\n",
      "Epoch 980, Average Loss: 8.110008382797242\n",
      "Epoch 990, Average Loss: 8.397021150588989\n",
      "Epoch 1000, Average Loss: 8.934802627563476\n",
      "Epoch 1010, Average Loss: 8.959167671203613\n",
      "Epoch 1020, Average Loss: 8.141686487197877\n",
      "Epoch 1030, Average Loss: 8.06565351486206\n",
      "Epoch 1040, Average Loss: 7.969109582901001\n",
      "Epoch 1050, Average Loss: 8.045661163330077\n",
      "Epoch 1060, Average Loss: 7.932127380371094\n",
      "Epoch 1070, Average Loss: 7.9032816886901855\n",
      "Epoch 1080, Average Loss: 7.9150628566741945\n",
      "Epoch 1090, Average Loss: 7.918437433242798\n",
      "Epoch 1100, Average Loss: 7.754780292510986\n",
      "Epoch 1110, Average Loss: 7.68538646697998\n",
      "Epoch 1120, Average Loss: 7.6928962707519535\n",
      "Epoch 1130, Average Loss: 7.718431997299194\n",
      "Epoch 1140, Average Loss: 7.807225036621094\n",
      "Epoch 1150, Average Loss: 7.824369096755982\n",
      "Epoch 1160, Average Loss: 7.620833778381348\n",
      "Epoch 1170, Average Loss: 7.620423603057861\n",
      "Epoch 1180, Average Loss: 7.615186643600464\n",
      "Epoch 1190, Average Loss: 7.692456245422363\n",
      "Epoch 1200, Average Loss: 8.122594547271728\n",
      "Epoch 1210, Average Loss: 8.110026454925537\n",
      "Epoch 1220, Average Loss: 7.670224952697754\n",
      "Epoch 1230, Average Loss: 7.608553123474121\n",
      "Epoch 1240, Average Loss: 7.578047895431519\n",
      "Epoch 1250, Average Loss: 7.367606830596924\n",
      "Epoch 1260, Average Loss: 7.283212661743164\n",
      "Epoch 1270, Average Loss: 7.338502645492554\n",
      "Epoch 1280, Average Loss: 7.4357232570648195\n",
      "Epoch 1290, Average Loss: 7.426928949356079\n",
      "Epoch 1300, Average Loss: 7.304949855804443\n",
      "Epoch 1310, Average Loss: 7.186160898208618\n",
      "Epoch 1320, Average Loss: 7.229657363891602\n",
      "Epoch 1330, Average Loss: 7.3524689197540285\n",
      "Epoch 1340, Average Loss: 7.310231399536133\n",
      "Epoch 1350, Average Loss: 7.202205419540405\n",
      "Epoch 1360, Average Loss: 7.095454216003418\n",
      "Epoch 1370, Average Loss: 7.1800426006317135\n",
      "Epoch 1380, Average Loss: 7.0077050685882565\n",
      "Epoch 1390, Average Loss: 7.118473148345947\n",
      "Epoch 1400, Average Loss: 7.544789218902588\n",
      "Epoch 1410, Average Loss: 8.051784372329712\n",
      "Epoch 1420, Average Loss: 7.447653293609619\n",
      "Epoch 1430, Average Loss: 7.255109262466431\n",
      "Epoch 1440, Average Loss: 7.433280086517334\n",
      "Epoch 1450, Average Loss: 7.833713722229004\n",
      "Epoch 1460, Average Loss: 7.3113995552062985\n",
      "Epoch 1470, Average Loss: 7.085025310516357\n",
      "Epoch 1480, Average Loss: 6.916199636459351\n",
      "Epoch 1490, Average Loss: 6.837946510314941\n",
      "Epoch 1500, Average Loss: 7.000894260406494\n",
      "Epoch 1510, Average Loss: 6.775941562652588\n",
      "Epoch 1520, Average Loss: 6.686474943161011\n",
      "Epoch 1530, Average Loss: 6.751755857467652\n",
      "Epoch 1540, Average Loss: 7.131658554077148\n",
      "Epoch 1550, Average Loss: 6.964242219924927\n",
      "Epoch 1560, Average Loss: 6.823013210296631\n",
      "Epoch 1570, Average Loss: 6.692031812667847\n",
      "Epoch 1580, Average Loss: 6.816372060775757\n",
      "Epoch 1590, Average Loss: 6.497101211547852\n",
      "Epoch 1600, Average Loss: 6.512427091598511\n",
      "Epoch 1610, Average Loss: 6.468251466751099\n",
      "Epoch 1620, Average Loss: 6.562576866149902\n",
      "Epoch 1630, Average Loss: 6.942764377593994\n",
      "Epoch 1640, Average Loss: 6.7043328285217285\n",
      "Epoch 1650, Average Loss: 6.404776000976563\n",
      "Epoch 1660, Average Loss: 6.447444200515747\n",
      "Epoch 1670, Average Loss: 6.69502625465393\n",
      "Epoch 1680, Average Loss: 6.515667676925659\n",
      "Epoch 1690, Average Loss: 6.290517568588257\n",
      "Epoch 1700, Average Loss: 6.629496335983276\n",
      "Epoch 1710, Average Loss: 6.824143266677856\n",
      "Epoch 1720, Average Loss: 7.57087779045105\n",
      "Epoch 1730, Average Loss: 7.006137371063232\n",
      "Epoch 1740, Average Loss: 6.559624481201172\n",
      "Epoch 1750, Average Loss: 6.349080610275268\n",
      "Epoch 1760, Average Loss: 6.195738697052002\n",
      "Epoch 1770, Average Loss: 6.358281755447388\n",
      "Epoch 1780, Average Loss: 6.2529960632324215\n",
      "Epoch 1790, Average Loss: 6.763889312744141\n",
      "Epoch 1800, Average Loss: 6.913477373123169\n",
      "Epoch 1810, Average Loss: 6.903860378265381\n",
      "Epoch 1820, Average Loss: 6.53536024093628\n",
      "Epoch 1830, Average Loss: 6.7719464778900145\n",
      "Epoch 1840, Average Loss: 6.660481643676758\n",
      "Epoch 1850, Average Loss: 6.594588804244995\n",
      "Epoch 1860, Average Loss: 6.612804365158081\n",
      "Epoch 1870, Average Loss: 6.475973796844483\n",
      "Epoch 1880, Average Loss: 6.5993517398834225\n",
      "Epoch 1890, Average Loss: 6.406216716766357\n",
      "Epoch 1900, Average Loss: 6.129737901687622\n",
      "Epoch 1910, Average Loss: 6.001789522171021\n",
      "Epoch 1920, Average Loss: 5.810801601409912\n",
      "Epoch 1930, Average Loss: 5.669395208358765\n",
      "Epoch 1940, Average Loss: 5.70389986038208\n",
      "Epoch 1950, Average Loss: 5.6761198997497555\n",
      "Epoch 1960, Average Loss: 5.540155649185181\n",
      "Epoch 1970, Average Loss: 5.617246341705322\n",
      "Epoch 1980, Average Loss: 5.624659299850464\n",
      "Epoch 1990, Average Loss: 5.839242219924927\n",
      "Epoch 2000, Average Loss: 5.560847663879395\n",
      "Epoch 2010, Average Loss: 5.753531789779663\n",
      "Epoch 2020, Average Loss: 5.798350524902344\n",
      "Epoch 2030, Average Loss: 5.633213901519776\n",
      "Epoch 2040, Average Loss: 5.4625935554504395\n",
      "Epoch 2050, Average Loss: 5.797200632095337\n",
      "Epoch 2060, Average Loss: 5.645018529891968\n",
      "Epoch 2070, Average Loss: 5.597653484344482\n",
      "Epoch 2080, Average Loss: 5.687207746505737\n",
      "Epoch 2090, Average Loss: 5.518633127212524\n",
      "Epoch 2100, Average Loss: 5.628941440582276\n",
      "Epoch 2110, Average Loss: 5.579203796386719\n",
      "Epoch 2120, Average Loss: 5.539701461791992\n",
      "Epoch 2130, Average Loss: 5.565907573699951\n",
      "Epoch 2140, Average Loss: 5.516776657104492\n",
      "Epoch 2150, Average Loss: 5.300768709182739\n",
      "Epoch 2160, Average Loss: 5.463080215454101\n",
      "Epoch 2170, Average Loss: 5.46084041595459\n",
      "Epoch 2180, Average Loss: 5.631338024139405\n",
      "Epoch 2190, Average Loss: 5.581158399581909\n",
      "Epoch 2200, Average Loss: 5.85604043006897\n",
      "Epoch 2210, Average Loss: 6.392429685592651\n",
      "Epoch 2220, Average Loss: 5.435760641098023\n",
      "Epoch 2230, Average Loss: 5.468538570404053\n",
      "Epoch 2240, Average Loss: 5.43822078704834\n",
      "Epoch 2250, Average Loss: 5.519921970367432\n",
      "Epoch 2260, Average Loss: 5.206878280639648\n",
      "Epoch 2270, Average Loss: 5.126280069351196\n",
      "Epoch 2280, Average Loss: 5.464421033859253\n",
      "Epoch 2290, Average Loss: 5.284937572479248\n",
      "Epoch 2300, Average Loss: 5.13849024772644\n",
      "Epoch 2310, Average Loss: 5.067407512664795\n",
      "Epoch 2320, Average Loss: 5.066242933273315\n",
      "Epoch 2330, Average Loss: 5.378307867050171\n",
      "Epoch 2340, Average Loss: 6.081954717636108\n",
      "Epoch 2350, Average Loss: 5.851465940475464\n",
      "Epoch 2360, Average Loss: 6.155460262298584\n",
      "Epoch 2370, Average Loss: 5.450954961776733\n",
      "Epoch 2380, Average Loss: 5.1797998428344725\n",
      "Epoch 2390, Average Loss: 5.135951089859009\n",
      "Epoch 2400, Average Loss: 5.0669961929321286\n",
      "Epoch 2410, Average Loss: 5.0176397323608395\n",
      "Epoch 2420, Average Loss: 4.982649660110473\n",
      "Epoch 2430, Average Loss: 5.166773843765259\n",
      "Epoch 2440, Average Loss: 5.244806098937988\n",
      "Epoch 2450, Average Loss: 5.206090497970581\n",
      "Epoch 2460, Average Loss: 5.264332866668701\n",
      "Epoch 2470, Average Loss: 5.231179046630859\n",
      "Epoch 2480, Average Loss: 5.147777509689331\n",
      "Epoch 2490, Average Loss: 5.062991380691528\n",
      "Epoch 2500, Average Loss: 4.981722450256347\n",
      "Epoch 2510, Average Loss: 5.212696886062622\n",
      "Epoch 2520, Average Loss: 5.179087495803833\n",
      "Epoch 2530, Average Loss: 4.998613739013672\n",
      "Epoch 2540, Average Loss: 5.0043559074401855\n",
      "Epoch 2550, Average Loss: 5.22234525680542\n",
      "Epoch 2560, Average Loss: 4.984699487686157\n",
      "Epoch 2570, Average Loss: 5.112930250167847\n",
      "Epoch 2580, Average Loss: 5.064137887954712\n",
      "Epoch 2590, Average Loss: 5.016681385040283\n",
      "Epoch 2600, Average Loss: 4.821387147903442\n",
      "Epoch 2610, Average Loss: 4.913604259490967\n",
      "Epoch 2620, Average Loss: 5.12946572303772\n",
      "Epoch 2630, Average Loss: 5.056812810897827\n",
      "Epoch 2640, Average Loss: 5.017928600311279\n",
      "Epoch 2650, Average Loss: 4.865338945388794\n",
      "Epoch 2660, Average Loss: 4.742520904541015\n",
      "Epoch 2670, Average Loss: 4.978773164749145\n",
      "Epoch 2680, Average Loss: 4.823734474182129\n",
      "Epoch 2690, Average Loss: 5.0966509819030765\n",
      "Epoch 2700, Average Loss: 5.239729309082032\n",
      "Epoch 2710, Average Loss: 4.86567440032959\n",
      "Epoch 2720, Average Loss: 4.773685789108276\n",
      "Epoch 2730, Average Loss: 4.7377125263214115\n",
      "Epoch 2740, Average Loss: 4.792244625091553\n",
      "Epoch 2750, Average Loss: 5.020421361923217\n",
      "Epoch 2760, Average Loss: 5.039653539657593\n",
      "Epoch 2770, Average Loss: 5.078441047668457\n",
      "Epoch 2780, Average Loss: 5.060022640228271\n",
      "Epoch 2790, Average Loss: 4.824644136428833\n",
      "Epoch 2800, Average Loss: 4.836228322982788\n",
      "Epoch 2810, Average Loss: 4.688071632385254\n",
      "Epoch 2820, Average Loss: 5.098168182373047\n",
      "Epoch 2830, Average Loss: 5.95090126991272\n",
      "Epoch 2840, Average Loss: 4.977221965789795\n",
      "Epoch 2850, Average Loss: 4.8689759254455565\n",
      "Epoch 2860, Average Loss: 4.818834733963013\n",
      "Epoch 2870, Average Loss: 4.7469542026519775\n",
      "Epoch 2880, Average Loss: 4.692241144180298\n",
      "Epoch 2890, Average Loss: 4.740378999710083\n",
      "Epoch 2900, Average Loss: 5.737720537185669\n",
      "Epoch 2910, Average Loss: 5.485857009887695\n",
      "Epoch 2920, Average Loss: 5.697048759460449\n",
      "Epoch 2930, Average Loss: 5.494553852081299\n",
      "Epoch 2940, Average Loss: 5.64098892211914\n",
      "Epoch 2950, Average Loss: 5.427072811126709\n",
      "Epoch 2960, Average Loss: 5.563334941864014\n",
      "Epoch 2970, Average Loss: 5.176933097839355\n",
      "Epoch 2980, Average Loss: 5.629800081253052\n",
      "Epoch 2990, Average Loss: 5.159541893005371\n",
      "Epoch 3000, Average Loss: 5.473147678375244\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6.构建测试集图数据对象",
   "id": "3e942a1212ab0648"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:11:35.409876Z",
     "start_time": "2024-11-20T16:11:35.058127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from function import metrics_to_dataframe, calculate_metrics\n",
    "\n",
    "# 加载最佳模型的状态字典\n",
    "model.load_state_dict(torch.load('gnn_best_model.pth', weights_only=True))\n",
    "\n",
    "# 将模型设置为评估模式\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 转换测试数据为张量\n",
    "X_test_torch = torch.tensor(X_test_scaled, dtype=torch.float)\n",
    "y_test_torch = torch.tensor(y_test, dtype=torch.float).view(-1, 1)  # 确保y是列向量\n",
    "\n",
    "# 创建测试集图数据对象\n",
    "test_data = Data(x=X_test_torch, edge_index=edge_index, y=y_test_torch).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 对训练集进行预测\n",
    "    out = model(train_data)\n",
    "    print(\"训练集预测结果:\")\n",
    "    print(out)\n",
    "\n",
    "    # 计算训练集的指标\n",
    "    train_metrics = calculate_metrics(train_data.y.cpu().numpy(), out.cpu().numpy())\n",
    "    print(\"训练集指标:\", train_metrics)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    test_out = model(test_data)\n",
    "    print(\"测试集预测结果:\")\n",
    "    print(test_out)\n",
    "\n",
    "    # 计算测试集的指标\n",
    "    test_metrics = calculate_metrics(test_data.y.cpu().numpy(), test_out.cpu().numpy())\n",
    "    print(\"测试集指标:\", test_metrics)\n",
    "\n",
    "    # 保存指标到CSV文件\n",
    "    metrics_df = metrics_to_dataframe(train_data.y.cpu().numpy(), out.cpu().numpy(),\n",
    "                                      test_data.y.cpu().numpy(), test_out.cpu().numpy(), 'GNN')\n",
    "    metrics_df.to_csv('gnn_metrics.csv', index=False)\n",
    "\n",
    "    print(metrics_df)\n"
   ],
   "id": "116c1a7b5eabdeb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集预测结果:\n",
      "tensor([[ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [ 66.8181],\n",
      "        [139.8364],\n",
      "        [155.8848],\n",
      "        [ 80.1820],\n",
      "        [ 51.9934],\n",
      "        [ 78.3552],\n",
      "        [146.4905],\n",
      "        [ 71.2654],\n",
      "        [ 55.6744],\n",
      "        [ 49.4663],\n",
      "        [114.6858],\n",
      "        [ 45.2909],\n",
      "        [113.2356],\n",
      "        [121.6429],\n",
      "        [ 66.9843],\n",
      "        [139.6713],\n",
      "        [ 91.2711],\n",
      "        [ 41.2199],\n",
      "        [103.9537],\n",
      "        [ 81.0079],\n",
      "        [ 61.6311],\n",
      "        [ 54.3573],\n",
      "        [102.4572],\n",
      "        [ 51.7242],\n",
      "        [ 93.5381],\n",
      "        [ 60.6645],\n",
      "        [124.4267],\n",
      "        [ 74.9495],\n",
      "        [103.5839],\n",
      "        [ 24.1408],\n",
      "        [ 82.8756],\n",
      "        [106.7130],\n",
      "        [119.7432],\n",
      "        [114.1906],\n",
      "        [ 82.1467],\n",
      "        [ 63.7557],\n",
      "        [ 48.1355],\n",
      "        [100.3587],\n",
      "        [ 53.2747],\n",
      "        [ 58.9528],\n",
      "        [ 49.2889],\n",
      "        [129.6292],\n",
      "        [ 14.7635],\n",
      "        [129.9938],\n",
      "        [ 34.7547],\n",
      "        [ 73.8693],\n",
      "        [ 63.5469],\n",
      "        [ 88.3726],\n",
      "        [101.9014],\n",
      "        [ 73.1608],\n",
      "        [101.0913],\n",
      "        [ 84.3620],\n",
      "        [ 51.4993],\n",
      "        [ 40.6137],\n",
      "        [ 86.4334],\n",
      "        [ 59.3677],\n",
      "        [ 72.0038],\n",
      "        [ 57.3356],\n",
      "        [ 36.1664],\n",
      "        [ 99.6146],\n",
      "        [109.9124],\n",
      "        [107.0211],\n",
      "        [ 75.5601],\n",
      "        [ 92.3901],\n",
      "        [ 85.1505],\n",
      "        [ 28.7698],\n",
      "        [171.7118],\n",
      "        [ 40.5573],\n",
      "        [ 68.9503],\n",
      "        [104.4569],\n",
      "        [ 79.1651],\n",
      "        [ 31.1362],\n",
      "        [137.7190],\n",
      "        [108.7355],\n",
      "        [ 84.6055],\n",
      "        [180.3845],\n",
      "        [ 95.2323],\n",
      "        [ 84.3756],\n",
      "        [ 75.3427],\n",
      "        [109.5409],\n",
      "        [ 77.4417],\n",
      "        [107.2354],\n",
      "        [162.9115],\n",
      "        [129.6849],\n",
      "        [139.2038],\n",
      "        [ 38.6866],\n",
      "        [105.8664],\n",
      "        [ 61.7943],\n",
      "        [120.4007],\n",
      "        [102.5339],\n",
      "        [ 80.7678],\n",
      "        [ 55.7027],\n",
      "        [ 25.5955],\n",
      "        [ 89.6341],\n",
      "        [ 92.2802],\n",
      "        [ 41.3027],\n",
      "        [100.9271],\n",
      "        [ 97.9348],\n",
      "        [ 97.7963],\n",
      "        [134.6658],\n",
      "        [ 97.9271],\n",
      "        [108.4625],\n",
      "        [ 68.3469],\n",
      "        [ 78.3572],\n",
      "        [ 45.7818],\n",
      "        [149.1494],\n",
      "        [ 21.4769],\n",
      "        [ 66.7532],\n",
      "        [140.2162],\n",
      "        [ 61.2551],\n",
      "        [ 87.6888],\n",
      "        [183.4601],\n",
      "        [ 78.8428],\n",
      "        [108.8688],\n",
      "        [137.5176],\n",
      "        [ 65.8698],\n",
      "        [ 61.4061],\n",
      "        [ 33.4327],\n",
      "        [ 94.0628],\n",
      "        [ 99.9153],\n",
      "        [ 99.6392],\n",
      "        [ 91.4341],\n",
      "        [ 63.3709],\n",
      "        [ 86.0815],\n",
      "        [ 43.9571],\n",
      "        [138.0581],\n",
      "        [ 43.8433],\n",
      "        [ 63.7914],\n",
      "        [ 72.2060],\n",
      "        [105.9775],\n",
      "        [181.5174],\n",
      "        [115.9420],\n",
      "        [ 98.9165],\n",
      "        [177.6999],\n",
      "        [147.1582],\n",
      "        [146.0306],\n",
      "        [145.6974],\n",
      "        [ 69.7413],\n",
      "        [159.9664],\n",
      "        [ 79.6137],\n",
      "        [ 45.0442],\n",
      "        [ 87.0159],\n",
      "        [ 81.6373],\n",
      "        [ 93.7593],\n",
      "        [152.5820],\n",
      "        [ 60.5185],\n",
      "        [ 85.2211],\n",
      "        [ 95.5184],\n",
      "        [ 91.4925],\n",
      "        [ 30.2137],\n",
      "        [101.8215],\n",
      "        [ 78.5637],\n",
      "        [136.5354],\n",
      "        [111.7087],\n",
      "        [ 68.7634],\n",
      "        [ 62.4934],\n",
      "        [ 60.6207],\n",
      "        [ 86.3495],\n",
      "        [141.2200],\n",
      "        [147.1467],\n",
      "        [ 55.9828],\n",
      "        [103.7410],\n",
      "        [ 80.6405],\n",
      "        [128.7130],\n",
      "        [124.7381],\n",
      "        [ 74.3621],\n",
      "        [ 39.8818],\n",
      "        [ 78.4891],\n",
      "        [112.0956],\n",
      "        [ 55.4687],\n",
      "        [ 95.9223],\n",
      "        [ 85.2618],\n",
      "        [100.8239],\n",
      "        [113.7058],\n",
      "        [104.7631],\n",
      "        [124.8278],\n",
      "        [ 42.1158],\n",
      "        [ 58.6649],\n",
      "        [112.5810],\n",
      "        [ 59.6240],\n",
      "        [ 51.6686],\n",
      "        [109.0420],\n",
      "        [ 61.3659],\n",
      "        [158.1041],\n",
      "        [104.4742],\n",
      "        [ 78.2528],\n",
      "        [140.8778],\n",
      "        [103.8966],\n",
      "        [ 84.1774],\n",
      "        [114.9645],\n",
      "        [ 46.8153],\n",
      "        [ 67.7004],\n",
      "        [ 72.3346],\n",
      "        [116.9846],\n",
      "        [150.7865],\n",
      "        [108.1844],\n",
      "        [122.6330],\n",
      "        [116.4624],\n",
      "        [ 77.3551],\n",
      "        [117.5102],\n",
      "        [ 82.2634],\n",
      "        [115.8393],\n",
      "        [111.9413],\n",
      "        [ 49.4181],\n",
      "        [ 66.5132],\n",
      "        [ 90.9364],\n",
      "        [171.2553],\n",
      "        [ 72.5978],\n",
      "        [101.2959],\n",
      "        [ 43.1027],\n",
      "        [ 57.4124],\n",
      "        [104.2786],\n",
      "        [118.7447],\n",
      "        [ 54.8787],\n",
      "        [ 95.7213],\n",
      "        [ 92.7574],\n",
      "        [ 65.1835],\n",
      "        [164.6931],\n",
      "        [ 45.6997],\n",
      "        [122.4148],\n",
      "        [ 90.1297],\n",
      "        [ 92.9016],\n",
      "        [ 50.7892],\n",
      "        [105.4133],\n",
      "        [137.2472],\n",
      "        [127.4741],\n",
      "        [166.4335],\n",
      "        [ 81.9483],\n",
      "        [ 91.6136],\n",
      "        [ 88.7415],\n",
      "        [101.7023],\n",
      "        [ 69.6138],\n",
      "        [ 79.9335],\n",
      "        [117.1505],\n",
      "        [ 42.2319],\n",
      "        [125.6934],\n",
      "        [ 83.1324],\n",
      "        [ 45.8609],\n",
      "        [169.2510],\n",
      "        [126.4535],\n",
      "        [122.1476],\n",
      "        [ 52.2859],\n",
      "        [104.9635],\n",
      "        [ 84.6275],\n",
      "        [ 45.9392],\n",
      "        [ 23.3535],\n",
      "        [ 86.4145],\n",
      "        [ 44.7690],\n",
      "        [138.1470],\n",
      "        [ 82.4021],\n",
      "        [ 39.2637],\n",
      "        [ 62.1369],\n",
      "        [159.3207],\n",
      "        [ 85.2333],\n",
      "        [ 48.6815],\n",
      "        [101.9763],\n",
      "        [ 71.8277],\n",
      "        [111.2318],\n",
      "        [145.8664],\n",
      "        [ 65.2505],\n",
      "        [103.5718],\n",
      "        [100.1967],\n",
      "        [ 45.0806],\n",
      "        [ 49.9676],\n",
      "        [188.1534],\n",
      "        [116.5282],\n",
      "        [ 92.0745],\n",
      "        [ 26.7146],\n",
      "        [ 79.3763],\n",
      "        [174.5115],\n",
      "        [112.0359],\n",
      "        [ 90.4691],\n",
      "        [ 17.6231],\n",
      "        [ 63.0421],\n",
      "        [ 66.3635],\n",
      "        [ 30.2424],\n",
      "        [ 87.9705],\n",
      "        [ 88.7345],\n",
      "        [ 71.1227],\n",
      "        [ 67.7453],\n",
      "        [115.2384],\n",
      "        [ 76.1587],\n",
      "        [ 81.2956],\n",
      "        [ 27.0392],\n",
      "        [112.1611],\n",
      "        [107.0474],\n",
      "        [ 59.1590],\n",
      "        [ 57.0167],\n",
      "        [106.8590],\n",
      "        [137.7558],\n",
      "        [ 58.7013],\n",
      "        [ 97.0730],\n",
      "        [180.3365],\n",
      "        [ 89.5732],\n",
      "        [145.6630],\n",
      "        [ 87.7516],\n",
      "        [ 90.0766],\n",
      "        [ 69.6627],\n",
      "        [ 93.8331],\n",
      "        [149.4458],\n",
      "        [107.4741],\n",
      "        [100.3078],\n",
      "        [150.8624],\n",
      "        [ 78.1553],\n",
      "        [184.8888],\n",
      "        [ 72.4711],\n",
      "        [ 72.5932],\n",
      "        [ 93.8694],\n",
      "        [ 91.8440],\n",
      "        [ 67.8863],\n",
      "        [ 66.7510],\n",
      "        [ 88.0715],\n",
      "        [143.7752],\n",
      "        [ 94.0232],\n",
      "        [ 94.9597],\n",
      "        [ 88.1384],\n",
      "        [160.6037],\n",
      "        [ 82.5340],\n",
      "        [ 93.3256],\n",
      "        [ 85.6742],\n",
      "        [101.6353],\n",
      "        [ 61.7845],\n",
      "        [ 67.0934],\n",
      "        [100.7505],\n",
      "        [110.7521],\n",
      "        [ 77.6359],\n",
      "        [ 96.2160],\n",
      "        [ 69.7270],\n",
      "        [ 86.6350],\n",
      "        [133.8518],\n",
      "        [ 39.8735],\n",
      "        [129.1041],\n",
      "        [ 56.7340],\n",
      "        [ 73.9608],\n",
      "        [ 77.1054],\n",
      "        [131.4522],\n",
      "        [114.8254],\n",
      "        [ 66.8087],\n",
      "        [ 73.6629],\n",
      "        [ 32.5968],\n",
      "        [ 87.5468],\n",
      "        [103.0065],\n",
      "        [ 56.7069],\n",
      "        [117.9598],\n",
      "        [107.7058],\n",
      "        [123.8781],\n",
      "        [ 93.2085],\n",
      "        [ 93.1694],\n",
      "        [132.7513],\n",
      "        [103.8938],\n",
      "        [133.5480],\n",
      "        [128.5882],\n",
      "        [144.1808],\n",
      "        [ 96.1951],\n",
      "        [ 30.2872],\n",
      "        [ 75.8936],\n",
      "        [ 62.0064],\n",
      "        [ 93.3169],\n",
      "        [ 60.6141],\n",
      "        [ 66.5134],\n",
      "        [ 80.6992],\n",
      "        [ 27.5715],\n",
      "        [ 88.7000],\n",
      "        [ 59.8559],\n",
      "        [ 65.7327],\n",
      "        [210.3618],\n",
      "        [ 87.9974],\n",
      "        [181.9370],\n",
      "        [131.9522],\n",
      "        [ 83.2136],\n",
      "        [ 62.9325],\n",
      "        [ 68.8748],\n",
      "        [ 78.0210],\n",
      "        [ 57.3750],\n",
      "        [180.1966],\n",
      "        [116.0304],\n",
      "        [ 75.0691],\n",
      "        [107.5083],\n",
      "        [ 47.0225],\n",
      "        [ 50.2573],\n",
      "        [ 36.6494],\n",
      "        [128.0783],\n",
      "        [ 21.7563],\n",
      "        [113.0801],\n",
      "        [109.4058],\n",
      "        [ 90.3714],\n",
      "        [ 57.9923],\n",
      "        [ 73.1873],\n",
      "        [140.6479],\n",
      "        [ 45.6883],\n",
      "        [137.4466],\n",
      "        [116.0856],\n",
      "        [122.2947],\n",
      "        [ 70.3379],\n",
      "        [110.7445],\n",
      "        [ 73.3722],\n",
      "        [ 71.8067],\n",
      "        [ 89.9216],\n",
      "        [122.3616],\n",
      "        [ 61.0775],\n",
      "        [ 73.8102],\n",
      "        [155.8927],\n",
      "        [180.4086],\n",
      "        [ 84.2091],\n",
      "        [ 61.1075],\n",
      "        [ 85.8723],\n",
      "        [ 74.2958],\n",
      "        [ 77.8003],\n",
      "        [ 83.5025],\n",
      "        [ 89.6283],\n",
      "        [ 44.6301],\n",
      "        [ 20.7615],\n",
      "        [ 43.8843],\n",
      "        [129.0458],\n",
      "        [100.5351],\n",
      "        [112.8751],\n",
      "        [121.0215],\n",
      "        [ 24.7367],\n",
      "        [ 50.9917],\n",
      "        [ 99.8947],\n",
      "        [ 73.8520],\n",
      "        [ 96.2484],\n",
      "        [ 78.6893],\n",
      "        [ 64.1427],\n",
      "        [112.6835],\n",
      "        [124.8639],\n",
      "        [143.6241],\n",
      "        [ 68.8697],\n",
      "        [ 86.7701],\n",
      "        [ 76.8431],\n",
      "        [ 38.7766],\n",
      "        [ 96.5296],\n",
      "        [168.5467],\n",
      "        [132.8935],\n",
      "        [ 68.6928],\n",
      "        [100.4377],\n",
      "        [ 99.2080],\n",
      "        [ 17.2949],\n",
      "        [ 28.6806],\n",
      "        [ 82.2246],\n",
      "        [ 77.6526],\n",
      "        [ 86.7271],\n",
      "        [ 41.2278],\n",
      "        [ 73.8561],\n",
      "        [ 94.5402],\n",
      "        [112.4502],\n",
      "        [126.0572],\n",
      "        [ 55.6933],\n",
      "        [ 89.6652],\n",
      "        [137.3558],\n",
      "        [ 51.0489],\n",
      "        [ 67.1380],\n",
      "        [ 82.2490],\n",
      "        [ 83.8354],\n",
      "        [170.1841],\n",
      "        [ 58.8784],\n",
      "        [136.8124],\n",
      "        [ 67.7865],\n",
      "        [ 96.1929],\n",
      "        [119.4508],\n",
      "        [ 74.4053],\n",
      "        [ 79.7316],\n",
      "        [139.7285],\n",
      "        [ 78.9939],\n",
      "        [134.3215],\n",
      "        [ 88.9819],\n",
      "        [ 96.7397],\n",
      "        [139.6772],\n",
      "        [107.2914],\n",
      "        [ 91.9443],\n",
      "        [ 38.9123],\n",
      "        [127.2392],\n",
      "        [ 92.4320],\n",
      "        [138.8114]], device='cuda:0')\n",
      "训练集指标: (0.8938177824020386, 5.3050895, 4.591084271669388, 13.0084)\n",
      "测试集预测结果:\n",
      "tensor([[ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 28.0839],\n",
      "        [ 99.7561],\n",
      "        [ 97.9898],\n",
      "        [131.8829],\n",
      "        [ 40.4217],\n",
      "        [ 33.2971],\n",
      "        [ 90.9215],\n",
      "        [ 45.3425],\n",
      "        [ 69.0979],\n",
      "        [ 37.8806],\n",
      "        [ 87.7540],\n",
      "        [132.9539],\n",
      "        [172.2462],\n",
      "        [113.9080],\n",
      "        [103.5606],\n",
      "        [ 82.1977],\n",
      "        [ 69.3939],\n",
      "        [150.9013],\n",
      "        [ 65.7072],\n",
      "        [122.0251],\n",
      "        [ 73.2155],\n",
      "        [ 87.5021],\n",
      "        [ 42.0219],\n",
      "        [ 51.6952],\n",
      "        [ 75.0617],\n",
      "        [ 54.8410],\n",
      "        [ 46.5312],\n",
      "        [ 99.1927],\n",
      "        [ 76.1438],\n",
      "        [108.7012],\n",
      "        [ 39.5396],\n",
      "        [ 83.4967],\n",
      "        [ 92.1743],\n",
      "        [112.2382],\n",
      "        [122.8242],\n",
      "        [ 58.0405],\n",
      "        [ 72.0124],\n",
      "        [166.9703],\n",
      "        [ 88.7088],\n",
      "        [ 69.8123],\n",
      "        [ 62.2463],\n",
      "        [ 62.6222],\n",
      "        [ 58.8969],\n",
      "        [102.4156],\n",
      "        [137.1771],\n",
      "        [117.4528],\n",
      "        [ 63.8268],\n",
      "        [ 93.8780],\n",
      "        [ 76.5294],\n",
      "        [ 21.2582],\n",
      "        [ 82.5697],\n",
      "        [ 78.9570],\n",
      "        [113.0067],\n",
      "        [101.1602],\n",
      "        [ 65.3139],\n",
      "        [134.7999],\n",
      "        [175.8793],\n",
      "        [103.3753],\n",
      "        [ 84.6167],\n",
      "        [ 25.4740],\n",
      "        [ 70.6656],\n",
      "        [ 99.8169],\n",
      "        [ 62.4023],\n",
      "        [156.5106],\n",
      "        [ 69.7829],\n",
      "        [ 97.7625],\n",
      "        [ 83.7070],\n",
      "        [ 67.9255],\n",
      "        [112.2777],\n",
      "        [109.9291],\n",
      "        [ 87.0428],\n",
      "        [ 76.6931],\n",
      "        [180.4230],\n",
      "        [123.5077],\n",
      "        [161.6140],\n",
      "        [ 59.3001],\n",
      "        [ 42.8867],\n",
      "        [ 72.0771],\n",
      "        [ 71.4025],\n",
      "        [ 78.3124],\n",
      "        [ 63.6648],\n",
      "        [131.1913],\n",
      "        [139.7921],\n",
      "        [ 81.3329],\n",
      "        [135.7507],\n",
      "        [ 78.2013],\n",
      "        [114.5081],\n",
      "        [130.7291],\n",
      "        [ 39.5900],\n",
      "        [154.0516],\n",
      "        [ 56.6239],\n",
      "        [ 88.4206],\n",
      "        [112.6681],\n",
      "        [111.6356],\n",
      "        [ 54.0002],\n",
      "        [ 93.4392],\n",
      "        [103.0092],\n",
      "        [154.5131],\n",
      "        [ 99.4594],\n",
      "        [157.7057],\n",
      "        [180.4772],\n",
      "        [ 87.2921],\n",
      "        [115.5744],\n",
      "        [182.2703],\n",
      "        [143.5070],\n",
      "        [ 52.0467],\n",
      "        [ 83.8576],\n",
      "        [ 54.7084],\n",
      "        [ 93.0404],\n",
      "        [ 50.6085],\n",
      "        [ 96.1984],\n",
      "        [ 66.9434],\n",
      "        [108.1717],\n",
      "        [ 90.5355]], device='cuda:0')\n",
      "测试集指标: (0.6769518852233887, 12.340494, 14.989784359931946, 22.556501)\n",
      "  model  R2_train  MAE_train  MAPE_train  RMSE_train   R2_test   MAE_test  \\\n",
      "0   GNN  0.893818   5.305089    4.591084     13.0084  0.676952  12.340494   \n",
      "\n",
      "   MAPE_test  RMSE_test  \n",
      "0  14.989784  22.556501  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-20T16:12:54.029665Z",
     "start_time": "2024-11-20T16:12:54.014300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 保存训练集和测试集的预测结果（包含真实值）\n",
    "train_predictions = pd.DataFrame({'Actual': train_data.y.cpu().detach().numpy().flatten(),\n",
    "                                  'Predicted': model(train_data).cpu().detach().numpy().flatten()})\n",
    "test_predictions = pd.DataFrame({'Actual': test_data.y.cpu().detach().numpy().flatten(),\n",
    "                                 'Predicted': model(test_data).cpu().detach().numpy().flatten()})\n",
    "\n",
    "train_predictions.to_csv('gnn_train_predictions.csv', index=False)\n",
    "test_predictions.to_csv('gnn_test_predictions.csv', index=False)"
   ],
   "id": "610248f1bf86c0d1",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
